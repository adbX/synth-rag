{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Synth-RAG","text":"<p>A retrieval-augmented generation (RAG) system for querying PDF manuals of MIDI synthesizers using ColPali multivector embeddings, hybrid search, and agentic workflows with LangGraph.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>ColPali Multivector Embeddings - Vision Language Models process PDF pages as images</li> <li>Hybrid Search - Combines dense (FastEmbed), sparse (BM25), and multivector representations</li> <li>Two-Stage Retrieval - Fast prefetch with HNSW-indexed vectors, precise reranking with ColPali</li> <li>Agentic RAG - LangGraph-powered agent with manual search and web fallback</li> <li>Scalable - Optimized for large PDF collections</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Ingest manuals\nuv run python -m synth_rag.manuals_ingest --subset test --collection midi_manuals\n\n# Query with hybrid search\nuv run python -m synth_rag.manuals_query \\\n    --question \"How do I set up MIDI channels on the Digitone II?\"\n\n# Use agentic workflow\nuv run python -m synth_rag.manuals_agent \\\n    --question \"What are the differences between Digitakt and Digitone?\"\n</code></pre>"},{"location":"#technology-stack","title":"Technology Stack","text":"<pre><code>graph TD\n    subgraph Storage[Storage Layer]\n        Qdrant[Qdrant&lt;br/&gt;Vector Database]\n    end\n\n    subgraph Embeddings[Embedding Models]\n        ColPali[ColPali&lt;br/&gt;vidore/colpali-v1.3&lt;br/&gt;Vision-Language Model]\n        FastEmbed[FastEmbed&lt;br/&gt;all-MiniLM-L6-v2&lt;br/&gt;Dense Embeddings]\n        BM25[BM25&lt;br/&gt;Sparse Embeddings]\n    end\n\n    subgraph Agent[Agent Layer]\n        LangGraph[LangGraph&lt;br/&gt;Agent Framework]\n        GPT[OpenAI GPT-4o-mini&lt;br/&gt;LLM]\n        Brave[Brave Search API&lt;br/&gt;Web Search]\n    end\n\n    subgraph Tools[Development]\n        UV[uv&lt;br/&gt;Package Manager]\n    end\n\n    Embeddings --&gt; Qdrant\n    Agent --&gt; Qdrant\n\n    style Qdrant fill:#dc143c,color:#fff\n    style ColPali fill:#4a90e2,color:#fff\n    style LangGraph fill:#2ecc71,color:#fff\n    style GPT fill:#9b59b6,color:#fff</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Get up and running</li> <li>Setup Instructions - Installation guide</li> <li>Usage Examples - Learn how to use each component</li> <li>Architecture - Understand the system design</li> <li>Benchmarking Guide - Evaluate performance with RAGBench</li> <li>API Reference - Explore the codebase</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#system-overview","title":"System Overview","text":"<pre><code>flowchart TB\n    User[User Query]\n\n    subgraph Agentic[\"Agentic Layer (LangGraph)\"]\n        ManualTool[Manual Retriever Tool]\n        WebTool[Web Search Tool&lt;br/&gt;Brave]\n    end\n\n    subgraph HybridSearch[\"Hybrid Search Layer (Qdrant)\"]\n        subgraph Prefetch[\"Prefetch Stage&lt;br/&gt;(HNSW Indexed)\"]\n            Dense[Dense vectors&lt;br/&gt;FastEmbed]\n            Sparse[Sparse vectors&lt;br/&gt;BM25]\n            RowPool[ColPali row-pooled&lt;br/&gt;multivectors]\n            ColPool[ColPali col-pooled&lt;br/&gt;multivectors]\n        end\n\n        Rerank[\"Rerank Stage&lt;br/&gt;\u2022 Original ColPali multivectors&lt;br/&gt;\u2022 MaxSim scoring\"]\n\n        Prefetch --&gt; Rerank\n    end\n\n    subgraph DocPipeline[\"Document Processing Pipeline\"]\n        subgraph Extract[Extraction Layer]\n            PDFRender[PDF Render&lt;br/&gt;pypdfium2]\n            TextExtract[Text Extraction&lt;br/&gt;pymupdf]\n        end\n\n        subgraph Embed[Embedding Layer]\n            ColPaliEmbed[ColPali&lt;br/&gt;Embeddings]\n            FastEmbed[FastEmbed + BM25&lt;br/&gt;Embeddings]\n        end\n\n        PDFRender --&gt; ColPaliEmbed\n        TextExtract --&gt; FastEmbed\n    end\n\n    User --&gt; Agentic\n    ManualTool --&gt; HybridSearch\n    HybridSearch --&gt; DocPipeline</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-document-processing","title":"1. Document Processing","text":""},{"location":"architecture/#pdf-rendering-pypdfium2","title":"PDF Rendering (<code>pypdfium2</code>)","text":"<p>Converts each PDF page to an RGB image for ColPali processing:</p> <pre><code>def render_pdf_to_images(pdf_path: Path) -&gt; list[Image]:\n    pdf = pdfium.PdfDocument(pdf_path)\n    images = []\n    for page in pdf:\n        bitmap = page.render(scale=2.0)\n        pil_image = bitmap.to_pil()\n        images.append(pil_image.convert(\"RGB\"))\n    return images\n</code></pre> <p>Why render to images?</p> <p>ColPali is a vision-language model that processes PDFs as images, capturing:</p> <ul> <li>Visual layout and structure</li> <li>Tables, diagrams, and figures</li> <li>Font styling and emphasis</li> <li>Spatial relationships</li> </ul>"},{"location":"architecture/#text-extraction-pymupdf","title":"Text Extraction (<code>pymupdf</code>)","text":"<p>Extracts plain text per page for:</p> <ol> <li>Dense/sparse text embeddings</li> <li>Payload metadata in search results</li> <li>Human-readable snippets</li> </ol>"},{"location":"architecture/#2-embedding-generation","title":"2. Embedding Generation","text":""},{"location":"architecture/#colpali-multivectors","title":"ColPali Multivectors","text":"<p>ColPali generates ~1,030 vectors per page (32\u00d732 patches + special tokens):</p> <pre><code># Original multivectors: [1030, 128]\noriginal_multivectors = colpali_model(image)\n\n# Mean-pooled variants for faster indexing\nrow_pooled = original_multivectors.reshape(32, 32, 128).mean(axis=1)  # [32, 128]\ncol_pooled = original_multivectors.reshape(32, 32, 128).mean(axis=0)  # [32, 128]\n</code></pre> <p>Why three variants?</p> <pre><code>graph LR\n    subgraph Original[Original Multivectors]\n        O1030[1030 vectors \u00d7 128 dim]\n        ONoIndex[\u274c No HNSW Index]\n        OUse[Precise reranking&lt;br/&gt;MaxSim scoring]\n    end\n\n    subgraph RowPooled[Row-Pooled]\n        R32[32 vectors \u00d7 128 dim]\n        RIndex[\u2705 HNSW Indexed]\n        RUse[Fast vertical&lt;br/&gt;structure matching]\n    end\n\n    subgraph ColPooled[Col-Pooled]\n        C32[32 vectors \u00d7 128 dim]\n        CIndex[\u2705 HNSW Indexed]\n        CUse[Fast horizontal&lt;br/&gt;structure matching]\n    end\n\n    style Original fill:#ffebee\n    style RowPooled fill:#e8f5e9\n    style ColPooled fill:#e8f5e9</code></pre>"},{"location":"architecture/#dense-embeddings-fastembed","title":"Dense Embeddings (FastEmbed)","text":"<p>384-dimensional vectors using <code>all-MiniLM-L6-v2</code>:</p> <pre><code>dense_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\ndense_vectors = list(dense_model.embed(text_chunks))\n</code></pre> <p>Chunking strategy: Semantic text splitter with 512-token chunks, 50-token overlap.</p>"},{"location":"architecture/#sparse-embeddings-bm25","title":"Sparse Embeddings (BM25)","text":"<p>Keyword-based retrieval with IDF weighting:</p> <pre><code>sparse_model = SparseTextEmbedding(\"Qdrant/bm25\")\nsparse_vectors = list(sparse_model.embed(text_chunks))\n</code></pre>"},{"location":"architecture/#3-qdrant-collection-schema","title":"3. Qdrant Collection Schema","text":"<p>Each page is stored as a point with multiple named vectors:</p> <pre><code>{\n    \"id\": \"&lt;uuid&gt;\",\n    \"payload\": {\n        \"manual_name\": \"Digitone-2-User-Manual\",\n        \"page_num\": 42,\n        \"page_text\": \"MIDI Settings...\",\n        \"pdf_path\": \"/path/to/manual.pdf\",\n    },\n    \"vectors\": {\n        \"colpali_original\": [1030 x 128],  # No HNSW\n        \"colpali_rows\": [32 x 128],         # HNSW indexed\n        \"colpali_cols\": [32 x 128],         # HNSW indexed\n        \"dense\": [384],                     # HNSW indexed\n        \"sparse\": {indices: [...], values: [...]}  # Inverted index\n    }\n}\n</code></pre>"},{"location":"architecture/#4-hybrid-search-reranking","title":"4. Hybrid Search &amp; Reranking","text":""},{"location":"architecture/#two-stage-retrieval","title":"Two-Stage Retrieval","text":"<p>Stage 1: Prefetch (Fast)</p> <p>Uses HNSW-indexed vectors to retrieve top-N candidates:</p> <pre><code>client.query_points(\n    collection_name=\"midi_manuals\",\n    prefetch=[\n        models.Prefetch(query=dense_vector, using=\"dense\", limit=50),\n        models.Prefetch(query=sparse_vector, using=\"sparse\", limit=50),\n        models.Prefetch(query=colpali_rows, using=\"colpali_rows\", limit=50),\n        models.Prefetch(query=colpali_cols, using=\"colpali_cols\", limit=50),\n    ],\n    # ... rerank configuration\n)\n</code></pre> <p>Stage 2: Rerank (Precise)</p> <p>Uses original ColPali multivectors with MaxSim scoring:</p> <pre><code>query=models.Query(\n    vector=original_colpali_query,  # [1030, 128]\n    using=\"colpali_original\",\n    rescorer=models.MaxSimRescorer(),\n)\n</code></pre>"},{"location":"architecture/#maxsim-scoring","title":"MaxSim Scoring","text":"<p>For each query vector (q_i), find the maximum similarity with document vectors:</p> <p>[ \\text{MaxSim}(Q, D) = \\sum_{i=1}^{|Q|} \\max_{j=1}^{|D|} \\text{sim}(q_i, d_j) ]</p>"},{"location":"architecture/#5-agentic-layer-langgraph","title":"5. Agentic Layer (LangGraph)","text":""},{"location":"architecture/#state-graph","title":"State Graph","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Agent\n    Agent --&gt; NeedTool: Decision\n    NeedTool --&gt; Tools: Yes\n    NeedTool --&gt; [*]: No\n    Tools --&gt; Agent: Return results\n\n    note right of Agent\n        call_model()\n        Analyzes state and decides\n        whether to call tools\n    end note\n\n    note right of Tools\n        tool_node()\n        Executes manual retriever\n        or web search tool\n    end note</code></pre>"},{"location":"architecture/#agent-implementation","title":"Agent Implementation","text":"<pre><code>class State(TypedDict):\n    messages: Annotated[list, lambda x, y: x + y]\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\nworkflow.add_edge(START, \"agent\")\nworkflow.add_conditional_edges(\"agent\", should_continue)\nworkflow.add_edge(\"tools\", \"agent\")\n</code></pre>"},{"location":"architecture/#agent-tools","title":"Agent Tools","text":"<p>1. Manual Retriever Tool</p> <pre><code>@tool\ndef manuals_retriever_tool(query: str) -&gt; str:\n    \"\"\"Retrieve information from MIDI synthesizer manuals.\"\"\"\n    results = search_manuals(query)\n    return format_results_with_citations(results)\n</code></pre> <p>2. Web Search Tool</p> <pre><code>@tool\ndef web_search_tool(query: str) -&gt; str:\n    \"\"\"Search the web using Brave Search API.\"\"\"\n    response = brave_client.search(q=query, count=5)\n    return format_web_results(response)\n</code></pre>"},{"location":"architecture/#agent-behavior","title":"Agent Behavior","text":"<p>The system prompt ensures:</p> <ol> <li>Always call manuals first - No exceptions</li> <li>Cite sources - Format: <code>(Manual Name, Page X)</code></li> <li>Structure responses - Manuals section, then web results</li> <li>Web as fallback - Only if manual search fails</li> </ol>"},{"location":"architecture/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/#why-colpali","title":"Why ColPali?","text":"<p>Traditional OCR-based approaches lose:</p> <ul> <li>Visual layout information</li> <li>Table structures</li> <li>Diagrams and figures</li> <li>Font emphasis (bold, italic)</li> </ul> <p>ColPali processes PDFs as images, preserving all visual information.</p>"},{"location":"architecture/#why-two-stage-retrieval","title":"Why Two-Stage Retrieval?","text":"<p>Indexing 1,030-dimensional multivectors with HNSW is slow and memory-intensive. Mean-pooling provides a fast approximation for prefetch, then MaxSim reranks precisely.</p>"},{"location":"architecture/#why-hybrid-search","title":"Why Hybrid Search?","text":"<pre><code>graph LR\n    subgraph QueryTypes[Query Type Examples]\n        Q1[Semantic Query&lt;br/&gt;'What is the synthesis engine?']\n        Q2[Keyword Query&lt;br/&gt;'MIDI CC 74']\n        Q3[Visual+Semantic&lt;br/&gt;'How do I set the filter?']\n    end\n\n    subgraph SearchMethods[Search Methods]\n        Dense[Dense&lt;br/&gt;FastEmbed&lt;br/&gt;semantic similarity]\n        Sparse[Sparse&lt;br/&gt;BM25&lt;br/&gt;keyword matching]\n        ColPali[ColPali&lt;br/&gt;multivector&lt;br/&gt;visual+semantic]\n    end\n\n    subgraph Result[Combined Result]\n        Hybrid[Hybrid Search&lt;br/&gt;Robust &amp; Query-Agnostic]\n    end\n\n    Q1 -.Best match.-&gt; Dense\n    Q2 -.Best match.-&gt; Sparse\n    Q3 -.Best match.-&gt; ColPali\n\n    Dense --&gt; Hybrid\n    Sparse --&gt; Hybrid\n    ColPali --&gt; Hybrid\n\n    style Q1 fill:#e1f5fe\n    style Q2 fill:#f3e5f5\n    style Q3 fill:#fff3e0\n    style Hybrid fill:#c8e6c9</code></pre> <p>Different search methods excel at different queries:</p> <ul> <li>Dense: \"What is the synthesis engine?\" (semantic)</li> <li>Sparse: \"MIDI CC 74\" (keywords)</li> <li>ColPali: \"How do I set the filter?\" (visual + semantic)</li> </ul> <p>Combining all three provides robust, query-agnostic retrieval.</p>"},{"location":"architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Usage Guide - Use the system</li> <li>API Reference - Explore the code</li> </ul>"},{"location":"benchmarking/","title":"RAGBench Benchmarking Guide","text":""},{"location":"benchmarking/#overview","title":"Overview","text":"<p>This guide describes how to benchmark the synth-rag system using the RAGBench dataset from Hugging Face. The benchmarking system evaluates hybrid search performance and computes comprehensive metrics including RAGAS faithfulness/context_relevancy and TruLens groundedness/context_relevance.</p>"},{"location":"benchmarking/#architecture","title":"Architecture","text":"<p>The benchmarking system consists of three independent modules:</p> <ol> <li>Document Ingestion (<code>benchmark_ingest.py</code>) - Loads RAGBench documents into a dedicated Qdrant collection</li> <li>Benchmark Runner (<code>benchmark_runner.py</code>) - Runs hybrid search queries and generates responses</li> <li>Metrics Evaluation (<code>benchmark_metrics.py</code>) - Computes evaluation metrics using RAGAS and TruLens</li> </ol>"},{"location":"benchmarking/#key-differences-from-main-system","title":"Key Differences from Main System","text":"<ul> <li>Text-only embeddings: Uses FastEmbed (dense) + BM25 (sparse) only, no ColPali</li> <li>Separate collections: Each RAGBench sub-dataset gets its own Qdrant collection</li> <li>Comprehensive logging: All queries, responses, and metrics are saved for analysis</li> </ul>"},{"location":"benchmarking/#quick-start","title":"Quick Start","text":""},{"location":"benchmarking/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the required dependencies installed:</p> <pre><code># Already included in pyproject.toml:\n# - ragas\n# - trulens_eval\n# - datasets\n</code></pre> <p>Set up your environment variables in <code>.env</code>:</p> <pre><code>QDRANT_URL=&lt;your-qdrant-url&gt;\nQDRANT_KEY=&lt;your-qdrant-key&gt;\nOPENAI_API_KEY=&lt;your-openai-key&gt;\n</code></pre>"},{"location":"benchmarking/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code># 1. Ingest RAGBench emanual dataset (all splits)\nuv run python -m synth_rag.benchmark_ingest \\\n    --dataset emanual \\\n    --split all \\\n    --collection ragbench_emanual \\\n    --recreate-collection\n\n# 2. Run benchmark on test split (recommended to start small)\nuv run python -m synth_rag.benchmark_runner \\\n    --dataset emanual \\\n    --split test \\\n    --collection ragbench_emanual \\\n    --model gpt-4o-mini \\\n    --top-k 5 \\\n    --max-examples 10  # Optional: limit for testing\n\n# 3. Compute metrics on the results\nuv run python -m synth_rag.benchmark_metrics \\\n    --results-file logs/benchmark_ragbench/emanual/20251130_120000_raw_results.jsonl\n</code></pre>"},{"location":"benchmarking/#step-1-document-ingestion","title":"Step 1: Document Ingestion","text":""},{"location":"benchmarking/#command","title":"Command","text":"<pre><code>uv run python -m synth_rag.benchmark_ingest \\\n    --dataset emanual \\\n    --split all \\\n    --collection ragbench_emanual \\\n    --recreate-collection\n</code></pre>"},{"location":"benchmarking/#options","title":"Options","text":"Option Type Default Description <code>--dataset</code> str emanual RAGBench sub-dataset (emanual, covidqa, cuad, etc.) <code>--split</code> str all Dataset split (train, validation, test, all) <code>--collection</code> str ragbench_{dataset} Qdrant collection name <code>--recreate-collection</code> flag False Delete and recreate collection <code>--chunk-size</code> int 512 Max tokens per text chunk"},{"location":"benchmarking/#what-it-does","title":"What It Does","text":"<ol> <li>Loads the specified RAGBench dataset from Hugging Face</li> <li>Extracts documents from each example</li> <li>Chunks documents using semantic-text-splitter</li> <li>Generates dense (FastEmbed) and sparse (BM25) embeddings</li> <li>Uploads to Qdrant with metadata (example_id, question, document_idx, etc.)</li> </ol>"},{"location":"benchmarking/#output","title":"Output","text":"<pre><code>\u2713 Loaded train split: 1054 examples\n\u2713 Loaded validation split: 132 examples\n\u2713 Loaded test split: 132 examples\n\n\u2713 Total examples: 1318\nProcessing examples: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1318/1318\n\u2705 Ingestion complete! Total points: 15234\n\u2713 Collection 'ragbench_emanual' now has 15234 points\n</code></pre>"},{"location":"benchmarking/#step-2-running-the-benchmark","title":"Step 2: Running the Benchmark","text":""},{"location":"benchmarking/#command_1","title":"Command","text":"<pre><code>uv run python -m synth_rag.benchmark_runner \\\n    --dataset emanual \\\n    --split test \\\n    --collection ragbench_emanual \\\n    --model gpt-4o-mini \\\n    --top-k 5\n</code></pre>"},{"location":"benchmarking/#options_1","title":"Options","text":"Option Type Default Description <code>--dataset</code> str emanual RAGBench sub-dataset <code>--split</code> str test Dataset split to benchmark <code>--collection</code> str ragbench_{dataset} Qdrant collection name <code>--model</code> str gpt-4o-mini OpenAI model for response generation <code>--top-k</code> int 5 Number of contexts to retrieve <code>--prefetch-limit</code> int 50 Prefetch limit for reranking <code>--max-examples</code> int None Limit number of examples (for testing)"},{"location":"benchmarking/#what-it-does_1","title":"What It Does","text":"<p>For each example in the dataset:</p> <ol> <li>Retrieve contexts: Uses hybrid search (dense + sparse) to find top-k relevant chunks</li> <li>Generate response: Uses LLM with retrieved contexts to answer the question</li> <li>Track metrics: Records query time, generation time, retrieval scores</li> <li>Save results: Writes results as JSONL for later evaluation</li> </ol>"},{"location":"benchmarking/#output_1","title":"Output","text":"<p>Results are saved to <code>logs/benchmark_ragbench/{dataset_name}/</code>:</p> <ul> <li><code>{timestamp}_run_config.json</code> - Configuration and metadata</li> <li><code>{timestamp}_raw_results.jsonl</code> - Detailed results (one JSON object per line)</li> </ul> <p>Example result entry:</p> <pre><code>{\n  \"example_id\": \"emanual_265\",\n  \"question\": \"How do I select Motion Lighting?\",\n  \"ground_truth_documents\": [\"...\"],\n  \"ground_truth_response\": \"...\",\n  \"retrieved_contexts\": [\"...\", \"...\", \"...\"],\n  \"retrieval_scores\": [0.892, 0.856, 0.824],\n  \"retrieval_metadata\": [...],\n  \"query_time_seconds\": 0.234,\n  \"generated_response\": \"To select Motion Lighting...\",\n  \"generation_time_seconds\": 1.456,\n  \"total_time_seconds\": 1.690,\n  \"ground_truth_adherence_score\": true,\n  \"ground_truth_relevance_score\": 0.95,\n  \"ground_truth_utilization_score\": 0.88,\n  \"ground_truth_completeness_score\": 1.0\n}\n</code></pre>"},{"location":"benchmarking/#step-3-computing-metrics","title":"Step 3: Computing Metrics","text":""},{"location":"benchmarking/#command_2","title":"Command","text":"<pre><code>uv run python -m synth_rag.benchmark_metrics \\\n    --results-file logs/benchmark_ragbench/emanual/20251130_120000_raw_results.jsonl\n</code></pre>"},{"location":"benchmarking/#options_2","title":"Options","text":"Option Type Default Description <code>--results-file</code> Path Required Path to JSONL results file <code>--skip-ragas</code> flag False Skip RAGAS metrics computation <code>--skip-trulens</code> flag False Skip TruLens metrics computation <code>--max-concurrent</code> int 10 Max concurrent requests for TruLens"},{"location":"benchmarking/#what-it-does_2","title":"What It Does","text":"<ol> <li>Load results: Reads JSONL results file</li> <li>RAGAS evaluation: Computes faithfulness and context_relevancy</li> <li>TruLens evaluation: Computes groundedness and context_relevance (async)</li> <li>Aggregate metrics: Calculates means, standard deviations, AUROC, RMSE</li> <li>Save outputs: Exports detailed results, aggregate metrics, and summary CSV</li> </ol>"},{"location":"benchmarking/#metrics-computed","title":"Metrics Computed","text":""},{"location":"benchmarking/#ragas-metrics","title":"RAGAS Metrics","text":"<ul> <li>Faithfulness: Measures how grounded the generated response is in the retrieved contexts</li> <li>Context Relevancy: Measures how relevant the retrieved contexts are to the question</li> </ul>"},{"location":"benchmarking/#trulens-metrics","title":"TruLens Metrics","text":"<ul> <li>Groundedness: Similar to faithfulness, checks if response is supported by context</li> <li>Context Relevance: Evaluates if retrieved contexts contain information to answer the question</li> </ul>"},{"location":"benchmarking/#aggregate-metrics","title":"Aggregate Metrics","text":"<ul> <li>Hallucination AUROC: AUROC for hallucination detection using faithfulness</li> <li>Relevance RMSE: Root Mean Squared Error for context relevance predictions</li> <li>Performance: Mean query time, generation time, total time</li> </ul>"},{"location":"benchmarking/#output-files","title":"Output Files","text":"<pre><code>logs/benchmark_ragbench/emanual/\n\u251c\u2500\u2500 20251130_120000_run_config.json\n\u251c\u2500\u2500 20251130_120000_raw_results.jsonl\n\u251c\u2500\u2500 20251130_120000_detailed_results.jsonl  # \u2190 With computed metrics\n\u251c\u2500\u2500 20251130_120000_metrics.json             # \u2190 Aggregate metrics\n\u2514\u2500\u2500 20251130_120000_summary.csv              # \u2190 Summary table\n</code></pre> <p>Example <code>metrics.json</code>:</p> <pre><code>{\n  \"num_examples\": 132,\n  \"mean_query_time\": 0.234,\n  \"mean_generation_time\": 1.456,\n  \"mean_total_time\": 1.690,\n  \"ragas_faithfulness_mean\": 0.847,\n  \"ragas_faithfulness_std\": 0.123,\n  \"ragas_context_relevancy_mean\": 0.782,\n  \"ragas_context_relevancy_std\": 0.145,\n  \"trulens_groundedness_mean\": 0.823,\n  \"trulens_groundedness_std\": 0.156,\n  \"trulens_context_relevance_mean\": 0.791,\n  \"trulens_context_relevance_std\": 0.138,\n  \"hallucination_auroc_ragas\": 0.892,\n  \"relevance_rmse_ragas\": 0.145\n}\n</code></pre>"},{"location":"benchmarking/#supported-ragbench-datasets","title":"Supported RAGBench Datasets","text":"<p>The system supports all 12 RAGBench sub-datasets:</p> <ol> <li>emanual - E-manuals (user manuals)</li> <li>covidqa - COVID-19 QA</li> <li>cuad - Contract understanding</li> <li>delucionqa - Delucion QA</li> <li>expertqa - Expert QA</li> <li>finqa - Financial QA</li> <li>hagrid - HAGRID dataset</li> <li>hotpotqa - Multi-hop QA</li> <li>msmarco - MS MARCO</li> <li>pubmedqa - PubMed QA</li> <li>tatqa - Table + text QA</li> <li>techqa - Technical QA</li> </ol> <p>To benchmark a different dataset, simply change the <code>--dataset</code> parameter:</p> <pre><code># Example: Benchmark CovidQA\nuv run python -m synth_rag.benchmark_ingest \\\n    --dataset covidqa \\\n    --split all \\\n    --recreate-collection\n\nuv run python -m synth_rag.benchmark_runner \\\n    --dataset covidqa \\\n    --split test\n</code></pre>"},{"location":"benchmarking/#best-practices","title":"Best Practices","text":""},{"location":"benchmarking/#1-start-small","title":"1. Start Small","text":"<p>Test with a small subset before running full benchmarks:</p> <pre><code># Test with 10 examples first\nuv run python -m synth_rag.benchmark_runner \\\n    --dataset emanual \\\n    --split test \\\n    --max-examples 10\n</code></pre>"},{"location":"benchmarking/#2-monitor-api-usage","title":"2. Monitor API Usage","text":"<ul> <li>TruLens and RAGAS both make OpenAI API calls for evaluation</li> <li>Use <code>--max-concurrent</code> to control rate limits</li> <li>Consider costs when evaluating large datasets</li> </ul>"},{"location":"benchmarking/#3-incremental-evaluation","title":"3. Incremental Evaluation","text":"<p>You can skip metrics you've already computed:</p> <pre><code># Skip RAGAS if already computed\nuv run python -m synth_rag.benchmark_metrics \\\n    --results-file logs/benchmark_ragbench/emanual/20251130_120000_raw_results.jsonl \\\n    --skip-ragas\n</code></pre>"},{"location":"benchmarking/#4-experiment-with-parameters","title":"4. Experiment with Parameters","text":"<p>Test different retrieval parameters:</p> <pre><code># Higher top-k for more context\nuv run python -m synth_rag.benchmark_runner \\\n    --dataset emanual \\\n    --split test \\\n    --top-k 10 \\\n    --prefetch-limit 100\n</code></pre>"},{"location":"benchmarking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"benchmarking/#issue-collection-not-found","title":"Issue: Collection Not Found","text":"<pre><code>Error: Collection ragbench_emanual does not exist\n</code></pre> <p>Solution: Run ingestion first: <pre><code>uv run python -m synth_rag.benchmark_ingest --dataset emanual --split all\n</code></pre></p>"},{"location":"benchmarking/#issue-ragas-evaluation-fails","title":"Issue: RAGAS Evaluation Fails","text":"<pre><code>Error: Rate limit exceeded\n</code></pre> <p>Solution: Add delays or reduce batch size in RAGAS evaluation code, or wait and retry.</p>"},{"location":"benchmarking/#issue-trulens-timeout","title":"Issue: TruLens Timeout","text":"<pre><code>Error: TruLens evaluation timeout\n</code></pre> <p>Solution: Reduce <code>--max-concurrent</code> parameter: <pre><code>uv run python -m synth_rag.benchmark_metrics \\\n    --results-file ... \\\n    --max-concurrent 5\n</code></pre></p>"},{"location":"benchmarking/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<pre><code>Error: CUDA out of memory\n</code></pre> <p>Solution: The benchmarking system doesn't use ColPali, so this shouldn't occur. If it does, reduce batch sizes in the embedding models.</p>"},{"location":"benchmarking/#comparing-results","title":"Comparing Results","text":"<p>To compare different configurations, run benchmarks with different parameters:</p> <pre><code># Baseline: top-k=5\nuv run python -m synth_rag.benchmark_runner \\\n    --dataset emanual \\\n    --split test \\\n    --top-k 5\n\n# Experiment: top-k=10\nuv run python -m synth_rag.benchmark_runner \\\n    --dataset emanual \\\n    --split test \\\n    --top-k 10\n</code></pre> <p>Then compare the <code>summary.csv</code> files from each run.</p>"},{"location":"benchmarking/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture - Understand the system internals</li> <li>Usage Guide - Learn about the main synth-rag features</li> <li>API Reference - Explore the codebase</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+</li> <li>uv package manager</li> <li>API keys for Qdrant, OpenAI, and Brave Search</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<ol> <li>Clone and install:</li> </ol> <pre><code>git clone https://github.com/adbX/synth-rag.git\ncd synth-rag\nuv sync\n</code></pre> <ol> <li>Create <code>.env</code> file:</li> </ol> <pre><code>cat &lt;&lt;'EOF' &gt; .env\nQDRANT_URL=\"https://&lt;your-qdrant-cluster&gt;\"\nQDRANT_KEY=\"&lt;your-api-key&gt;\"\nOPENAI_API_KEY=\"&lt;your-openai-key&gt;\"\nBRAVE_API_KEY=\"&lt;your-brave-key&gt;\"\nEOF\n</code></pre> <ol> <li>Verify installation:</li> </ol> <pre><code>uv run python -c \"import torch; print('Torch:', torch.__version__)\"\nuv run python -c \"from colpali_engine.models import ColPali; print('ColPali ready')\"\nuv run python -c \"from qdrant_client import QdrantClient; print('Qdrant client ok')\"\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<pre><code>flowchart LR\n    A[Install Dependencies] --&gt; B[Configure API Keys]\n    B --&gt; C[Ingest Test Manuals]\n    C --&gt; D[Query Manuals]\n    D --&gt; E[Use Agent]\n\n    style A fill:#e3f2fd\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n    style D fill:#e8f5e9\n    style E fill:#fce4ec</code></pre>"},{"location":"quickstart/#step-1-ingest-test-manuals","title":"Step 1: Ingest Test Manuals","text":"<pre><code>uv run python -m synth_rag.manuals_ingest \\\n    --subset test \\\n    --collection midi_manuals \\\n    --device mps \\\n    --recreate-collection \\\n    --clear-tmp\n</code></pre> <p>Device Selection</p> <ul> <li><code>--device mps</code> for Apple Silicon (M1/M2/M3)</li> <li><code>--device cuda:0</code> for NVIDIA GPUs</li> <li><code>--device cpu</code> as fallback</li> </ul>"},{"location":"quickstart/#step-2-query-the-manuals","title":"Step 2: Query the Manuals","text":"<pre><code>uv run python -m synth_rag.manuals_query \\\n    --question \"How do I adjust reverb settings on the Digitone II?\" \\\n    --collection midi_manuals \\\n    --top-k 5 \\\n    --device mps\n</code></pre>"},{"location":"quickstart/#step-3-use-the-agent","title":"Step 3: Use the Agent","text":"<pre><code>uv run python -m synth_rag.manuals_agent \\\n    --question \"What are the key differences between Digitakt and Digitone?\" \\\n    --collection midi_manuals \\\n    --model gpt-4o-mini \\\n    --device mps\n</code></pre>"},{"location":"quickstart/#benchmarking","title":"Benchmarking","text":"<p>Ingesting the RAGBench <code>emanual</code> dataset:</p> <pre><code>uv run python -m synth_rag.benchmark_ingest \\\n    --dataset emanual \\\n    --split all \\\n    --collection ragbench_emanual \\\n    --recreate-collection\n</code></pre> <p>Evaluating the results of the RAGBench <code>emanual</code> dataset:</p> <p>More information on benchmarking can be found on the Benchmarking page.</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Setup Guide - Detailed configuration</li> <li>Usage Guide - Explore all features</li> <li>Architecture - Understand the system</li> </ul>"},{"location":"setup/","title":"Setup Guide","text":""},{"location":"setup/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.13 or higher</li> <li>Package Manager: uv</li> <li>GPU (optional): Apple Silicon (MPS) or NVIDIA GPU (CUDA)</li> </ul>"},{"location":"setup/#installation","title":"Installation","text":""},{"location":"setup/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/adbX/synth-rag.git\ncd synth-rag\n</code></pre>"},{"location":"setup/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>uv sync\n</code></pre>"},{"location":"setup/#3-configure-api-keys","title":"3. Configure API Keys","text":"<p>Create a <code>.env</code> file:</p> <pre><code># Qdrant Vector Database\nQDRANT_URL=\"https://xyz-example.eu-central.aws.cloud.qdrant.io:6333\"\nQDRANT_KEY=\"your-qdrant-api-key-here\"\n\n# OpenAI (for LLM)\nOPENAI_API_KEY=\"sk-your-openai-key-here\"\n\n# Brave Search (for web search)\nBRAVE_API_KEY=\"your-brave-search-key-here\"\n</code></pre>"},{"location":"setup/#getting-api-keys","title":"Getting API Keys","text":"QdrantOpenAIBrave Search <p>Sign up at cloud.qdrant.io</p> <p>Visit platform.openai.com</p> <p>Go to brave.com/search/api</p>"},{"location":"setup/#directory-structure","title":"Directory Structure","text":"<pre><code>graph TD\n    Root[synth-rag/]\n\n    Root --&gt; Env[.env&lt;br/&gt;API keys - you create this]\n    Root --&gt; Pyproject[pyproject.toml&lt;br/&gt;Python dependencies]\n    Root --&gt; Lock[uv.lock&lt;br/&gt;Locked dependencies]\n\n    Root --&gt; Src[src/]\n    Src --&gt; SynthRag[synth_rag/]\n    SynthRag --&gt; Settings[settings.py]\n    SynthRag --&gt; Ingest[manuals_ingest.py]\n    SynthRag --&gt; Query[manuals_query.py]\n    SynthRag --&gt; Agent[manuals_agent.py]\n    SynthRag --&gt; UI[manuals_ui.py]\n\n    Root --&gt; Docs[documents/]\n    Docs --&gt; MIDI[midi_synthesizers/]\n    MIDI --&gt; Input[input/]\n    Input --&gt; Test[test/&lt;br/&gt;3 test PDFs]\n    Input --&gt; Full[full/&lt;br/&gt;8 full PDFs]\n    MIDI --&gt; Tmp[tmp/&lt;br/&gt;\u2699\ufe0f auto-created]\n    Tmp --&gt; Pages[pages/&lt;br/&gt;rendered images]\n    Tmp --&gt; Text[text/&lt;br/&gt;extracted text]\n\n    Root --&gt; Logs[logs/&lt;br/&gt;\u2699\ufe0f auto-created]\n    Logs --&gt; QueryLogs[manuals_queries/&lt;br/&gt;query logs]\n\n    style Env fill:#fff3e0\n    style Tmp fill:#e8f5e9\n    style Logs fill:#e8f5e9\n    style Settings fill:#e3f2fd\n    style Ingest fill:#f3e5f5\n    style Query fill:#f3e5f5\n    style Agent fill:#f3e5f5\n    style UI fill:#f3e5f5</code></pre>"},{"location":"setup/#device-configuration","title":"Device Configuration","text":""},{"location":"setup/#apple-silicon-m1m2m3","title":"Apple Silicon (M1/M2/M3)","text":"<pre><code>--device mps\n</code></pre>"},{"location":"setup/#nvidia-gpu","title":"NVIDIA GPU","text":"<pre><code>--device cuda:0\n</code></pre>"},{"location":"setup/#cpu-only","title":"CPU Only","text":"<pre><code>--device cpu\n</code></pre>"},{"location":"setup/#model-downloads","title":"Model Downloads","text":"<pre><code>graph LR\n    subgraph Download[Auto-Download on First Run]\n        ColPali[vidore/colpali-v1.3&lt;br/&gt;~2GB&lt;br/&gt;Vision-Language Model]\n        MiniLM[sentence-transformers/&lt;br/&gt;all-MiniLM-L6-v2&lt;br/&gt;~90MB&lt;br/&gt;Dense Embeddings]\n        BM25[Qdrant/bm25&lt;br/&gt;&lt;1MB&lt;br/&gt;Sparse Embeddings]\n    end\n\n    Cache[~/.cache/huggingface/]\n\n    ColPali --&gt; Cache\n    MiniLM --&gt; Cache\n    BM25 --&gt; Cache\n\n    style ColPali fill:#ffccbc\n    style MiniLM fill:#c5e1a5\n    style BM25 fill:#b2ebf2\n    style Cache fill:#fff9c4</code></pre> <p>Model Summary:</p> Model Size Purpose <code>vidore/colpali-v1.3</code> ~2GB ColPali vision-language model <code>sentence-transformers/all-MiniLM-L6-v2</code> ~90MB Dense text embeddings <code>Qdrant/bm25</code> &lt;1MB Sparse keyword embeddings"},{"location":"setup/#next-steps","title":"Next Steps","text":"<ul> <li>Usage Guide - Learn how to use each component</li> <li>Quickstart - Run your first queries</li> <li>Architecture - Understand the system design</li> </ul>"},{"location":"usage/","title":"Usage Guide","text":""},{"location":"usage/#overview","title":"Overview","text":"<p>Synth-RAG provides three main interfaces:</p> <ol> <li>Ingestion (<code>manuals_ingest.py</code>) - Index PDF manuals into Qdrant</li> <li>Query (<code>manuals_query.py</code>) - Hybrid search over manuals</li> <li>Agent (<code>manuals_agent.py</code>) - Agentic RAG with web search fallback</li> </ol>"},{"location":"usage/#1-ingesting-pdf-manuals","title":"1. Ingesting PDF Manuals","text":""},{"location":"usage/#basic-ingestion","title":"Basic Ingestion","text":"<pre><code>uv run python -m synth_rag.manuals_ingest \\\n    --subset test \\\n    --collection midi_manuals \\\n    --device cpu \\\n    --recreate-collection \\\n    --clear-tmp\n</code></pre> <pre><code>uv run python -m synth_rag.manuals_ingest \\\n    --subset test \\\n    --collection midi_manuals \\\n    --device mps \\\n    --recreate-collection \\\n    --clear-tmp\n</code></pre>"},{"location":"usage/#options","title":"Options","text":"<pre><code>uv run python -m synth_rag.manuals_ingest \\\n    --subset {test,full} \\           # Which PDF subset to ingest\n    --collection NAME \\               # Qdrant collection name\n    --device {mps,cuda:0,cpu} \\      # Compute device\n    --batch-size N \\                  # Batch size for ColPali (default: 4)\n    --clear-tmp \\                     # Clear tmp directories first\n    --recreate-collection             # Delete &amp; recreate collection\n</code></pre>"},{"location":"usage/#ingestion-pipeline","title":"Ingestion Pipeline","text":"<pre><code>graph TD\n    Start[PDF Files] --&gt; Stage1[1. PDF Rendering&lt;br/&gt;pypdfium2&lt;br/&gt;RGB images]\n    Start --&gt; Stage2[2. Text Extraction&lt;br/&gt;pymupdf&lt;br/&gt;per-page text]\n\n    Stage1 --&gt; Stage3[3. ColPali Embeddings&lt;br/&gt;Original + mean-pooled&lt;br/&gt;rows/cols]\n    Stage2 --&gt; Stage4[4. Text Embeddings&lt;br/&gt;Dense FastEmbed&lt;br/&gt;+ Sparse BM25]\n\n    Stage3 --&gt; Stage5[5. Upload to Qdrant&lt;br/&gt;All embeddings&lt;br/&gt;+ metadata]\n    Stage4 --&gt; Stage5\n\n    Stage5 --&gt; Complete[\u2713 Indexed]\n\n    style Start fill:#e3f2fd\n    style Stage1 fill:#f3e5f5\n    style Stage2 fill:#f3e5f5\n    style Stage3 fill:#fff3e0\n    style Stage4 fill:#e8f5e9\n    style Stage5 fill:#ffccbc\n    style Complete fill:#c8e6c9</code></pre> <p>Ingestion Steps:</p> <ol> <li>PDF Rendering: Each page is rendered to RGB image using <code>pypdfium2</code></li> <li>Text Extraction: Per-page text extracted with <code>pymupdf</code></li> <li>ColPali Embeddings: Original multivectors + mean-pooled rows/cols</li> <li>Text Embeddings: Dense (FastEmbed) + sparse (BM25) vectors</li> <li>Upload: All embeddings and metadata uploaded to Qdrant</li> </ol>"},{"location":"usage/#2-querying-manuals","title":"2. Querying Manuals","text":""},{"location":"usage/#basic-query","title":"Basic Query","text":"<pre><code>uv run python -m synth_rag.manuals_query \\\n    --question \"How do I set up MIDI channels?\" \\\n    --collection midi_manuals \\\n    --device mps\n</code></pre>"},{"location":"usage/#options_1","title":"Options","text":"<pre><code>uv run python -m synth_rag.manuals_query \\\n    --question \"Your question here\" \\     # Query (required)\n    --collection NAME \\                    # Collection name\n    --top-k N \\                           # Results to return (default: 5)\n    --prefetch-limit N \\                  # Prefetch for reranking (default: 50)\n    --device {mps,cuda:0,cpu} \\           # Device for ColPali\n    --manual-filter NAME                  # Filter by manual name (optional)\n</code></pre>"},{"location":"usage/#example-filter-by-manual","title":"Example: Filter by Manual","text":"<pre><code>uv run python -m synth_rag.manuals_query \\\n    --question \"How does the reverb work?\" \\\n    --manual-filter \"Digitone\" \\\n    --collection midi_manuals\n</code></pre>"},{"location":"usage/#query-logs","title":"Query Logs","text":"<p>Queries are automatically logged to <code>logs/manuals_queries/&lt;timestamp&gt;.json</code>.</p>"},{"location":"usage/#3-agentic-rag","title":"3. Agentic RAG","text":""},{"location":"usage/#basic-agent-usage","title":"Basic Agent Usage","text":"<pre><code>uv run python -m synth_rag.manuals_agent \\\n    --question \"What are the differences between Digitakt and Digitone?\" \\\n    --collection midi_manuals \\\n    --device mps\n</code></pre>"},{"location":"usage/#options_2","title":"Options","text":"<pre><code>uv run python -m synth_rag.manuals_agent \\\n    --question \"Your question here\" \\     # Query (required)\n    --collection NAME \\                    # Collection name\n    --model MODEL_NAME \\                   # OpenAI model (default: gpt-4o-mini)\n    --device {mps,cuda:0,cpu} \\           # Device for ColPali\n    --top-k N                             # Results per retrieval (default: 3)\n</code></pre>"},{"location":"usage/#how-the-agent-works","title":"How the Agent Works","text":"<ol> <li>Always queries manuals first (no exceptions)</li> <li>Falls back to web search only if manual search fails</li> <li>Provides citations with manual names and page numbers</li> <li>Structures responses with clear sections</li> </ol>"},{"location":"usage/#4-gradio-web-ui","title":"4. Gradio Web UI","text":""},{"location":"usage/#launch-ui","title":"Launch UI","text":"<pre><code>uv run python -m synth_rag.manuals_ui\n</code></pre> <p>Opens a browser at <code>http://localhost:7860</code> with a chat interface.</p>"},{"location":"usage/#common-workflows","title":"Common Workflows","text":""},{"location":"usage/#workflow-1-first-time-setup","title":"Workflow 1: First-Time Setup","text":"<pre><code>flowchart LR\n    A[1. Ingest&lt;br/&gt;Test Data] --&gt; B[2. Test&lt;br/&gt;Query]\n    B --&gt; C[3. Try&lt;br/&gt;Agent]\n\n    style A fill:#e8f5e9\n    style B fill:#e3f2fd\n    style C fill:#f3e5f5</code></pre> <pre><code># 1. Ingest test data\nuv run python -m synth_rag.manuals_ingest \\\n    --subset test \\\n    --collection midi_manuals \\\n    --recreate-collection\n\n# 2. Test query\nuv run python -m synth_rag.manuals_query \\\n    --question \"How many tracks does the Digitone II have?\"\n\n# 3. Try agent\nuv run python -m synth_rag.manuals_agent \\\n    --question \"Explain the Digitone II's FM synthesis engine\"\n</code></pre>"},{"location":"usage/#workflow-2-update-collection","title":"Workflow 2: Update Collection","text":"<pre><code># Clear old data and re-ingest\nuv run python -m synth_rag.manuals_ingest \\\n    --subset full \\\n    --collection midi_manuals \\\n    --recreate-collection \\\n    --clear-tmp\n</code></pre>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture - Understand the internals</li> <li>API Reference - Explore the codebase</li> </ul>"},{"location":"api/manuals_agent/","title":"Agent Module","text":"<p>Agentic RAG for PDF manuals using LangGraph, Qdrant, and Brave Search.</p>"},{"location":"api/manuals_agent/#overview","title":"Overview","text":"<p>The <code>manuals_agent</code> module implements an autonomous agent that:</p> <ol> <li>Queries local manuals first (always, no exceptions)</li> <li>Falls back to web search if needed</li> <li>Generates cited, grounded answers</li> <li>Handles multi-step reasoning</li> </ol> <p>Powered by LangGraph and OpenAI GPT-4o-mini.</p>"},{"location":"api/manuals_agent/#command-line-interface","title":"Command-Line Interface","text":"<pre><code>uv run python -m synth_rag.manuals_agent [OPTIONS]\n</code></pre>"},{"location":"api/manuals_agent/#options","title":"Options","text":"Option Type Default Description <code>--question</code> str required Question to ask <code>--collection</code> str <code>midi_manuals</code> Qdrant collection name <code>--model</code> str <code>gpt-4o-mini</code> OpenAI model name <code>--device</code> choice <code>mps</code> Device for ColPali (<code>mps</code>, <code>cuda:0</code>, <code>cpu</code>) <code>--top-k</code> int <code>3</code> Results per retrieval"},{"location":"api/manuals_agent/#examples","title":"Examples","text":"<pre><code># Basic agent query\nuv run python -m synth_rag.manuals_agent \\\n    --question \"How do I set up MIDI channels on the Digitone II?\"\n\n# Use more expensive model\nuv run python -m synth_rag.manuals_agent \\\n    --question \"Explain the FM synthesis engine in detail\" \\\n    --model gpt-4o\n</code></pre>"},{"location":"api/manuals_agent/#module-reference","title":"Module Reference","text":""},{"location":"api/manuals_agent/#synth_rag.manuals_agent","title":"<code>manuals_agent</code>","text":"<p>Agentic RAG for PDF manuals using LangGraph, Qdrant, and Brave Search.</p>"},{"location":"api/manuals_agent/#synth_rag.manuals_agent.ManualsRetriever","title":"<code>ManualsRetriever</code>","text":"<p>Retriever for PDF manuals using hybrid search + ColPali reranking.</p> Source code in <code>src/synth_rag/manuals_agent.py</code> <pre><code>class ManualsRetriever:\n    \"\"\"Retriever for PDF manuals using hybrid search + ColPali reranking.\"\"\"\n\n    def __init__(self, collection_name: str, device: str, top_k: int = 3):\n        self.collection_name = collection_name\n        self.device = device\n        self.top_k = top_k\n        self.client = get_qdrant_client()\n\n        # Load models lazily\n        self._colpali_model = None\n        self._colpali_processor = None\n        self._dense_model = None\n        self._sparse_model = None\n\n    def _load_models(self):\n        \"\"\"Lazy load models on first use.\"\"\"\n        if self._colpali_model is None:\n            try:\n                logger.info(\"Loading ColPali model...\")\n                self._colpali_model = ColPali.from_pretrained(\n                    \"vidore/colpali-v1.3\",\n                    torch_dtype=torch.bfloat16,\n                    device_map=self.device,\n                ).eval()\n                self._colpali_processor = ColPaliProcessor.from_pretrained(\"vidore/colpali-v1.3\")\n\n                logger.info(\"Loading FastEmbed models...\")\n                self._dense_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n                self._sparse_model = SparseTextEmbedding(\"Qdrant/bm25\")\n                logger.info(\"All models loaded successfully\")\n            except Exception as e:\n                logger.error(f\"Failed to load models: {e}\", exc_info=True)\n                raise RuntimeError(f\"Model loading failed: {type(e).__name__}: {e}\")\n\n    def retrieve(self, query: str) -&gt; list[dict]:\n        \"\"\"Retrieve relevant manual pages for a query.\"\"\"\n        logger.info(f\"Retrieving results for query: {query[:100]}...\")\n\n        try:\n            self._load_models()\n        except Exception as e:\n            logger.error(f\"Model loading failed during retrieve: {e}\")\n            raise\n\n        try:\n            # Generate embeddings\n            logger.debug(\"Generating dense embedding...\")\n            dense_embedding = list(self._dense_model.embed([query]))[0]\n\n            logger.debug(\"Generating sparse embedding...\")\n            sparse_embedding = list(self._sparse_model.embed([query]))[0]\n\n            logger.debug(\"Generating ColPali embedding...\")\n            processed_query = self._colpali_processor.process_queries([query]).to(self.device)\n            with torch.no_grad():\n                query_embedding = self._colpali_model(**processed_query)[0]\n            query_embedding_list = query_embedding.cpu().float().numpy().tolist()\n        except Exception as e:\n            logger.error(f\"Embedding generation failed: {e}\", exc_info=True)\n            raise RuntimeError(f\"Embedding generation failed: {type(e).__name__}: {e}\")\n\n        try:\n            # Hybrid search with reranking\n            logger.info(f\"Querying Qdrant collection '{self.collection_name}'...\")\n            response = self.client.query_points(\n                collection_name=self.collection_name,\n                query=query_embedding_list,\n                prefetch=[\n                    models.Prefetch(\n                        query=dense_embedding.tolist(),\n                        limit=50,\n                        using=\"dense\",\n                    ),\n                    models.Prefetch(\n                        query=sparse_embedding.as_object(),\n                        limit=50,\n                        using=\"sparse\",\n                    ),\n                    models.Prefetch(\n                        query=query_embedding_list,\n                        limit=50,\n                        using=\"colpali_rows\",\n                    ),\n                    models.Prefetch(\n                        query=query_embedding_list,\n                        limit=50,\n                        using=\"colpali_cols\",\n                    ),\n                ],\n                limit=self.top_k,\n                using=\"colpali_original\",\n                with_payload=True,\n            )\n            logger.info(f\"Qdrant returned {len(response.points)} results\")\n        except Exception as e:\n            logger.error(f\"Qdrant query failed: {e}\", exc_info=True)\n            raise RuntimeError(f\"Qdrant query failed: {type(e).__name__}: {e}\")\n\n        # Format results\n        results = []\n        for point in response.points:\n            results.append({\n                \"manual_name\": point.payload.get(\"manual_name\"),\n                \"page_num\": point.payload.get(\"page_num\"),\n                \"text\": point.payload.get(\"full_text\", point.payload.get(\"text\", \"\")),\n                \"score\": point.score,\n            })\n\n        return results\n</code></pre>"},{"location":"api/manuals_agent/#synth_rag.manuals_agent.ManualsRetriever.retrieve","title":"<code>retrieve(query)</code>","text":"<p>Retrieve relevant manual pages for a query.</p> Source code in <code>src/synth_rag/manuals_agent.py</code> <pre><code>def retrieve(self, query: str) -&gt; list[dict]:\n    \"\"\"Retrieve relevant manual pages for a query.\"\"\"\n    logger.info(f\"Retrieving results for query: {query[:100]}...\")\n\n    try:\n        self._load_models()\n    except Exception as e:\n        logger.error(f\"Model loading failed during retrieve: {e}\")\n        raise\n\n    try:\n        # Generate embeddings\n        logger.debug(\"Generating dense embedding...\")\n        dense_embedding = list(self._dense_model.embed([query]))[0]\n\n        logger.debug(\"Generating sparse embedding...\")\n        sparse_embedding = list(self._sparse_model.embed([query]))[0]\n\n        logger.debug(\"Generating ColPali embedding...\")\n        processed_query = self._colpali_processor.process_queries([query]).to(self.device)\n        with torch.no_grad():\n            query_embedding = self._colpali_model(**processed_query)[0]\n        query_embedding_list = query_embedding.cpu().float().numpy().tolist()\n    except Exception as e:\n        logger.error(f\"Embedding generation failed: {e}\", exc_info=True)\n        raise RuntimeError(f\"Embedding generation failed: {type(e).__name__}: {e}\")\n\n    try:\n        # Hybrid search with reranking\n        logger.info(f\"Querying Qdrant collection '{self.collection_name}'...\")\n        response = self.client.query_points(\n            collection_name=self.collection_name,\n            query=query_embedding_list,\n            prefetch=[\n                models.Prefetch(\n                    query=dense_embedding.tolist(),\n                    limit=50,\n                    using=\"dense\",\n                ),\n                models.Prefetch(\n                    query=sparse_embedding.as_object(),\n                    limit=50,\n                    using=\"sparse\",\n                ),\n                models.Prefetch(\n                    query=query_embedding_list,\n                    limit=50,\n                    using=\"colpali_rows\",\n                ),\n                models.Prefetch(\n                    query=query_embedding_list,\n                    limit=50,\n                    using=\"colpali_cols\",\n                ),\n            ],\n            limit=self.top_k,\n            using=\"colpali_original\",\n            with_payload=True,\n        )\n        logger.info(f\"Qdrant returned {len(response.points)} results\")\n    except Exception as e:\n        logger.error(f\"Qdrant query failed: {e}\", exc_info=True)\n        raise RuntimeError(f\"Qdrant query failed: {type(e).__name__}: {e}\")\n\n    # Format results\n    results = []\n    for point in response.points:\n        results.append({\n            \"manual_name\": point.payload.get(\"manual_name\"),\n            \"page_num\": point.payload.get(\"page_num\"),\n            \"text\": point.payload.get(\"full_text\", point.payload.get(\"text\", \"\")),\n            \"score\": point.score,\n        })\n\n    return results\n</code></pre>"},{"location":"api/manuals_agent/#synth_rag.manuals_agent.create_manuals_retriever_tool","title":"<code>create_manuals_retriever_tool(retriever)</code>","text":"<p>Create a LangChain tool for the manuals retriever.</p> Source code in <code>src/synth_rag/manuals_agent.py</code> <pre><code>def create_manuals_retriever_tool(retriever: ManualsRetriever):\n    \"\"\"Create a LangChain tool for the manuals retriever.\"\"\"\n\n    @tool(\"manuals_retriever_tool\")\n    def retrieve_manuals(query: str) -&gt; str:\n        \"\"\"\n        PRIMARY TOOL - MUST be called FIRST for every question.\n        Search PDF manuals for MIDI synthesizer information including features, settings, operations, and technical details.\n        This tool searches indexed PDF manuals and returns relevant pages with page numbers for citation.\n        \"\"\"\n        try:\n            results = retriever.retrieve(query)\n\n            if not results:\n                logger.warning(f\"No results found for query: {query[:100]}\")\n                return \"No relevant information found in the manuals.\"\n\n            # Format results as text\n            output = []\n            for i, result in enumerate(results, 1):\n                output.append(\n                    f\"[{i}] {result['manual_name']} (Page {result['page_num']}, Score: {result['score']:.3f})\\n\"\n                    f\"{result['text'][:800]}\\n\"\n                )\n\n            logger.info(f\"Successfully retrieved {len(results)} results\")\n            return \"\\n---\\n\".join(output)\n        except Exception as e:\n            error_msg = f\"ERROR: Failed to search manuals - {type(e).__name__}: {e}\"\n            logger.error(f\"Tool execution failed: {e}\", exc_info=True)\n            return error_msg\n\n    return retrieve_manuals\n</code></pre>"},{"location":"api/manuals_agent/#synth_rag.manuals_agent.create_brave_search_tool","title":"<code>create_brave_search_tool()</code>","text":"<p>Create a Brave Search tool.</p> Source code in <code>src/synth_rag/manuals_agent.py</code> <pre><code>def create_brave_search_tool():\n    \"\"\"Create a Brave Search tool.\"\"\"\n    from langchain_community.tools import BraveSearch\n\n    api_settings = get_api_settings()\n\n    @tool(\"web_search_tool\")\n    def search_web(query: str) -&gt; str:\n        \"\"\"\n        SECONDARY TOOL - Only use AFTER manuals_retriever_tool has been called.\n        Search the web for supplementary information about MIDI synthesizers or music production.\n        Use only when manuals don't have sufficient information or for general context.\n        \"\"\"\n        search = BraveSearch.from_api_key(\n            api_key=api_settings.brave_key,\n            search_kwargs={\"count\": 3}\n        )\n        return search.run(query)\n\n    return search_web\n</code></pre>"},{"location":"api/manuals_agent/#synth_rag.manuals_agent.create_agent_graph","title":"<code>create_agent_graph(collection_name, device, top_k, model)</code>","text":"<p>Create the LangGraph agent.</p> Source code in <code>src/synth_rag/manuals_agent.py</code> <pre><code>def create_agent_graph(\n    collection_name: str,\n    device: str,\n    top_k: int,\n    model: str,\n):\n    \"\"\"Create the LangGraph agent.\"\"\"\n\n    # Create tools\n    retriever = ManualsRetriever(collection_name, device, top_k)\n    manuals_tool = create_manuals_retriever_tool(retriever)\n    brave_tool = create_brave_search_tool()\n\n    tools = [manuals_tool, brave_tool]\n    tool_node = ToolNode(tools=tools)\n\n    # Create LLM with tools\n    api_settings = get_api_settings()\n    os.environ[\"OPENAI_API_KEY\"] = api_settings.openai_key\n\n    llm = ChatOpenAI(model=model, temperature=0)\n    llm_with_tools = llm.bind_tools(tools)\n\n    # Define agent node\n    def agent(state: State):\n        messages = state[\"messages\"]\n\n        # Prepend system message if not already present\n        if not messages or not isinstance(messages[0], SystemMessage):\n            messages = [SystemMessage(content=SYSTEM_PROMPT)] + messages\n\n        response = llm_with_tools.invoke(messages)\n        return {\"messages\": [response]}\n\n    # Define routing function\n    def route(state: State):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n\n        if hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) &gt; 0:\n            return \"tools\"\n\n        return END\n\n    # Build graph\n    graph_builder = StateGraph(State)\n\n    graph_builder.add_node(\"agent\", agent)\n    graph_builder.add_node(\"tools\", tool_node)\n\n    graph_builder.add_conditional_edges(\n        \"agent\",\n        route,\n        {\"tools\": \"tools\", END: END},\n    )\n\n    graph_builder.add_edge(\"tools\", \"agent\")\n    graph_builder.add_edge(START, \"agent\")\n\n    return graph_builder.compile()\n</code></pre>"},{"location":"api/manuals_agent/#synth_rag.manuals_agent.run_agent","title":"<code>run_agent(question, collection_name, model, device, top_k)</code>","text":"<p>Run the agent with a question.</p> Source code in <code>src/synth_rag/manuals_agent.py</code> <pre><code>def run_agent(question: str, collection_name: str, model: str, device: str, top_k: int):\n    \"\"\"Run the agent with a question.\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"Question: {question}\")\n    print(f\"Collection: {collection_name}\")\n    print(f\"Model: {model}\")\n    print(f\"{'='*80}\\n\")\n\n    # Create and run agent\n    graph = create_agent_graph(collection_name, device, top_k, model)\n\n    print(\"Agent is thinking...\\n\")\n\n    for event in graph.stream({\"messages\": [(\"user\", question)]}):\n        for value in event.values():\n            last_message = value[\"messages\"][-1]\n\n            # Print tool calls\n            if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n                for tool_call in last_message.tool_calls:\n                    print(f\"\ud83d\udd27 Using tool: {tool_call['name']}\")\n                    print(f\"   Args: {json.dumps(tool_call['args'], indent=2)}\\n\")\n\n            # Print tool responses\n            elif isinstance(last_message, ToolMessage):\n                print(f\"\ud83d\udcc4 Tool response from {last_message.name}:\")\n                content = last_message.content\n                if len(content) &gt; 500:\n                    print(f\"   {content[:500]}...\\n\")\n                else:\n                    print(f\"   {content}\\n\")\n\n            # Print final answer\n            elif hasattr(last_message, \"content\") and last_message.content:\n                if not hasattr(last_message, \"tool_calls\") or not last_message.tool_calls:\n                    print(f\"\\n{'='*80}\")\n                    print(\"\ud83e\udd16 Final Answer:\")\n                    print(f\"{'='*80}\")\n                    print(last_message.content)\n                    print(f\"{'='*80}\\n\")\n</code></pre>"},{"location":"api/manuals_agent/#agent-architecture","title":"Agent Architecture","text":""},{"location":"api/manuals_agent/#langgraph-state-machine","title":"LangGraph State Machine","text":"<pre><code>from langgraph.graph import StateGraph, START, END\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\nworkflow.add_edge(START, \"agent\")\nworkflow.add_conditional_edges(\"agent\", should_continue)\nworkflow.add_edge(\"tools\", \"agent\")\nworkflow.add_edge(\"agent\", END)\n</code></pre>"},{"location":"api/manuals_agent/#flow-diagram","title":"Flow Diagram","text":"<pre><code>flowchart TD\n    Start([START]) --&gt; Agent[Agent&lt;br/&gt;LLM]\n    Agent --&gt; Decision{Need Tool?}\n    Decision --&gt;|YES| Tools[Tools&lt;br/&gt;Execute]\n    Decision --&gt;|NO| End([END])\n    Tools --&gt; Agent\n\n    style Start fill:#e8f5e9\n    style Agent fill:#e3f2fd\n    style Tools fill:#fff3e0\n    style End fill:#ffcdd2</code></pre>"},{"location":"api/manuals_agent/#agent-tools","title":"Agent Tools","text":""},{"location":"api/manuals_agent/#1-manual-retriever-tool","title":"1. Manual Retriever Tool","text":"<pre><code>@tool\ndef manuals_retriever_tool(query: str) -&gt; str:\n    \"\"\"Retrieve information from MIDI synthesizer manuals.\"\"\"\n    results = search_manuals(query)\n    return format_results_with_citations(results)\n</code></pre>"},{"location":"api/manuals_agent/#2-web-search-tool","title":"2. Web Search Tool","text":"<pre><code>@tool\ndef web_search_tool(query: str) -&gt; str:\n    \"\"\"Search the web using Brave Search API.\"\"\"\n    search = BraveSearch.from_api_key(api_key=brave_api_key)\n    return search.run(query)\n</code></pre>"},{"location":"api/manuals_agent/#system-prompt","title":"System Prompt","text":"<p>The agent is guided by a strict system prompt that ensures:</p> <ol> <li>Always call manuals first - No exceptions</li> <li>Cite sources - Format: <code>(Manual Name, Page X)</code></li> <li>Structure responses - Manuals section, then web results</li> <li>Use web as fallback - Only if manual search fails</li> </ol>"},{"location":"api/manuals_agent/#execution-flow","title":"Execution Flow","text":"<pre><code>sequenceDiagram\n    actor User\n    participant Agent\n    participant ManualTool as Manual Retriever\n    participant WebTool as Web Search\n\n    User-&gt;&gt;Agent: Submit question\n    activate Agent\n    Agent-&gt;&gt;Agent: Analyze question\n\n    Note over Agent,ManualTool: ALWAYS call manuals first\n    Agent-&gt;&gt;ManualTool: Query manuals\n    activate ManualTool\n    ManualTool--&gt;&gt;Agent: Return results\n    deactivate ManualTool\n\n    Agent-&gt;&gt;Agent: Process manual results\n\n    alt Insufficient information\n        Agent-&gt;&gt;WebTool: Search web\n        activate WebTool\n        WebTool--&gt;&gt;Agent: Return web results\n        deactivate WebTool\n    end\n\n    Agent-&gt;&gt;Agent: Synthesize answer&lt;br/&gt;with citations\n    Agent--&gt;&gt;User: Return structured response\n    deactivate Agent</code></pre> <p>Execution Steps:</p> <ol> <li>User submits question</li> <li>Agent analyzes question</li> <li>Agent calls <code>manuals_retriever_tool</code> first</li> <li>Agent processes manual results</li> <li>If insufficient, agent calls <code>web_search_tool</code></li> <li>Agent synthesizes final answer with citations</li> <li>Agent returns structured response</li> </ol>"},{"location":"api/manuals_agent/#configuration","title":"Configuration","text":""},{"location":"api/manuals_agent/#model-selection","title":"Model Selection","text":"<pre><code>graph LR\n    subgraph Fast[Fast &amp; Economical]\n        Mini[gpt-4o-mini&lt;br/&gt;\u26a1 Fast&lt;br/&gt;\ud83d\udcb0 Low Cost&lt;br/&gt;\u2713 Good Quality]\n    end\n\n    subgraph Balanced[Balanced]\n        GPT4o[gpt-4o&lt;br/&gt;\u2699\ufe0f Medium Speed&lt;br/&gt;\ud83d\udcb5 Medium Cost&lt;br/&gt;\u2713\u2713 Better Quality]\n    end\n\n    subgraph Premium[Premium]\n        Turbo[gpt-4-turbo&lt;br/&gt;\ud83d\udc22 Slower&lt;br/&gt;\ud83d\udc8e High Cost&lt;br/&gt;\u2713\u2713\u2713 Best Quality]\n    end\n\n    Fast -.upgrade.-&gt; Balanced\n    Balanced -.upgrade.-&gt; Premium\n\n    style Mini fill:#c8e6c9\n    style GPT4o fill:#fff9c4\n    style Turbo fill:#ffccbc</code></pre> <p>Model Comparison:</p> Model Speed Cost Quality <code>gpt-4o-mini</code> Fast Low Good <code>gpt-4o</code> Medium Medium Better <code>gpt-4-turbo</code> Slower High Best"},{"location":"api/manuals_agent/#top-k-tuning","title":"Top-K Tuning","text":"<ul> <li>Small (3): Focused, fast (default)</li> <li>Medium (5): More context</li> <li>Large (10): Comprehensive, slower</li> </ul>"},{"location":"api/manuals_ingest/","title":"Ingestion Module","text":"<p>Index PDF manuals into Qdrant with ColPali multivectors and FastEmbed embeddings.</p>"},{"location":"api/manuals_ingest/#overview","title":"Overview","text":"<p>The <code>manuals_ingest</code> module handles the complete ingestion pipeline:</p> <ol> <li>PDF Rendering - Convert pages to RGB images</li> <li>Text Extraction - Extract per-page text</li> <li>Embedding Generation - ColPali, FastEmbed, BM25</li> <li>Collection Creation - Configure Qdrant schema</li> <li>Upload - Batch upload to Qdrant</li> </ol>"},{"location":"api/manuals_ingest/#command-line-interface","title":"Command-Line Interface","text":"<pre><code>uv run python -m synth_rag.manuals_ingest [OPTIONS]\n</code></pre>"},{"location":"api/manuals_ingest/#options","title":"Options","text":"Option Type Default Description <code>--subset</code> choice <code>test</code> Which subset to ingest (<code>test</code> or <code>full</code>) <code>--collection</code> str <code>midi_manuals</code> Qdrant collection name <code>--device</code> choice <code>mps</code> Device for ColPali (<code>mps</code>, <code>cuda:0</code>, <code>cpu</code>) <code>--batch-size</code> int <code>4</code> Batch size for ColPali embeddings <code>--clear-tmp</code> flag <code>False</code> Clear tmp directories before ingestion <code>--recreate-collection</code> flag <code>False</code> Delete and recreate the collection"},{"location":"api/manuals_ingest/#examples","title":"Examples","text":"<pre><code># Ingest test subset (recommended first)\nuv run python -m synth_rag.manuals_ingest \\\n    --subset test \\\n    --collection midi_manuals \\\n    --device mps \\\n    --recreate-collection\n\n# Ingest full collection with larger batch size\nuv run python -m synth_rag.manuals_ingest \\\n    --subset full \\\n    --batch-size 8 \\\n    --device cuda:0\n</code></pre>"},{"location":"api/manuals_ingest/#module-reference","title":"Module Reference","text":""},{"location":"api/manuals_ingest/#synth_rag.manuals_ingest","title":"<code>manuals_ingest</code>","text":"<p>Ingest PDF manuals into Qdrant with ColPali multivectors + FastEmbed dense/sparse embeddings.</p>"},{"location":"api/manuals_ingest/#synth_rag.manuals_ingest.upsert_with_retry","title":"<code>upsert_with_retry(client, collection_name, points, max_retries=5, base_delay=1.0)</code>","text":"<p>Upsert points with exponential backoff retry on transient errors.</p> Source code in <code>src/synth_rag/manuals_ingest.py</code> <pre><code>def upsert_with_retry(\n    client,\n    collection_name: str,\n    points: list,\n    max_retries: int = 5,\n    base_delay: float = 1.0,\n):\n    \"\"\"Upsert points with exponential backoff retry on transient errors.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            client.upsert(collection_name=collection_name, points=points)\n            return\n        except UnexpectedResponse as e:\n            # Retry on 503 Service Unavailable or other transient errors\n            if e.status_code in (503, 429, 502, 504) and attempt &lt; max_retries - 1:\n                delay = base_delay * (2 ** attempt)\n                print(f\"    Qdrant returned {e.status_code}, retrying in {delay:.1f}s...\")\n                time.sleep(delay)\n            else:\n                raise\n</code></pre>"},{"location":"api/manuals_ingest/#synth_rag.manuals_ingest.render_pdf_pages","title":"<code>render_pdf_pages(pdf_path, output_dir)</code>","text":"<p>Render PDF pages to RGB images using pypdfium2.</p> Source code in <code>src/synth_rag/manuals_ingest.py</code> <pre><code>def render_pdf_pages(pdf_path: Path, output_dir: Path) -&gt; list[Path]:\n    \"\"\"Render PDF pages to RGB images using pypdfium2.\"\"\"\n    pdf = pdfium.PdfDocument(pdf_path)\n    page_paths = []\n\n    manual_name = pdf_path.stem\n    manual_dir = output_dir / manual_name\n    manual_dir.mkdir(parents=True, exist_ok=True)\n\n    for page_idx in range(len(pdf)):\n        page = pdf[page_idx]\n        # Render at 2x scale for better quality\n        bitmap = page.render(scale=2.0)\n        pil_image = bitmap.to_pil()\n\n        page_path = manual_dir / f\"page_{page_idx:04d}.png\"\n        pil_image.save(page_path, format=\"PNG\")\n        page_paths.append(page_path)\n\n    return page_paths\n</code></pre>"},{"location":"api/manuals_ingest/#synth_rag.manuals_ingest.extract_page_text","title":"<code>extract_page_text(pdf_path)</code>","text":"<p>Extract text from each page using pymupdf.</p> Source code in <code>src/synth_rag/manuals_ingest.py</code> <pre><code>def extract_page_text(pdf_path: Path) -&gt; list[str]:\n    \"\"\"Extract text from each page using pymupdf.\"\"\"\n    doc = pymupdf.open(pdf_path)\n    page_texts = []\n\n    for page_num in range(len(doc)):\n        page = doc[page_num]\n        text = page.get_text()\n        page_texts.append(text)\n\n    return page_texts\n</code></pre>"},{"location":"api/manuals_ingest/#synth_rag.manuals_ingest.save_text_manifest","title":"<code>save_text_manifest(manual_name, page_texts, output_dir)</code>","text":"<p>Save page text as JSON manifest.</p> Source code in <code>src/synth_rag/manuals_ingest.py</code> <pre><code>def save_text_manifest(manual_name: str, page_texts: list[str], output_dir: Path) -&gt; Path:\n    \"\"\"Save page text as JSON manifest.\"\"\"\n    manifest_path = output_dir / f\"{manual_name}.json\"\n    manifest = {\n        \"manual_name\": manual_name,\n        \"pages\": [\n            {\"page_num\": i, \"text\": text}\n            for i, text in enumerate(page_texts)\n        ]\n    }\n\n    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(manifest, f, indent=2, ensure_ascii=False)\n\n    return manifest_path\n</code></pre>"},{"location":"api/manuals_ingest/#synth_rag.manuals_ingest.mean_pool_colpali_embeddings","title":"<code>mean_pool_colpali_embeddings(embeddings, input_ids, processor, model)</code>","text":"<p>Mean pool ColPali embeddings by rows and columns. Returns: (original, pooled_rows, pooled_cols)</p> Source code in <code>src/synth_rag/manuals_ingest.py</code> <pre><code>def mean_pool_colpali_embeddings(\n    embeddings: torch.Tensor,\n    input_ids: torch.Tensor,\n    processor: ColPaliProcessor,\n    model: ColPali,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Mean pool ColPali embeddings by rows and columns.\n    Returns: (original, pooled_rows, pooled_cols)\n    \"\"\"\n    # embeddings shape: (batch_size, num_vectors, dim)\n    # For ColPali: (batch_size, 1030, 128)\n\n    batch_originals = []\n    batch_rows = []\n    batch_cols = []\n\n    for batch_idx in range(embeddings.shape[0]):\n        embedding = embeddings[batch_idx]  # (1030, 128)\n\n        # Identify image tokens\n        mask = input_ids[batch_idx] == processor.image_token_id\n\n        # ColPali always uses 32x32 patches\n        x_patches, y_patches = 32, 32\n\n        # Extract image patch embeddings\n        image_patch_embeddings = embedding[mask].view(x_patches, y_patches, model.dim)\n\n        # Mean pool by rows and columns\n        pooled_rows = image_patch_embeddings.mean(dim=1)  # (x_patches, 128)\n        pooled_cols = image_patch_embeddings.mean(dim=0)  # (y_patches, 128)\n\n        # Concatenate special tokens (postfix for ColPali)\n        special_tokens = embedding[~mask]\n        pooled_rows = torch.cat([pooled_rows, special_tokens])\n        pooled_cols = torch.cat([pooled_cols, special_tokens])\n\n        batch_originals.append(embedding)\n        batch_rows.append(pooled_rows)\n        batch_cols.append(pooled_cols)\n\n    return batch_originals, batch_rows, batch_cols\n</code></pre>"},{"location":"api/manuals_ingest/#synth_rag.manuals_ingest.create_collection","title":"<code>create_collection(client, collection_name, recreate=False)</code>","text":"<p>Create Qdrant collection with multivector + dense + sparse configs.</p> Source code in <code>src/synth_rag/manuals_ingest.py</code> <pre><code>def create_collection(client, collection_name: str, recreate: bool = False):\n    \"\"\"Create Qdrant collection with multivector + dense + sparse configs.\"\"\"\n    if recreate and client.collection_exists(collection_name):\n        client.delete_collection(collection_name)\n        print(f\"Deleted existing collection: {collection_name}\")\n\n    if client.collection_exists(collection_name):\n        print(f\"Collection {collection_name} already exists, skipping creation\")\n        return\n\n    client.create_collection(\n        collection_name=collection_name,\n        vectors_config={\n            # Original ColPali multivectors (no HNSW for reranking only)\n            \"colpali_original\": models.VectorParams(\n                size=128,\n                distance=models.Distance.COSINE,\n                multivector_config=models.MultiVectorConfig(\n                    comparator=models.MultiVectorComparator.MAX_SIM\n                ),\n                hnsw_config=models.HnswConfigDiff(m=0),  # Disable HNSW\n            ),\n            # Mean-pooled rows for first-stage retrieval\n            \"colpali_rows\": models.VectorParams(\n                size=128,\n                distance=models.Distance.COSINE,\n                multivector_config=models.MultiVectorConfig(\n                    comparator=models.MultiVectorComparator.MAX_SIM\n                ),\n            ),\n            # Mean-pooled columns for first-stage retrieval\n            \"colpali_cols\": models.VectorParams(\n                size=128,\n                distance=models.Distance.COSINE,\n                multivector_config=models.MultiVectorConfig(\n                    comparator=models.MultiVectorComparator.MAX_SIM\n                ),\n            ),\n            # Dense text embeddings from FastEmbed\n            \"dense\": models.VectorParams(\n                size=384,  # all-MiniLM-L6-v2 dimension\n                distance=models.Distance.COSINE,\n            ),\n        },\n        sparse_vectors_config={\n            # BM25 sparse embeddings\n            \"sparse\": models.SparseVectorParams(\n                modifier=models.Modifier.IDF\n            )\n        },\n    )\n    print(f\"Created collection: {collection_name}\")\n</code></pre>"},{"location":"api/manuals_ingest/#synth_rag.manuals_ingest.chunk_text","title":"<code>chunk_text(text, max_tokens=256)</code>","text":"<p>Chunk text using semantic-text-splitter.</p> Source code in <code>src/synth_rag/manuals_ingest.py</code> <pre><code>def chunk_text(text: str, max_tokens: int = 256) -&gt; list[str]:\n    \"\"\"Chunk text using semantic-text-splitter.\"\"\"\n    splitter = TextSplitter.from_tiktoken_model(\"gpt-3.5-turbo\", capacity=max_tokens)\n    chunks = splitter.chunks(text)\n    return list(chunks)\n</code></pre>"},{"location":"api/manuals_ingest/#synth_rag.manuals_ingest.ingest_manuals","title":"<code>ingest_manuals(subset, collection_name, device, batch_size, clear_tmp, recreate_collection)</code>","text":"<p>Main ingestion pipeline.</p> Source code in <code>src/synth_rag/manuals_ingest.py</code> <pre><code>def ingest_manuals(\n    subset: Literal[\"test\", \"full\"],\n    collection_name: str,\n    device: str,\n    batch_size: int,\n    clear_tmp: bool,\n    recreate_collection: bool,\n):\n    \"\"\"Main ingestion pipeline.\"\"\"\n    # Setup\n    client = get_qdrant_client()\n    input_dir = get_manual_input_dir(subset)\n    pages_dir, text_dir = ensure_tmp_dirs(clear=clear_tmp)\n\n    # Create collection\n    create_collection(client, collection_name, recreate=recreate_collection)\n\n    # Load models\n    print(\"Loading ColPali model...\")\n    colpali_model = ColPali.from_pretrained(\n        \"vidore/colpali-v1.3\",\n        torch_dtype=torch.bfloat16,\n        device_map=device,\n    ).eval()\n    colpali_processor = ColPaliProcessor.from_pretrained(\"vidore/colpali-v1.3\")\n\n    print(\"Loading FastEmbed models...\")\n    dense_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n    sparse_model = SparseTextEmbedding(\"Qdrant/bm25\")\n\n    # Get all PDFs\n    pdf_files = sorted(input_dir.glob(\"*.pdf\"))\n    print(f\"Found {len(pdf_files)} PDF files in {input_dir}\")\n\n    point_id = 0\n\n    for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n        manual_name = pdf_path.stem\n\n        # Render pages\n        print(f\"\\n  Rendering pages for {manual_name}...\")\n        page_image_paths = render_pdf_pages(pdf_path, pages_dir)\n\n        # Extract text\n        print(\"  Extracting text...\")\n        page_texts = extract_page_text(pdf_path)\n\n        # Save text manifest\n        save_text_manifest(manual_name, page_texts, text_dir)\n\n        # Process pages in batches for ColPali\n        print(\"  Generating ColPali embeddings...\")\n        all_page_embeddings = []\n\n        for i in range(0, len(page_image_paths), batch_size):\n            batch_paths = page_image_paths[i:i + batch_size]\n            batch_images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n\n            # Generate ColPali embeddings\n            processed_images = colpali_processor.process_images(batch_images).to(device)\n            with torch.no_grad():\n                image_embeddings = colpali_model(**processed_images)\n\n            # Mean pool\n            originals, rows, cols = mean_pool_colpali_embeddings(\n                image_embeddings,\n                processed_images.input_ids,\n                colpali_processor,\n                colpali_model,\n            )\n\n            for j, (orig, row, col) in enumerate(zip(originals, rows, cols)):\n                page_idx = i + j\n                all_page_embeddings.append({\n                    \"page_num\": page_idx,\n                    \"original\": orig.cpu().float().numpy().tolist(),\n                    \"rows\": row.cpu().float().numpy().tolist(),\n                    \"cols\": col.cpu().float().numpy().tolist(),\n                    \"text\": page_texts[page_idx],\n                    \"image_path\": str(batch_paths[j]),\n                })\n\n        # Now process text chunks for dense/sparse embeddings\n        print(\"  Processing text chunks...\")\n        points = []\n\n        for page_data in all_page_embeddings:\n            page_num = page_data[\"page_num\"]\n            page_text = page_data[\"text\"]\n\n            # Chunk the page text\n            if page_text.strip():\n                chunks = chunk_text(page_text)\n            else:\n                chunks = [\"\"]  # Empty page\n\n            # Generate dense and sparse embeddings for chunks\n            dense_embeddings = list(dense_model.embed(chunks))\n            sparse_embeddings = list(sparse_model.embed(chunks))\n\n            # Create points (one per page, with first chunk's text embedding)\n            # We align by using the first chunk for simplicity\n            point = PointStruct(\n                id=point_id,\n                vector={\n                    \"colpali_original\": page_data[\"original\"],\n                    \"colpali_rows\": page_data[\"rows\"],\n                    \"colpali_cols\": page_data[\"cols\"],\n                    \"dense\": dense_embeddings[0].tolist() if dense_embeddings else [0.0] * 384,\n                    \"sparse\": sparse_embeddings[0].as_object() if sparse_embeddings else models.SparseVector(indices=[], values=[]),\n                },\n                payload={\n                    \"manual_name\": manual_name,\n                    \"page_num\": page_num,\n                    \"text\": page_text[:1000],  # Store snippet\n                    \"full_text\": page_text,\n                    \"image_path\": page_data[\"image_path\"],\n                    \"num_chunks\": len(chunks),\n                },\n            )\n            points.append(point)\n            point_id += 1\n\n        # Upsert in smaller batches to avoid payload size limits\n        print(f\"  Upserting {len(points)} points in batches...\")\n        upsert_batch_size = 10  # Keep payload size manageable\n        for i in range(0, len(points), upsert_batch_size):\n            batch = points[i:i + upsert_batch_size]\n            upsert_with_retry(client, collection_name, batch)\n            print(f\"    Upserted {min(i + upsert_batch_size, len(points))}/{len(points)} points\")\n\n    print(f\"\\n\u2705 Ingestion complete! Total points: {point_id}\")\n\n    # Print collection info\n    info = client.get_collection(collection_name)\n    print(f\"Collection '{collection_name}' now has {info.points_count} points\")\n</code></pre>"},{"location":"api/manuals_ingest/#pipeline-details","title":"Pipeline Details","text":"<pre><code>flowchart LR\n    subgraph Input[Input]\n        PDF[PDF Files]\n    end\n\n    subgraph Extract[Extraction]\n        Render[PDF Rendering&lt;br/&gt;pypdfium2]\n        Text[Text Extraction&lt;br/&gt;pymupdf]\n    end\n\n    subgraph Embed[Embedding Generation]\n        ColPali[ColPali&lt;br/&gt;3 variants]\n        FastEmbed[FastEmbed&lt;br/&gt;Dense vectors]\n        BM25[BM25&lt;br/&gt;Sparse vectors]\n    end\n\n    subgraph Storage[Storage]\n        Schema[Collection Schema&lt;br/&gt;Create/Configure]\n        Upload[Upload to Qdrant]\n    end\n\n    PDF --&gt; Render\n    PDF --&gt; Text\n\n    Render --&gt; ColPali\n    Text --&gt; FastEmbed\n    Text --&gt; BM25\n\n    ColPali --&gt; Schema\n    FastEmbed --&gt; Schema\n    BM25 --&gt; Schema\n\n    Schema --&gt; Upload\n\n    style PDF fill:#e3f2fd\n    style Render fill:#f3e5f5\n    style Text fill:#f3e5f5\n    style ColPali fill:#fff3e0\n    style FastEmbed fill:#e8f5e9\n    style BM25 fill:#e8f5e9\n    style Upload fill:#ffccbc</code></pre>"},{"location":"api/manuals_ingest/#1-pdf-rendering","title":"1. PDF Rendering","text":"<p>Uses <code>pypdfium2</code> to render each PDF page to an RGB image.</p>"},{"location":"api/manuals_ingest/#2-text-extraction","title":"2. Text Extraction","text":"<p>Uses <code>pymupdf</code> to extract text per page.</p>"},{"location":"api/manuals_ingest/#3-colpali-embeddings","title":"3. ColPali Embeddings","text":"<p>Generates three variants of multivectors:</p> <ul> <li>Original: [1030, 128] - for precise reranking</li> <li>Row-pooled: [32, 128] - for fast vertical matching</li> <li>Col-pooled: [32, 128] - for fast horizontal matching</li> </ul>"},{"location":"api/manuals_ingest/#4-fastembed-bm25","title":"4. FastEmbed + BM25","text":"<p>Creates dense and sparse text embeddings for hybrid search.</p>"},{"location":"api/manuals_ingest/#5-collection-schema","title":"5. Collection Schema","text":"<p>Configures Qdrant collection with:</p> <ul> <li>Multiple named vector fields</li> <li>HNSW indexing for fast retrieval</li> <li>Sparse vectors for keyword search</li> <li>Payload metadata for results</li> </ul>"},{"location":"api/manuals_query/","title":"Query Module","text":"<p>Query PDF manuals using hybrid search with ColPali reranking.</p>"},{"location":"api/manuals_query/#overview","title":"Overview","text":"<p>The <code>manuals_query</code> module implements two-stage hybrid search:</p> <ol> <li>Prefetch Stage - Fast retrieval using HNSW-indexed vectors</li> <li>Rerank Stage - Precise reranking with original ColPali multivectors</li> </ol>"},{"location":"api/manuals_query/#command-line-interface","title":"Command-Line Interface","text":"<pre><code>uv run python -m synth_rag.manuals_query [OPTIONS]\n</code></pre>"},{"location":"api/manuals_query/#options","title":"Options","text":"Option Type Default Description <code>--question</code> str required Question to ask <code>--collection</code> str <code>midi_manuals</code> Qdrant collection name <code>--top-k</code> int <code>5</code> Number of results to return <code>--prefetch-limit</code> int <code>50</code> Results to prefetch for reranking <code>--device</code> choice <code>mps</code> Device for ColPali (<code>mps</code>, <code>cuda:0</code>, <code>cpu</code>) <code>--manual-filter</code> str <code>None</code> Filter by manual name (optional)"},{"location":"api/manuals_query/#examples","title":"Examples","text":"<pre><code># Basic query\nuv run python -m synth_rag.manuals_query \\\n    --question \"How do I adjust the filter cutoff?\" \\\n    --collection midi_manuals\n\n# Filter by specific manual\nuv run python -m synth_rag.manuals_query \\\n    --question \"What are the MIDI CC numbers?\" \\\n    --manual-filter \"Digitone\" \\\n    --top-k 10\n</code></pre>"},{"location":"api/manuals_query/#module-reference","title":"Module Reference","text":""},{"location":"api/manuals_query/#synth_rag.manuals_query","title":"<code>manuals_query</code>","text":"<p>Query PDF manuals using hybrid search (dense + sparse + ColPali reranking).</p>"},{"location":"api/manuals_query/#synth_rag.manuals_query.query_manuals","title":"<code>query_manuals(question, collection_name, top_k, prefetch_limit, device, manual_filter=None)</code>","text":"<p>Query manuals using hybrid search with ColPali reranking.</p> <p>Returns a dict with results and metadata.</p> Source code in <code>src/synth_rag/manuals_query.py</code> <pre><code>def query_manuals(\n    question: str,\n    collection_name: str,\n    top_k: int,\n    prefetch_limit: int,\n    device: str,\n    manual_filter: str | None = None,\n) -&gt; dict:\n    \"\"\"\n    Query manuals using hybrid search with ColPali reranking.\n\n    Returns a dict with results and metadata.\n    \"\"\"\n    client = get_qdrant_client()\n\n    # Load models\n    print(\"Loading ColPali model...\")\n    colpali_model = ColPali.from_pretrained(\n        \"vidore/colpali-v1.3\",\n        torch_dtype=torch.bfloat16,\n        device_map=device,\n    ).eval()\n    colpali_processor = ColPaliProcessor.from_pretrained(\"vidore/colpali-v1.3\")\n\n    print(\"Loading FastEmbed models...\")\n    dense_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n    sparse_model = SparseTextEmbedding(\"Qdrant/bm25\")\n\n    # Generate query embeddings\n    print(f\"Generating embeddings for: '{question}'\")\n\n    # Dense embedding\n    dense_embedding = list(dense_model.embed([question]))[0]\n\n    # Sparse embedding\n    sparse_embedding = list(sparse_model.embed([question]))[0]\n\n    # ColPali query embedding\n    processed_query = colpali_processor.process_queries([question]).to(device)\n    with torch.no_grad():\n        query_embedding = colpali_model(**processed_query)[0]\n    query_embedding_list = query_embedding.cpu().float().numpy().tolist()\n\n    # Build filter if manual_filter is provided\n    query_filter = None\n    if manual_filter:\n        query_filter = models.Filter(\n            must=[\n                models.FieldCondition(\n                    key=\"manual_name\",\n                    match=models.MatchValue(value=manual_filter),\n                )\n            ]\n        )\n\n    # Perform hybrid search with prefetch and rerank\n    print(f\"Querying collection '{collection_name}'...\")\n\n    start_time = datetime.now()\n\n    response = client.query_points(\n        collection_name=collection_name,\n        query=query_embedding_list,\n        prefetch=[\n            # Prefetch using dense embeddings\n            models.Prefetch(\n                query=dense_embedding.tolist(),\n                limit=prefetch_limit,\n                using=\"dense\",\n            ),\n            # Prefetch using sparse embeddings\n            models.Prefetch(\n                query=sparse_embedding.as_object(),\n                limit=prefetch_limit,\n                using=\"sparse\",\n            ),\n            # Prefetch using ColPali rows\n            models.Prefetch(\n                query=query_embedding_list,\n                limit=prefetch_limit,\n                using=\"colpali_rows\",\n            ),\n            # Prefetch using ColPali columns\n            models.Prefetch(\n                query=query_embedding_list,\n                limit=prefetch_limit,\n                using=\"colpali_cols\",\n            ),\n        ],\n        limit=top_k,\n        using=\"colpali_original\",  # Final rerank with original ColPali vectors\n        query_filter=query_filter,\n        with_payload=True,\n    )\n\n    query_time = (datetime.now() - start_time).total_seconds()\n\n    # Format results\n    results = []\n    for idx, point in enumerate(response.points):\n        results.append({\n            \"rank\": idx + 1,\n            \"score\": point.score,\n            \"manual_name\": point.payload.get(\"manual_name\"),\n            \"page_num\": point.payload.get(\"page_num\"),\n            \"text_snippet\": point.payload.get(\"text\", \"\")[:500],\n            \"image_path\": point.payload.get(\"image_path\"),\n            \"point_id\": point.id,\n        })\n\n    return {\n        \"question\": question,\n        \"collection\": collection_name,\n        \"top_k\": top_k,\n        \"prefetch_limit\": prefetch_limit,\n        \"manual_filter\": manual_filter,\n        \"query_time_seconds\": query_time,\n        \"results\": results,\n        \"timestamp\": datetime.now().isoformat(),\n    }\n</code></pre>"},{"location":"api/manuals_query/#synth_rag.manuals_query.pretty_print_results","title":"<code>pretty_print_results(query_result)</code>","text":"<p>Pretty print query results.</p> Source code in <code>src/synth_rag/manuals_query.py</code> <pre><code>def pretty_print_results(query_result: dict):\n    \"\"\"Pretty print query results.\"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"QUESTION: {query_result['question']}\")\n    print(f\"Collection: {query_result['collection']}\")\n    print(f\"Query time: {query_result['query_time_seconds']:.3f}s\")\n    print(\"=\" * 80)\n\n    if not query_result[\"results\"]:\n        print(\"\\nNo results found.\")\n        return\n\n    for result in query_result[\"results\"]:\n        print(f\"\\n[{result['rank']}] Score: {result['score']:.4f}\")\n        print(f\"Manual: {result['manual_name']}\")\n        print(f\"Page: {result['page_num']}\")\n        print(f\"Image: {result['image_path']}\")\n        print(f\"\\nText snippet:\")\n        print(f\"{result['text_snippet']}\")\n        print(\"-\" * 80)\n</code></pre>"},{"location":"api/manuals_query/#synth_rag.manuals_query.save_query_log","title":"<code>save_query_log(query_result)</code>","text":"<p>Save query results to logs directory.</p> Source code in <code>src/synth_rag/manuals_query.py</code> <pre><code>def save_query_log(query_result: dict) -&gt; Path:\n    \"\"\"Save query results to logs directory.\"\"\"\n    logs_dir = ensure_logs_dir()\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_path = logs_dir / f\"{timestamp}.json\"\n\n    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(query_result, f, indent=2, ensure_ascii=False)\n\n    print(f\"\\n\u2705 Query log saved to: {log_path}\")\n    return log_path\n</code></pre>"},{"location":"api/manuals_query/#search-pipeline","title":"Search Pipeline","text":"<pre><code>sequenceDiagram\n    actor User\n    participant QE as Query Embedding\n    participant Prefetch as Prefetch Stage&lt;br/&gt;(HNSW)\n    participant Rerank as Rerank Stage&lt;br/&gt;(MaxSim)\n    participant Results\n\n    User-&gt;&gt;QE: Submit question\n    activate QE\n\n    par Generate embeddings\n        QE-&gt;&gt;QE: ColPali embedding\n        QE-&gt;&gt;QE: Dense embedding\n        QE-&gt;&gt;QE: Sparse embedding\n    end\n\n    QE-&gt;&gt;Prefetch: Query vectors\n    deactivate QE\n    activate Prefetch\n\n    par Parallel prefetch\n        Prefetch-&gt;&gt;Prefetch: Dense search (50 results)\n        Prefetch-&gt;&gt;Prefetch: Sparse search (50 results)\n        Prefetch-&gt;&gt;Prefetch: ColPali rows (50 results)\n        Prefetch-&gt;&gt;Prefetch: ColPali cols (50 results)\n    end\n\n    Prefetch-&gt;&gt;Rerank: Top 50 candidates\n    deactivate Prefetch\n    activate Rerank\n\n    Rerank-&gt;&gt;Rerank: MaxSim scoring&lt;br/&gt;with original&lt;br/&gt;ColPali vectors\n    Rerank-&gt;&gt;Results: Top K results\n    deactivate Rerank\n\n    Results--&gt;&gt;User: Formatted results&lt;br/&gt;with citations</code></pre>"},{"location":"api/manuals_query/#how-it-works","title":"How It Works","text":"<ol> <li>Query Embedding: Generate ColPali, dense, and sparse embeddings for the question</li> <li>Hybrid Search: Combine multiple search strategies in parallel</li> <li>Rerank: Use original ColPali multivectors with MaxSim scoring</li> <li>Format Results: Pretty-print with manual names, pages, and snippets</li> </ol>"},{"location":"api/manuals_query/#query-logs","title":"Query Logs","text":"<p>All queries are logged to <code>logs/manuals_queries/&lt;timestamp&gt;.json</code> with:</p> <ul> <li>Question</li> <li>Timestamp</li> <li>Collection name</li> <li>Top-K and prefetch settings</li> <li>All results with scores and metadata</li> </ul>"},{"location":"api/manuals_query/#tuning-tips","title":"Tuning Tips","text":""},{"location":"api/manuals_query/#prefetch-limit","title":"Prefetch Limit","text":"<ul> <li>Small (20-30): Fast, may miss relevant pages</li> <li>Medium (50): Balanced (default)</li> <li>Large (100+): Better recall, slower</li> </ul>"},{"location":"api/manuals_query/#top-k","title":"Top-K","text":"<ul> <li>Small (3-5): Focused results</li> <li>Large (10+): Exploratory queries</li> </ul>"},{"location":"api/manuals_query/#manual-filter","title":"Manual Filter","text":"<p>Use when you know which manual to search:</p> <pre><code>--manual-filter \"Digitone\"\n</code></pre>"},{"location":"api/manuals_ui/","title":"UI Module","text":"<p>Gradio web interface for the Synth-RAG chatbot.</p>"},{"location":"api/manuals_ui/#overview","title":"Overview","text":"<p>The <code>manuals_ui</code> module provides a web-based chat interface using Gradio. It integrates the agentic RAG workflow with a user-friendly UI.</p>"},{"location":"api/manuals_ui/#quick-start","title":"Quick Start","text":""},{"location":"api/manuals_ui/#launch-ui","title":"Launch UI","text":"<pre><code>uv run python -m synth_rag.manuals_ui\n</code></pre> <p>Opens automatically in browser at <code>http://localhost:7860</code>.</p>"},{"location":"api/manuals_ui/#module-reference","title":"Module Reference","text":""},{"location":"api/manuals_ui/#synth_rag.manuals_ui","title":"<code>manuals_ui</code>","text":"<p>Gradio UI for MIDI Manuals RAG Chatbot.</p>"},{"location":"api/manuals_ui/#synth_rag.manuals_ui.chat_function","title":"<code>chat_function(message, history, model, collection, device, top_k)</code>","text":"<p>Chat function for Gradio ChatInterface.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>User's question</p> required <code>history</code> <code>list</code> <p>Chat history in OpenAI format</p> required <code>model</code> <code>str</code> <p>OpenAI model name</p> required <code>collection</code> <code>str</code> <p>Qdrant collection name</p> required <code>device</code> <code>str</code> <p>Device for ColPali model</p> required <code>top_k</code> <code>int</code> <p>Number of results to retrieve</p> required Source code in <code>src/synth_rag/manuals_ui.py</code> <pre><code>def chat_function(message: str, history: list, model: str, collection: str, device: str, top_k: int):\n    \"\"\"\n    Chat function for Gradio ChatInterface.\n\n    Args:\n        message: User's question\n        history: Chat history in OpenAI format\n        model: OpenAI model name\n        collection: Qdrant collection name\n        device: Device for ColPali model\n        top_k: Number of results to retrieve\n    \"\"\"\n    # Set OpenAI API key\n    api_settings = get_api_settings()\n    os.environ[\"OPENAI_API_KEY\"] = api_settings.openai_key\n\n    # Create agent graph\n    graph = create_agent_graph(\n        collection_name=collection,\n        device=device,\n        top_k=top_k,\n        model=model,\n    )\n\n    # Stream agent responses\n    accumulated_response = \"\"\n    tool_messages = []\n\n    for event in graph.stream({\"messages\": [(\"user\", message)]}):\n        for value in event.values():\n            last_message = value[\"messages\"][-1]\n\n            # Handle tool calls\n            if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n                for tool_call in last_message.tool_calls:\n                    tool_name = tool_call.get(\"name\", \"unknown\")\n\n                    # Show inline status based on tool\n                    if \"manuals\" in tool_name.lower():\n                        status = \"\ud83d\udd0d Searching PDF manuals...\"\n                    elif \"web\" in tool_name.lower() or \"search\" in tool_name.lower():\n                        status = \"\ud83c\udf10 Searching the web...\"\n                    else:\n                        status = f\"\ud83d\udd27 Using tool: {tool_name}...\"\n\n                    if status not in tool_messages:\n                        tool_messages.append(status)\n                        accumulated_response = \"\\n\".join(tool_messages)\n                        yield accumulated_response\n\n            # Handle tool responses (optional: can show brief feedback)\n            elif isinstance(last_message, ToolMessage):\n                # Optionally show that tool completed\n                pass\n\n            # Handle final answer\n            elif hasattr(last_message, \"content\") and last_message.content:\n                if not hasattr(last_message, \"tool_calls\") or not last_message.tool_calls:\n                    # This is the final answer\n                    yield last_message.content\n                    return\n</code></pre>"},{"location":"api/manuals_ui/#synth_rag.manuals_ui.create_ui","title":"<code>create_ui()</code>","text":"<p>Create the Gradio UI.</p> Source code in <code>src/synth_rag/manuals_ui.py</code> <pre><code>def create_ui():\n    \"\"\"Create the Gradio UI.\"\"\"\n\n    with gr.Blocks(title=\"MIDI Manuals RAG Chatbot\") as demo:\n        gr.Markdown(\"# \ud83c\udfb9 MIDI Manuals RAG Chatbot\")\n        gr.Markdown(\n            \"Ask questions about MIDI synthesizer manuals. \"\n            \"The agent can search PDF manuals or the web to answer your questions.\"\n        )\n\n        # Configuration row\n        with gr.Row():\n            model_dropdown = gr.Dropdown(\n                choices=[\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-turbo\"],\n                value=\"gpt-4o-mini\",\n                label=\"Model\",\n                info=\"OpenAI model to use\",\n            )\n            collection_textbox = gr.Textbox(\n                value=\"midi_manuals\",\n                label=\"Collection\",\n                info=\"Qdrant collection name\",\n            )\n            device_dropdown = gr.Dropdown(\n                choices=[\"mps\", \"cuda:0\", \"cpu\"],\n                value=\"mps\",\n                label=\"Device\",\n                info=\"Device for ColPali model\",\n            )\n            top_k_slider = gr.Slider(\n                minimum=1,\n                maximum=10,\n                value=3,\n                step=1,\n                label=\"Top-K\",\n                info=\"Number of results to retrieve\",\n            )\n\n        # Chat interface\n        gr.ChatInterface(\n            fn=chat_function,\n            additional_inputs=[\n                model_dropdown,\n                collection_textbox,\n                device_dropdown,\n                top_k_slider,\n            ],\n            examples=[\n                [\"How can I setup encoders of the Faderfox EC-4 MIDI controller such that encoders 2, 3 &amp; 4 control the Reverb of the Digitone II on MIDI channels 6, 7, &amp; 8 respectively? Then, I also want to use encoder 1 to control encoders 2, 3 &amp; 4 simultaneously on the faderfox\", \"gpt-4o\", \"midi_manuals\", \"mps\", 5],\n                [\"What are all the ways to increase decay times in the FM Drum Machine of Digitone II?\", \"gpt-4o\", \"midi_manuals\", \"mps\", 5],\n                [\"What synthesis methods does the Digitone II use?\", \"gpt-4o-mini\", \"midi_manuals\", \"mps\", 3],\n            ],\n            title=None,  # Already have title above\n            description=None,  # Already have description above\n            chatbot=gr.Chatbot(height=400),\n            textbox=gr.Textbox(placeholder=\"Ask a question about MIDI synthesizers...\", scale=7, submit_btn=True),\n        )\n\n        gr.Markdown(\n            \"---\\n\"\n            \"**Note:** First query may take longer as models are loaded. \"\n            \"Subsequent queries will be faster.\"\n        )\n\n    return demo\n</code></pre>"},{"location":"api/manuals_ui/#synth_rag.manuals_ui.main","title":"<code>main()</code>","text":"<p>Launch the Gradio UI.</p> Source code in <code>src/synth_rag/manuals_ui.py</code> <pre><code>def main():\n    \"\"\"Launch the Gradio UI.\"\"\"\n    demo = create_ui()\n    demo.launch(share=False)\n</code></pre>"},{"location":"api/manuals_ui/#features","title":"Features","text":""},{"location":"api/manuals_ui/#chat-interface","title":"Chat Interface","text":"<pre><code>flowchart LR\n    User[User] --&gt; Input[Type Message]\n    Input --&gt; Agent[Agentic RAG&lt;br/&gt;Backend]\n    Agent --&gt; Stream[Streaming&lt;br/&gt;Response]\n    Stream --&gt; Render[Markdown&lt;br/&gt;Rendering]\n    Render --&gt; Display[Display with&lt;br/&gt;Citations]\n    Display --&gt; User\n\n    Clear[Clear Chat&lt;br/&gt;Button] -.reset.-&gt; Input\n\n    style User fill:#e3f2fd\n    style Agent fill:#f3e5f5\n    style Stream fill:#e8f5e9\n    style Render fill:#fff3e0\n    style Display fill:#c8e6c9\n    style Clear fill:#ffccbc</code></pre> <p>Features:</p> <ul> <li>Multi-turn conversations: Context is maintained across messages</li> <li>Streaming responses: Real-time token-by-token generation</li> <li>Markdown rendering: Formatted responses with citations</li> <li>Clear chat: Reset conversation history</li> </ul>"},{"location":"api/manuals_ui/#configuration","title":"Configuration","text":""},{"location":"api/manuals_ui/#default-settings","title":"Default Settings","text":"<ul> <li>Collection: <code>midi_manuals</code></li> <li>Model: <code>gpt-4o-mini</code></li> <li>Device: <code>mps</code> (auto-detected)</li> <li>Top-K: 3</li> </ul>"},{"location":"api/manuals_ui/#customization","title":"Customization","text":"<p>Modify <code>manuals_ui.py</code> to change defaults:</p> <pre><code>COLLECTION_NAME = \"my_custom_collection\"\nMODEL_NAME = \"gpt-4o\"\nDEVICE = \"cuda:0\"\n</code></pre>"},{"location":"api/settings/","title":"Settings Module","text":"<p>Configuration and utility functions for Synth-RAG.</p>"},{"location":"api/settings/#overview","title":"Overview","text":"<p>The <code>settings</code> module provides:</p> <ul> <li>API configuration - Loads credentials from <code>.env</code> file</li> <li>Directory paths - Centralized path management</li> <li>Client factories - Singleton Qdrant client</li> <li>Utility functions - Path validation, directory creation</li> </ul>"},{"location":"api/settings/#module-reference","title":"Module Reference","text":""},{"location":"api/settings/#synth_rag.settings","title":"<code>settings</code>","text":""},{"location":"api/settings/#synth_rag.settings.REPO_ROOT","title":"<code>REPO_ROOT = Path(__file__).resolve().parents[2]</code>  <code>module-attribute</code>","text":""},{"location":"api/settings/#synth_rag.settings.DOCS_DIR","title":"<code>DOCS_DIR = REPO_ROOT / 'documents'</code>  <code>module-attribute</code>","text":""},{"location":"api/settings/#synth_rag.settings.MIDI_DIR","title":"<code>MIDI_DIR = DOCS_DIR / 'midi_synthesizers'</code>  <code>module-attribute</code>","text":""},{"location":"api/settings/#synth_rag.settings.INPUT_DIR","title":"<code>INPUT_DIR = MIDI_DIR / 'input'</code>  <code>module-attribute</code>","text":""},{"location":"api/settings/#synth_rag.settings.TMP_DIR","title":"<code>TMP_DIR = MIDI_DIR / 'tmp'</code>  <code>module-attribute</code>","text":""},{"location":"api/settings/#synth_rag.settings.TMP_PAGES_DIR","title":"<code>TMP_PAGES_DIR = TMP_DIR / 'pages'</code>  <code>module-attribute</code>","text":""},{"location":"api/settings/#synth_rag.settings.TMP_TEXT_DIR","title":"<code>TMP_TEXT_DIR = TMP_DIR / 'text'</code>  <code>module-attribute</code>","text":""},{"location":"api/settings/#synth_rag.settings.LOGS_DIR","title":"<code>LOGS_DIR = REPO_ROOT / 'logs'</code>  <code>module-attribute</code>","text":""},{"location":"api/settings/#synth_rag.settings.MANUAL_QUERY_LOGS_DIR","title":"<code>MANUAL_QUERY_LOGS_DIR = LOGS_DIR / 'manuals_queries'</code>  <code>module-attribute</code>","text":""},{"location":"api/settings/#synth_rag.settings.APISettings","title":"<code>APISettings</code>  <code>dataclass</code>","text":"Source code in <code>src/synth_rag/settings.py</code> <pre><code>@dataclass(frozen=True)\nclass APISettings:\n    qdrant_url: str\n    qdrant_key: str\n    openai_key: str\n    brave_key: str\n</code></pre>"},{"location":"api/settings/#synth_rag.settings.get_api_settings","title":"<code>get_api_settings()</code>  <code>cached</code>","text":"Source code in <code>src/synth_rag/settings.py</code> <pre><code>@lru_cache(maxsize=1)\ndef get_api_settings() -&gt; APISettings:\n    return APISettings(\n        qdrant_url=_get_env_var(\"QDRANT_URL\"),\n        qdrant_key=_get_env_var(\"QDRANT_KEY\"),\n        openai_key=_get_env_var(\"OPENAI_API_KEY\"),\n        brave_key=_get_env_var(\"BRAVE_API_KEY\"),\n    )\n</code></pre>"},{"location":"api/settings/#synth_rag.settings.get_qdrant_client","title":"<code>get_qdrant_client()</code>  <code>cached</code>","text":"Source code in <code>src/synth_rag/settings.py</code> <pre><code>@lru_cache(maxsize=1)\ndef get_qdrant_client() -&gt; QdrantClient:\n    api = get_api_settings()\n    return QdrantClient(url=api.qdrant_url, api_key=api.qdrant_key, timeout=60)\n</code></pre>"},{"location":"api/settings/#synth_rag.settings.get_manual_input_dir","title":"<code>get_manual_input_dir(subset)</code>","text":"Source code in <code>src/synth_rag/settings.py</code> <pre><code>def get_manual_input_dir(subset: Literal[\"test\", \"full\"]) -&gt; Path:\n    target = INPUT_DIR / subset\n    if not target.exists():\n        raise FileNotFoundError(f\"Manual subset directory not found: {target}\")\n    return target\n</code></pre>"},{"location":"api/settings/#synth_rag.settings.ensure_tmp_dirs","title":"<code>ensure_tmp_dirs(clear=False)</code>","text":"Source code in <code>src/synth_rag/settings.py</code> <pre><code>def ensure_tmp_dirs(clear: bool = False) -&gt; tuple[Path, Path]:\n    TMP_DIR.mkdir(parents=True, exist_ok=True)\n    TMP_PAGES_DIR.mkdir(parents=True, exist_ok=True)\n    TMP_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n    if clear:\n        for path in (TMP_PAGES_DIR, TMP_TEXT_DIR):\n            if path.exists():\n                shutil.rmtree(path)\n            path.mkdir(parents=True, exist_ok=True)\n    return TMP_PAGES_DIR, TMP_TEXT_DIR\n</code></pre>"},{"location":"api/settings/#synth_rag.settings.ensure_logs_dir","title":"<code>ensure_logs_dir()</code>","text":"Source code in <code>src/synth_rag/settings.py</code> <pre><code>def ensure_logs_dir() -&gt; Path:\n    MANUAL_QUERY_LOGS_DIR.mkdir(parents=True, exist_ok=True)\n    return MANUAL_QUERY_LOGS_DIR\n</code></pre>"},{"location":"api/settings/#usage-examples","title":"Usage Examples","text":""},{"location":"api/settings/#load-api-settings","title":"Load API Settings","text":"<pre><code>from synth_rag.settings import get_api_settings\n\nsettings = get_api_settings()\nprint(settings.qdrant_url)\nprint(settings.openai_key)\n</code></pre>"},{"location":"api/settings/#get-qdrant-client","title":"Get Qdrant Client","text":"<pre><code>from synth_rag.settings import get_qdrant_client\n\nclient = get_qdrant_client()\ncollections = client.get_collections()\n</code></pre>"},{"location":"api/settings/#work-with-directories","title":"Work with Directories","text":"<pre><code>from synth_rag.settings import get_manual_input_dir, ensure_tmp_dirs\n\n# Get input directory for test subset\ntest_dir = get_manual_input_dir(\"test\")\n\n# Ensure tmp directories exist\npages_dir, text_dir = ensure_tmp_dirs(clear=True)\n</code></pre>"},{"location":"api/settings/#environment-variables","title":"Environment Variables","text":"<pre><code>graph TD\n    EnvFile[.env file]\n\n    EnvFile --&gt; Qdrant[QDRANT_URL&lt;br/&gt;QDRANT_KEY]\n    EnvFile --&gt; OpenAI[OPENAI_API_KEY]\n    EnvFile --&gt; Brave[BRAVE_API_KEY]\n\n    Qdrant --&gt; VectorDB[Vector Database&lt;br/&gt;Storage &amp; Retrieval]\n    OpenAI --&gt; LLM[LLM&lt;br/&gt;Agent Reasoning]\n    Brave --&gt; WebSearch[Web Search&lt;br/&gt;Fallback Tool]\n\n    style EnvFile fill:#fff3e0\n    style Qdrant fill:#dc143c,color:#fff\n    style OpenAI fill:#9b59b6,color:#fff\n    style Brave fill:#ff6b35,color:#fff\n    style VectorDB fill:#f5f5f5\n    style LLM fill:#f5f5f5\n    style WebSearch fill:#f5f5f5</code></pre> <p>Required Variables:</p> Variable Description Example <code>QDRANT_URL</code> Qdrant cluster URL <code>https://xyz.aws.cloud.qdrant.io:6333</code> <code>QDRANT_KEY</code> Qdrant API key <code>your-api-key-here</code> <code>OPENAI_API_KEY</code> OpenAI API key <code>sk-...</code> <code>BRAVE_API_KEY</code> Brave Search API key <code>BSA...</code>"},{"location":"api/settings/#directory-structure","title":"Directory Structure","text":"<pre><code>graph TD\n    Root[synth-rag/]\n\n    Root --&gt; Docs[documents/&lt;br/&gt;DOCS_DIR]\n    Docs --&gt; MIDI[midi_synthesizers/&lt;br/&gt;MIDI_DIR]\n\n    MIDI --&gt; Input[input/&lt;br/&gt;INPUT_DIR]\n    Input --&gt; Test[test/&lt;br/&gt;Test subset]\n    Input --&gt; Full[full/&lt;br/&gt;Full collection]\n\n    MIDI --&gt; Tmp[tmp/&lt;br/&gt;TMP_DIR]\n    Tmp --&gt; Pages[pages/&lt;br/&gt;TMP_PAGES_DIR&lt;br/&gt;rendered images]\n    Tmp --&gt; Text[text/&lt;br/&gt;TMP_TEXT_DIR&lt;br/&gt;extracted text]\n\n    Root --&gt; Logs[logs/&lt;br/&gt;LOGS_DIR]\n    Logs --&gt; QueryLogs[manuals_queries/&lt;br/&gt;MANUAL_QUERY_LOGS_DIR]\n\n    style Root fill:#e3f2fd\n    style Docs fill:#f3e5f5\n    style MIDI fill:#fff3e0\n    style Input fill:#e8f5e9\n    style Tmp fill:#ffecb3\n    style Logs fill:#ffe0b2</code></pre>"}]}