{
  "file": "2012.09302.pdf",
  "total_chunks": 50,
  "chunks": [
    {
      "chunk_id": 0,
      "content": "TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural\nBackdoors\nRen Pang∗ Zheng Zhang∗ Xiangshan Gao† Zhaohan Xi∗\nShouling Ji† Peng Cheng† Xiapu Luo‡ Ting Wang∗\n∗ Pennsylvania State University, {rbp5354, zxz147, zxx5113, ting}@psu.edu\n† Zhejiang University, {corazju, sji, lunar heart}@zju.edu.cn\n‡ Hong Kong Polytechnic University, csxluo@comp.polyu.edu.hk\nAbstract—Neural backdoors represent one primary threat\nto the security of deep learning systems. The intensive re\u0002search has produced a plethora of backdoor attacks/defenses,\nresulting in a constant arms race. However, due to the lack\nof evaluation benchmarks, many critical questions remain\nunder-explored: (i) what are the strengths and limitations of\ndifferent attacks/defenses? (ii) what are the best practices to\noperate them? and (iii) how can the existing attacks/defenses\nbe further improved?\nTo bridge this gap, we design and implement TROJAN\u0002ZOO, the first open-source platform for evaluating neural\nbackdoor attacks/defenses in a unified, holistic, and practical\nmanner. Thus far, focusing on the computer vision domain,\nit has incorporated 8 representative attacks, 14 state-of-the\u0002art defenses, 6 attack performance metrics, 10 defense utility\nmetrics, as well as rich tools for in-depth analysis of the\nattack-defense interactions. Leveraging TROJANZOO, we\nconduct a systematic study on the existing attacks/defenses,\nunveiling their complex design spectrum: both manifest intri\u0002cate trade-offs among multiple desiderata (e.g., the effective\u0002ness, evasiveness, and transferability of attacks). We further\nexplore improving the existing attacks/defenses, leading to\na number of interesting findings: (i) one-pixel triggers often\nsuffice; (ii) training from scratch often outperforms perturb\u0002ing benign models to craft trojan models; (iii) optimizing\ntriggers and trojan models jointly greatly improves both\nattack effectiveness and evasiveness; (iv) individual defenses",
      "length": 1959
    },
    {
      "chunk_id": 1,
      "content": "attack effectiveness and evasiveness; (iv) individual defenses\ncan often be evaded by adaptive attacks; and (v) exploiting\nmodel interpretability significantly improves defense robust\u0002ness. We envision that TROJANZOO will serve as a valuable\nplatform to facilitate future research on neural backdoors.\nIndex Terms—backdoor attack, backdoor defense, bench\u0002mark platform, deep learning security\n1. Introduction\nToday’s deep learning (DL) systems are large, complex\nsoftware artifacts. With the increasing system complexity\nand training cost, it becomes not only tempting but also\nnecessary to exploit pre-trained deep neural networks\n(DNNs) in building DL systems. It was estimated that\nas of 2016, over 13.7% of DL-related repositories on\nGitHub re-use at least one pre-trained DNN [27]. On the\nupside, this “plug-and-play” paradigm greatly simplifies\nthe development cycles [47]. On the downside, as most\npre-trained DNNs are contributed by untrusted third par\u0002ties [8], their lack of standardization or regulation entails\nprofound security implications.\nIn particular, pre-trained DNNs can be exploited to\nlaunch neural backdoor attacks [21], [38], [43], one pri\u0002mary threat to the security of DL systems. In such attacks,\na maliciously crafted DNN (“trojan model”) forces its host\nsystem to misbehave once certain pre-defined conditions\n(“triggers”) are met but to function normally otherwise,\nwhich can result in consequential damages in security\u0002sensitive domains [7], [15], [61].\nMotivated by this, intensive research has led to a\nplethora of attacks that craft trojan model via exploiting\nproperties such as neural activation patterns [12], [21],\n[32], [38], [55], [68] and defenses that mitigate trojan\nmodels during inspection [11], [23], [26], [36], [37], [62]\nor detect trigger inputs at inference [10], [14], [19], [60].\nWith the rapid development of new attacks/defenses, a\nnumber of open questions have emerged: RQ1 – What are\nthe strengths and limitations of different attacks/defenses?",
      "length": 2000
    },
    {
      "chunk_id": 2,
      "content": "the strengths and limitations of different attacks/defenses?\nRQ2 – What are the best practices (e.g., optimization\nstrategies) to operate them? RQ3 – How can the existing\nbackdoor attacks/defenses be further improved?\nDespite their importance for understanding and mit\u0002igating the vulnerabilities incurred by neural backdoors,\nthese questions are largely under-explored due to the\nfollowing challenges.\nNon-holistic evaluations – Most studies conduct the\nevaluation with a fairly limited set of attacks/defenses,\nresulting in incomplete assessment. For instance, it is un\u0002known whether STRIP [19] is effective against the newer\nABE attack [31]. Further, the evaluation often uses simple,\nmacro-level metrics, failing to comprehensively character\u0002ize given attacks/defenses. For instance, most studies use\nattack success rate (ASR) and clean accuracy drop (CAD) to\nassess attack performance, which is insufficient to describe\nthe attack’s ability of trading between these two metrics.\nNon-unified platforms – Due to the lack of unified\nbenchmarks, different attacks/defenses are often evaluated\nunder inconsistent settings, leading to non-comparable\nconclusions. For instance, TNN [38] and LB [68] are\nevaluated with distinct trigger definitions (i.e., shape, size,\nand transparency), datasets, and DNNs, making it difficult\nto directly compare their assessment.\nNon-adaptive attacks – The evaluation of the exist\u0002ing defense [19], [23], [36], [62] often assume static,\nnon-adaptive attacks, without fully accounting for the\nadversary’s possible countermeasures, which however is\n1\narXiv:2012.09302v4 [cs.LG] 21 Oct 2022\ncritical for modeling the adversary’s optimal strategies and\nassessing the attack vulnerabilities in realistic settings.\nOur Work\nTo this end, we design, implement, and evaluate TRO\u0002JANZOO, an open-source platform for assessing neural\nbackdoor attacks/defenses in a unified, holistic, and prac\u0002tical manner. Note that while it is extensible to other",
      "length": 1969
    },
    {
      "chunk_id": 3,
      "content": "domains (e.g., NLP), currently, TROJANZOO focuses on the\nimage classification task in the computer vision domain.\nOur contributions are summarized in three major aspects:\nPlatform – To our best knowledge, TROJANZOO repre\u0002sents the first open-source platform specifically designed\nfor evaluating neural backdoor attacks/defenses. At the\nmoment of writing (02/06/2022), focusing on the com\u0002puter vision domain, TROJANZOO has incorporated 8 rep\u0002resentative attacks, 14 state-of-the-art defenses, 6 attack\nperformance metrics, 10 defense utility metrics, as well\nas a benchmark suite of 5 DNN models, 5 downstream\nmodels, and 6 datasets. Further, TROJANZOO implements a\nrich set of tools for in-depth analysis of the attack-defense\ninteractions, including measuring feature-space similarity,\ntracing neural activation patterns, and comparing attribu\u0002tion maps.\nMeasurement – Leveraging TROJANZOO, we conduct\na systematic study of the existing attacks/defenses, un\u0002veiling the complex design spectrum for the adversary\nand the defender. Different attacks manifest delicate trade\u0002offs among effectiveness, evasiveness, and transferabil\u0002ity. For instance, weaker attacks (i.e., lower ASR) tend\nto show higher transferability. Meanwhile, different de\u0002fenses demonstrate trade-offs among robustness, utility\u0002preservation, and detection accuracy. For instance, while\neffective against a variety of attacks, model sanitization\n[36], [40] also incur a significant accuracy drop. These\nobservations indicate the importance of using comprehen\u0002sive metrics to evaluate neural backdoor attacks/defenses,\nand suggest the optimal practices of applying them under\ngiven settings.\nExploration – We further explore improving exist\u0002ing attacks/defenses, leading to a number of previously\nunknown findings including (i) one-pixel triggers often\nsuffice (over 95% ASR); (ii) training from scratch often\noutperforms perturbing benign models to forge trojan",
      "length": 1931
    },
    {
      "chunk_id": 4,
      "content": "outperforms perturbing benign models to forge trojan\nmodels; (iii) leveraging DNN architectures (e.g., skip con\u0002nects) in optimizing trojan models improves the attack\neffectiveness; (iv) most individual defenses are vulnerable\nto adaptive attacks; and (v) exploiting model interpretabil\u0002ity significantly improves defense robustness. We envision\nthat the TROJANZOO platform and our findings will facil\u0002itate future research on neural backdoors and shed light\non designing and building DL systems in a more secure\nand informative manner.1\nRoadmap\nThe remainder of the paper proceeds as follows. § 3\nintroduces fundamental concepts and assumptions; § 4\n1. All the data, models, and code used in the paper are released at:\nhttps://github.com/ain-soph/trojanzoo.\nRe-training\nIntegration & Fine-tuning\nTrigger Input Malfunction\nAdversary\nVictim\nTrojan Model\nBenign FE Malicious FE\nPerturbation\nh\ng\n∗\ng\n∗\ng\nf\n∗\nFigure 1: Illustration of neural backdoor attacks.\ndetails the design and implementation of TROJANZOO and\nsystemizes existing attacks/defenses; equipped with TRO\u0002JANZOO, § 5 conducts a systematic evaluation of existing\nattacks/defenses; § 6 explores their further improvement;\n§ 7 discusses the limitations of TROJANZOO and points to\nfuture directions; the paper is concluded in § 8.\n2. Related Work\nSome recent studies have surveyed neural backdoor\nattacks/defenses (e.g., [33]); yet, none of them provides\nbenchmark implementation or empirical evaluation to ex\u0002plore their strengths/limitations. Compared with the rich\ncollection of platforms for adversarial attacks/defenses\n(e.g., CLEVERHANS [2], DEEPSEC [35], and ADVBOX\n[1]), only few platforms currently support evaluating neu\u0002ral backdoors. For instance, ART [3] integrates 3 attacks\nand 3 defenses.\nIn comparison, TROJANZOO differs in major aspects:\n(i) to our best knowledge, it features the most com\u0002prehensive library of attacks/defenses; (ii) it regards the\nevaluation metrics as a first-class citizen and implements",
      "length": 1984
    },
    {
      "chunk_id": 5,
      "content": "evaluation metrics as a first-class citizen and implements\n6 attack performance metrics and 10 defense utility met\u0002rics, which holistically assess given attacks/defenses; (iii)\nbesides reference implementation, it also provides rich\nutility tools for in-depth analysis of attack-defense inter\u0002actions, such as measuring feature-space similarity, trac\u0002ing neural activation patterns, and comparing attribution\nmaps. r The work closest to ours is perhaps TROJAI [4],\nwhich is a contest platform for model-inspection defenses\nagainst neural backdoors. While compared with TRO\u0002JANZOO, TROJAI provides a much larger pool of trojan\nmodels (over 10K) across different modalities (e.g., vision\nand NLP), TROJANZOO departs from TROJAI in majors\naspects and offers its unique value. (i) Given its contest\u0002like setting, TROJAI is a closed platform focusing on\nevaluating model-inspection defenses (i.e., detecting trojan\nmodels) against fixed attacks, while TROJANZOO is an\nopen platform that provides extensible datasets, models,\nattacks, and defenses. Thus, TROJANZOO may serve the\nneeds ranging from conducting comparative studies of\nexisting attacks/defenses to exploring and evaluating new\nattacks/defenses. (ii) While TROJAI focuses on model\u0002inspection defenses, TROJANZOO integrates four major de\u0002fense categories. (iii) In TROJAI, for its purpose, the con\u0002crete attacks behind the trojan models are unknown, which\nmakes it challenging to assess the strengths/limitations of\ngiven defenses with respect to different attacks, while in\nTROJANZOO one may directly evaluate such interactions.\n(iv) As the attacks are fixed in TROJAI, one may not\n2\nevaluate adaptive attacks. (v) The main metric used in\nTROJAI is the accuracy that defenses successfully detect\ntrojan models, while TROJANZOO provides a much richer\nset of metrics to characterize attacks/defenses.\n3. Fundamentals\nWe first introduce fundamental concepts and assump\u0002tions used throughout the paper. The important notations",
      "length": 1978
    },
    {
      "chunk_id": 6,
      "content": "are summarized in Table 1.\nNotation Definition\nA, D attack, defense\nx, x∗clean input, trigger input\nxi i-th dimension of x\nr trigger\nm mask (α for each pixel)\nf, f ∗ benign model, trojan model\nffeat upstream feature extractor\ng, g∗ downstream classifier, surrogate classifier\nt adversary’s target class\nT reference set\nR\u000f, Fδ trigger, model feasible sets\nTable 1. Symbols and notations.\n3.1. Preliminaries\nDeep neural networks (DNNs) – Deep neural net\u0002works (DNNs) represent a class of ML models to learn\nhigh-level abstractions of complex data. We assume a\npredictive setting, in which a DNN fθ (parameterized by\nθ) encodes a function fθ R\nn → Sm, where n and m\ndenote the input dimensionality and the number of classes.\nGiven input x, f(x) is a probability vector (simplex) over\nm classes.\nPre-trained DNNs – Today, it becomes not only\ntempting but also necessary to reuse pre-trained models in\ndomains in which data labeling or model training is ex\u0002pensive [70]. Under the transfer learning setting, as shown\nin Figure 1, the feature extractor (FE) g of a pre-trained\nmodel is often reused and composed with a classifier h to\nform an end-to-end model f. As the data used to train g\nmay differ from the downstream task, it is often necessary\nto fine-tune f = h ◦ g in a supervised manner. One may\nopt to perform full-tuning to train both g and h or partial\u0002tuning to train h only with g fixed [27].\nNeural backdoor attacks – With the increasing use\nof DNN models in security-sensitive domains, the ad\u0002versary is strongly incentivized to forge malicious FEs\nas attack vectors and lure victim users to re-use them\nduring system development [21]. Specifically, through a\nmalicious FE, the backdoor attack infects the target model\nwith malicious functions desired by the adversary, which\nare activated once pre-defined conditions (“triggers”) are\npresent. We refer to such infected models as “trojan mod\u0002els”. Typically, a trojan model reacts to trigger-embedded",
      "length": 1960
    },
    {
      "chunk_id": 7,
      "content": "inputs (e.g., images with specific watermarks) in a highly\npredictable manner (e.g., misclassified to a target class) but\nfunctions normally otherwise.\n3.2. Specifics\nTrigger mixing operator – For given trigger r, the\noperator ⊕ mixes a clean input x ∈ R\nn with r to generate\na trigger input x ⊕ r. Typically, r comprises three parts:\n(i) mask m ∈ {0, 1}\nn\nspecifies where r is applied (i.e.,\nx’s i-th feature xi\nis retained if miis on and mixed\nwith r otherwise); (ii) transparency α ∈ [0, 1] specifies\nthe mixing weight; and (iii) pattern p(x) ∈ R\nn\nspecifies\nr’s color intensity, which can be a constant, randomly\ndrawn from a distribution (e.g., by perturbing a template),\nor dependent on x [45]. Formally, the trigger embedding\noperator is defined as:\nx ⊕ r = (1 − m) [(1 − α)x + αp(x)] + m x (1)\nwhere denotes element-wise multiplication.\nAttack objectives – The trojan model satisfies that\nwith high probability, (i) trigger inputs are classified to the\ntarget class desired by the adversary and (ii) clean input\nare still correctly classified. Formally, the adversary forges\nthe malicious FE by optimizing the following objective:\nmin\nr∈R,θ\nE(x,y)∈T [`(fθ(x ⊕ r), t) + λ`(fθ(x), y)] (2)\nwhere T represents the training set, t denotes the target\nclass, and trigger r is selected from the feasible set R\n(which constrains r’s shape, transparency, and/or pattern).\nIntuitively, the first and second terms describe (i) and (ii),\nrespectively, and the hyper-parameter λ balances the two\nobjectives.\nAdversary’s knowledge – If the downstream classifier\nh is known to the adversary, f shares the same architecture\nwith the model h ◦ g used by the victim; otherwise, the\nadversary may resort to a surrogate classifier h\n∗\n(i.e., h\n∗ ◦\ng) or re-define the loss `(f(x ⊕ r), t) in terms of latent\nrepresentations [43], [68] as ∆(g(x ⊕ r), φt), that is, the\ndifference(e.g., MSE loss) between g(x⊕r) and φt where\nφtis the average latent representation of class t.",
      "length": 1959
    },
    {
      "chunk_id": 8,
      "content": "φtis the average latent representation of class t.\nMalicious FE training – To optimize Eqn. 2, one\nmay perturb a benign FE [38], [55] or train the malicious\nFE from scratch (details in § 6). To satisfy the trigger\nconstraint, r can be fixed [21], partially defined [38]\n(e.g., with its mask fixed), or optimized with f jointly [43].\n4. Platform\nAs illustrated in Figure 2, TROJANZOO comprises three\nmajor components: (i) the attack library integrates a set of\nrepresentative attacks that, for given benign models and\nclean inputs, are able to generate trojan models and trigger\ninputs; (ii) the defense library integrates a set of state\u0002of-the-art defenses that are able to provide model- and\ninput-level protection against trojan models and trigger\ninputs; and (iii) the analysis engine, equipped with attack\nperformance metrics, defense utility metrics, and feature\u0002rich utility tools, is able to conduct unified and holistic\nevaluation across different attacks/defenses.\nIn its current implementation, TROJANZOO has incor\u0002porated 8 attacks, 14 defenses, 6 attack performance met\u0002rics, and 10 defense utility metrics, which we systematize\nas follows.\n4.1. Attack Library\nWhile neural backdoor attacks can be characterized\nfrom a number of aspects, here we focus on 4 key de\u0002sign choices by the adversary that directly impact attack\n3\nDefense Library\n(14 Defense Methods)\nAttack Library\n(8 Attack Methods)\n(10 Defense Metrics)\nDefense Metrics\nClean Inputs\nBenign Models\nFilteringTransformation\nInput Defense\nAttack-Defense\nAnalysis\nAttack Performance\nAnalysis\nAnalysis Engine\nSanitization Inspection\nModel Defense\nTrojan Models\nTrigger Inputs\nDefense Utility\nAnalysis\nUtility Tools\n(6 Attack Metrics)\nAttack Metrics\nFigure 2: Overall system design of TROJANZOO.\nperformance. Table 2 summarizes the representative neural\nbackdoor attacks currently implemented in TROJANZOO,\nwhich are characterized along the above 4 dimensions.\nMore specifically,\nAttack Architecture Trigger Fine-tuning Defense",
      "length": 1994
    },
    {
      "chunk_id": 9,
      "content": "More specifically,\nAttack Architecture Trigger Fine-tuning Defense\nModifiability Optimizability Survivability Adaptivity\nBN [21] # # # #\nESB [57] # # #\nTNN [38] # G# # #\nRB [39] # G# # #\nTB [12] # G# # #\nLB [68] # # #\nABE [31] # # #\nIMC [43] #\nTable 2. Summary of representative neural backdoor attacks\ncurrently implemented in TROJANZOO ( – full optimization,\nG# – partial optimization, # – no optimization)\nNon-optimization – The attack simply solves Eqn. 2\nunder pre-defined triggers (i.e., shape, transparency, and\npattern) without optimization for other desiderata.\n– BadNet (BN) [21], as the representative, pre-defines\ntrigger r, generates trigger inputs {(x ⊕ r, t)}, and crafts\nthe trojan model f\n∗ by re-training a benign model f with\nsuch data.\nArchitecture modifiability – whether the attack is\nable to change the DNN architecture. Being allowed to\nmodify both the architecture and the parameters enables a\nlarger attack spectrum, but also renders the trojan model\nmore susceptible to certain defenses (e.g., model specifi\u0002cation checking).\n– Embarrassingly-Simple-Backdoor (ESB) [57], as the\nrepresentative, modifies f’s architecture by adding a mod\u0002ule which overwrites the prediction as t if r is recognized.\nWithout disturbing f’s original configuration, f\n∗\nretains\nf’s predictive power on clean inputs.\nTrigger optimizability – whether the attack uses a\nfixed, pre-defined trigger or optimizes it during craft\u0002ing the trojan model. Trigger optimization often leads\nto stronger attacks with respect to given desiderata\n(e.g., trigger stealthiness).\n– TrojanNN (TNN) [38] fixes r’s shape and position,\noptimizes its pattern to activate neurons rarely activated\nby clean inputs in pre-processing, and then forges f\n∗ by\nre-training f in a manner similar to BN.\n– Reflection-Backdoor (RB) [39] optimizes trigger\nstealthiness by defining r as the physical reflection of a\nclean image x\nr\n(selected from a pool): r = x\nr⊗k, where k",
      "length": 1943
    },
    {
      "chunk_id": 10,
      "content": "clean image x\nr\n(selected from a pool): r = x\nr⊗k, where k\nis a convolution kernel, and ⊗ is the convolution operator.\n– Targeted-Backdoor (TB) [12] randomly generates r’s\nposition in training, which makes f\n∗\neffective regardless\nof r’s position and allows the adversary to optimize r’s\nstealthiness by placing it at the most plausible position\n(e.g., an eyewear watermark over eyes).\nFine-tuning survivability – whether the backdoor\nremains effective if the model is fine-tuned. A pre-trained\nmodel is often composed with a classifier and fine-tuned\nusing the data from the downstream task. It is desirable\nto ensure that the backdoor remains effective after fine\u0002tuning.\n– Latent Backdoor (LB) [68] accounts for the im\u0002pact of downstream fine-tuning by optimizing g with re\u0002spect to latent representations rather than final predictions.\nSpecifically, it instantiates Eqn. 2 with the following loss\nfunction: `(g(x ⊕ r), t) = ∆(g(x ⊕ r), φt), where ∆\nmeasures the difference of two latent representations and\nφt denotes the average representation of class t, defined\nas φt = arg minφ E(x,t)∈T [g(x)].\nDefense adaptivity – whether the attack is optimiz\u0002able to evade possible defenses. For the attack to be\neffective, it is essential to optimize the evasiveness of\nthe trojan model and the trigger input with respect to the\ndeployed defenses.\n– Adversarial-Backdoor-Embedding (ABE) [31] ac\u0002counts for possible defenses in forging g\n∗ In solving\nEqn. 2, ABE also optimizes the indistinguishability of the\nlatent representations of trigger and clean inputs. Specif\u0002ically, it uses a discriminative network d to predict the\nrepresentation of a given input x as trigger or clean.\nFormally, the loss is defined as ∆(d ◦ g(x), b(x)), where\nb(x) encodes whether x is trigger or clean, while g\n∗\nand d\nare trained using an adversarial learning framework [20].\nMulti-optimization – whether the attack is optimiz\u0002able with respect to multiple objectives listed above.",
      "length": 1958
    },
    {
      "chunk_id": 11,
      "content": "– Input-Model Co-optimization (IMC) [43] is moti\u0002vated by the mutual-reinforcement effect between r and\nf\n∗ optimizing one amplifies the effectiveness of the other.\nInstead of solving Eqn. 2 by first pre-defining r and then\noptimizing f\n∗ IMC optimizes r and f\n∗\njointly, which\nenlarges the search spaces for r and f\n∗ leading to attacks\nsatisfying multiple desiderata (e.g., fine-tuning survivabil\u0002ity and defense adaptivity).\n4\nNeural Backdoor Defense Category Mitigation Detection Target Design Rationale\nInput Model Input Model Trigger\nRandomized-Smoothing (RS) [14]\nInput\nReformation\nX A’s fidelity (x’s and x\n∗\n’s surrounding class boundaries)\nDown-Upsampling (DU) [66] X A’s fidelity (x’s and x\n∗\n’s high-level features)\nManifold-Projection (MP) [41] X A’s fidelity (x’s and x\n∗\n’s manifold projections)\nActivation-Clustering (AC) [10]\nInput\nFiltering\nX distinct activation patterns of {x} and {x\n∗}\nSpectral-Signature (SS) [59] X distinct activation patterns of {x} and {x\n∗} (spectral space)\nSTRIP (STRIP) [19] X distinct self-entropy of x’s and x\n∗\n’s mixtures with clean inputs\nNEO (NEO) [60] X sensitivity of f\n∗\n’s prediction to trigger perturbation\nAdversarial-Retraining (AR) [40] Model X A’s fidelity (x’s and x\n∗\n’s surrounding class boundaries)\nFine-Pruning (FP) [36] Sanitization X A’s use of neurons rarely activated by clean inputs\nNeuralCleanse (NC) [62]\nModel\nInpsection\nX X abnormally small perturbation from other classes to t in f\nDeepInspect (DI) [11] X X abnormally small perturbation from other classes to t in f\n∗\nTABOR (TABOR) [23] X X abnormally small perturbation from other classes to t in f\nNeuronInspect (NI) [26] X distinct explanations of f and f\n∗ with respect to clean inputs\nABS (ABS) [37] X X A’s use of neurons elevating t’s prediction\nTable 3. Summary of representative neural backdoor defenses currently implemented in TROJANZOO (A – backdoor attack, x –\nclean input, x\n∗\n– trigger input, f – benign model, f\n∗\n– trojan model, t – target class)",
      "length": 1989
    },
    {
      "chunk_id": 12,
      "content": "clean input, x\n∗\n– trigger input, f – benign model, f\n∗\n– trojan model, t – target class)\n4.2. Attack Performance Metrics\nCurrently, TROJANZOO incorporates 6 metrics to assess\nthe effectiveness, evasiveness, and transferability of given\nattacks.\nAttack success rate (ASR) – which measures the like\u0002lihood that trigger inputs are classified to the target class\nt:\nAttack Success Rate (ASR) =\n# successful trials\n# total trials\n(3)\nTypically, higher ASR indicates more effective attacks.\nTrojan misclassification confidence (TMC) – which is\nthe average confidence score assigned to class t of trigger\ninputs in successful attacks. Intuitively, TMC complements\nASR and measures attack efficacy from another perspec\u0002tive. For two attacks with the same ASR, we consider the\none with higher TMC a stronger one.\nClean accuracy drop (CAD) – which measures the dif\u0002ference of the classification accuracy of benign and trojan\nmodels; CAD measures whether the attack directs its in\u0002fluence to trigger inputs only.\nClean classification confidence (CCC) – which is the\naverage confidence assigned to the ground-truth classes\nof clean inputs; CCC complements CAD by measuring\nattack specificity from the perspective of classification\nconfidence.\nEfficacy-specificity AUC (AUC) – which quantifies the\naggregated trade-off between attack efficacy (measured\nby ASR) and attack specificity (measured by CAD). As\nrevealed in [43], there exists an intricate balance: at a\nproper cost of specificity, it is possible to significantly\nimprove efficacy, and vice versa; AUC measures the area\nunder the ASR-CAD curve. Intuitively, smaller AUC implies\na more significant trade-off effect.\nNeuron-separation ratio (NSR) – which measures the\nintersection between neurons activated by clean and trig\u0002ger inputs. In the penultimate layer of the model, we find\nNc and Nt, the top-k active neurons with respect to clean\nand trigger inputs, respectively, and calculate their jaccard\nindex:\nNeuron Separation Ratio (NSR) = 1 −",
      "length": 1992
    },
    {
      "chunk_id": 13,
      "content": "index:\nNeuron Separation Ratio (NSR) = 1 −\n|Nt ∩ Nc|\n|Nt ∪ Nc|\n(4)\nIntuitively, NSR compares the neural activation patterns of\nclean and trigger inputs.\n4.3. Defense Library\nThe existing defenses against neural backdoors, ac\u0002cording to their strategies, can be classified into 4 major\ncategories, as summarized in Table 3. Notably, we focus\non the setting of transfer learning or outsourced training,\nwhich precludes certain other defenses such as purging\npoisoning training data [53]. Next, we detail the 14 repre\u0002sentative defenses currently implemented in TROJANZOO.\nInput reformation – which, before feeding an incom\u0002ing input to the model, first reforms it to mitigate the\ninfluence of the potential trigger, yet without explicitly\ndetecting whether it is a trigger input. It typically exploits\nthe high fidelity of attack A, that is, A tends to retain\nthe perceptual similarity of a clean input x and its trigger\ncounterpart x\n∗ – Randomized-Smoothing (RS) [14] exploits the\npremise that A retains the similarity of x and x\n∗\nin terms\nof their surrounding class boundaries and classifies an\ninput by averaging the predictions within its vicinity (via\nadding Gaussian noise).\n– Down-Upsampling (DU) [66] exploits the premise\nthat A retains the similarity of x and x\n∗\nin terms of\ntheir high-level features while the trigger r is typically\nnot perturbation-tolerant. By downsampling and then up\u0002sampling x\n∗ it is possible to mitigate r’s influence.\n– Manifold-Projection (MP) [41] exploits the premise\nthat A retains the similarity of x and x\n∗\nin terms of\ntheir projections to the data manifold. Thus, it trains\nan autoencoder to learn an approximate manifold, which\nprojects x\n∗\nto the manifold.\nInput filtering – which detects whether an incoming\ninput is embedded with a trigger and possibly recovers\nthe clean input. It typically distinguishes clean and trigger\ninputs using their distinct characteristics.\n– Activation-Clustering (AC) [10] distinguishes clean",
      "length": 1970
    },
    {
      "chunk_id": 14,
      "content": "inputs using their distinct characteristics.\n– Activation-Clustering (AC) [10] distinguishes clean\nand trigger inputs by clustering their latent representa\u0002tions. While AC is also applicable for purging poisoning\ndata, we consider its use as an input filtering method at\ninference time.\n5\n– Spectral-Signature (SS) [59] exploits the similar\nproperty in the spectral space.\n– STRIP [19] mixes a given input with a clean input\nand measures the self-entropy of its prediction. If the input\nis trigger-embedded, the mixture remains dominated by\nthe trigger and tends to be misclassified, resulting in low\nself-entropy.\n– NEO [60] detects a trigger input by searching for a\nposition, if replaced by a “blocker”, changes its prediction,\nand uses this substitution to recover its original prediction.\nModel sanitization – which, before using a pre\u0002trained model f, sanitizes it to mitigate the potential\nbackdoor, yet without explicitly detecting whether f is\ntampered.\n– Adversarial-Retraining (AR) [40] treats trigger in\u0002puts as one type of adversarial inputs and applies adver\u0002sarial training over the pre-trained model to improves its\nrobustness to backdoor attacks.\n– Fine-Pruning (FP) [36] uses the property that the\nattack exploits spare model capacity. It thus prunes rarely\nused neurons and then applies fine-tuning to defend\nagainst pruning-aware attacks.\nModel inspection – which determines whether f is a\ntrojan model and, if so, recovers the target class and the\npotential trigger, at the model checking stage.\n– NeuralCleanse (NC) [62] searches for potential trig\u0002gers in each class t. If t is trigger-embedded, the minimum\nperturbation required to change the predictions of the\ninputs in other classes to t is abnormally small.\n– DeepInspect (DI) [11] follows a similar pipeline but\nuses a generative network to generate trigger candidates.\n– TABOR [23] extends NC by adding a new regularizer\nto control the trigger search space.\n– NeuronInspect (NI) [26] exploits the property that",
      "length": 1990
    },
    {
      "chunk_id": 15,
      "content": "to control the trigger search space.\n– NeuronInspect (NI) [26] exploits the property that\nthe explanation heatmaps of benign and trojan models\nmanifest distinct characteristics. Using the features ex\u0002tracted from such heatmaps, NI detects trojan models as\noutliers.\n– ABS [37] inspects f to sift out abnormal neurons\nwith large elevation difference (i.e., active only with re\u0002spect to one specific class) and identifies triggers by\nmaximizing abnormal neuron activation while preserving\nnormal neuron behaviors.\n4.4. Defense Utility Metrics\nCurrently, TROJANZOO incorporates 10 metrics to eval\u0002uate the robustness, utility-preservation, and genericity of\ngiven defenses. The metrics are tailored to the objectives\nof each defense category (e.g., trigger input detection). For\nease of exposition, below we consider the performance of\na given defense D with respect to a given attack A.\nAttack rate deduction (ARD) – which measures the\ndifference of A’s ASR before and after D. Intuitively,\nARD indicates D’s impact on A’s efficacy. Intuitively,\nlarger ARD indicates more effective defense. We also use\nA’s TMC to measure D’s influence on the classification\nconfidence of trigger inputs.\nClean accuracy drop (CAD) – which measures the dif\u0002ference of the ACC of clean inputs before and after D\nis applied. It measures D’s impact on clean inputs. Note\nthat CAD here is defined differently from its counterpart in\nattack performance metrics. We also use CCC to measure\nD’s influence on the classification confidence of clean\ninputs.\nTrue positive rate (TPR) – which, for input-filtering\nmethods, measures the performance of detecting trigger\ninputs.\nTrue Positive Rate (TPR) =\n# detected trigger inputs\n# total trigger inputs\n(5)\nCorrespondingly, we use false positive rate (FPR) to mea\u0002sure the error of misclassifying clean inputs as trigger\ninputs.\nAnomaly index value (AIV) – which measures the\nanomaly of trojan models in model-inspection defenses.",
      "length": 1948
    },
    {
      "chunk_id": 16,
      "content": "anomaly of trojan models in model-inspection defenses.\nMost existing methods (e.g., [11], [23], [37], [62]) formal\u0002ize finding trojan models as outlier detection: each class t\nis associated with a score (e.g., minimum perturbation); if\nits score significantly deviates from others, t is considered\nto contain a backdoor. AIV, the absolute deviations from\nmedian normalized by median absolute deviation (MAD),\nprovide a reliable measure for such dispersion. Typically,\nt with AIV larger than 2 has over 95% probability of being\nanomaly.\nMask L1 norm (MLN) – which measures the `1-norm\nof the triggers recovered by model-inspection methods.\nMask jaccard similarity (MJS) – which further mea\u0002sures the intersection between the recovered trigger and\nthe ground-truth trigger (injected by the adversary). Let\nmo\nand mr be the masks of original and recovered trig\u0002gers. We define MJS as the Jaccard similarity of mo\nand\nmr Mask Jaccard Similarity (MJS) =\n|O(mo\n) ∩ O(mr)|\n|O(mo) ∪ O(mr)|\n(6)\nwhere O(m) denotes the set of non-zero elements in m.\nAverage running time (ART) – which measures D’s\noverhead. For model sanitization or inspection, which is\nperformed offline, ART is measured as the running time\nper model; while for input filtering or reformation, which\nis executed online, ART is measured as the execution time\nper input.\n5. Assessment\nLeveraging TROJANZOO, we conduct a systematic as\u0002sessment of the existing attacks and defenses and un\u0002veil their complex design spectrum: both attacks and de\u0002fenses tend to manifest intricate trade-offs among multiple\ndesiderata. We begin by describing the setting of the\nevaluation.\n5.1. Experimental Setting\nDatasets – In the evaluation, we primarily use 5\ndatasets: CIFAR10 [29], CIFAR100 [29], ImageNet [16],\nGTSRB [52], and VGGFace2 [9], with their statistics sum\u0002marized in Table 4.\nModels – We consider 3 representative DNN mod\u0002els: VGG [51], ResNet [24], and DenseNet [25]. Us\u0002ing models of distinct architectures (e.g., residual blocks",
      "length": 1986
    },
    {
      "chunk_id": 17,
      "content": "versus skip connections), we factor out the influence of\nindividual model characteristics. By default, we assume\n6\nDataset # Class # Dimension Model ACC\nCIFAR10 10 32×32\nResNet18 95.37%\nDenseNet121 93.84%\nVGG13 92.44%\nCIFAR100 100 32×32\nResNet18\n73.97%\nGTSRB 43 32×32 98.18%\nImageNet 10 224×224 92.40%\nVGGFace2 20 224×224 90.77%\nTable 4. ACC of benign models over different datasets.\nthe downstream classifier comprising one fully-connected\nlayer with softmax activation (1FCN). We also consider\nother types of classifiers, including Bayes, SVM, and\nRandom Forest. The ACC of benign models is summarized\nin Table 4.\nAttacks, Defenses, and Metrics – In the evaluation,\nwe exemplify with 8 attacks in Table 2 and 12 defenses in\nTable 3, and measure them using all the metrics in § 4.2\nand § 4.4. In all the experiments, we generate 10 trojan\nmodels for a given attack under each setting and 100 pairs\nof clean-trigger inputs with respect to each trojan model.\nThe reported results are averaged over these cases.\nImplementation – All the models, algorithms, and\nmeasurements are implemented in PyTorch. The default\nparameter setting is summarized in Table 20 and 21 (§ A).\n5.2. Attack Evaluation\nWe evaluate the existing attacks under the vanilla\nsetting (without defenses), aiming to understand the im\u0002pact of various design choices on the attack performance.\nDue to space limitations, we mainly report the results\non CIFAR10 and defer the results on other datasets to\n§ B. Overall, different attacks manifest intricate trade-offs\namong effectiveness, evasiveness, and transferability, as\ndetailed below.\n5.2.1. Effectiveness vs. Evasiveness (Trigger) We start\nwith the effectiveness-evasiveness trade-off. Intuitively,\nthe effectiveness measures whether the trigger inputs are\nsuccessfully misclassified into the target class, while the\nevasiveness measures whether the trigger inputs and trojan\nmodels are distinguishable from their normal counterparts.",
      "length": 1953
    },
    {
      "chunk_id": 18,
      "content": "models are distinguishable from their normal counterparts.\nHere, we first consider the evasiveness of triggers.\nASR (%)\nTrigger size ( )\nBN TNN RB TB LB ESB ABE IMC\nTMC\n0\n20\n40\n60\n80\n100\n0.0\n1.0\n2 2 3 3 4 4 5 5\nFigure 3: ASR and TMC with respect to trigger size (α = 0.8).\nTrigger size – Recall that the trigger definition com\u0002prises mask m, transparency α, and pattern p. We mea\u0002sure how the attack effectiveness varies with the trigger\nsize |m|. To make fair comparison, we bound the clean\naccuracy drop (CAD) of all the attacks below 3% via\ncontrolling the number of optimization iterations niter.\nFigure 3 plots the attack success rate (ASR) and trojan mis\u0002classification confidence (TMC) of various attacks under\nvarying |m| (with fixed α = 0.8).\nObserve that most attacks seem insensitive to |m|:\nas |m| varies from 2×2 to 5×5, the ASR of most at\u0002tacks increases by less than 10%, except RB with over\n30% growth. This may be attributed to its additional\nconstraints: RB defines the trigger to be the reflection\nof another image; thus, increasing |m| may improve its\nperturbation spaces. Compared with other attacks, TB and\nESB perform poorly because TB aims to force inputs with\nrandom triggers to be misclassified while ESB is unable\nto account for trigger transparency during training. Also\nobserve that the TMC of most attacks remains close to 1.0\nregardless of |m|.\nASR (%)\n0\n20\n40\n60\n80\n100\nTrigger transparency ( )\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nBN LB\nTNN\nIMC\nRB\nTB\nESB\nABE\nFigure 4: ASR with respect to trigger transparency (|m| = 3×3).\nTrigger transparency – Under the same setting, we\nevaluate the impact of trigger transparency α. Figure 4\nplots the ASR of various attacks as a function of α (|m| =\n3×3).\nCompared with trigger size, α has a more profound\nimpact. The ASR of most attacks drops sharply once α\nexceeds 0.6, among which TB approaches 10% if α ≥ 0.8,\nand ESB works only if α is close to 0, due to its reliance",
      "length": 1946
    },
    {
      "chunk_id": 19,
      "content": "and ESB works only if α is close to 0, due to its reliance\non recognizing the trigger precisely to overwrite the model\nprediction. Meanwhile, LB and IMC seem insensitive to\nα. This may be attributed to that LB optimizes trojan\nmodels with respect to latent representations (rather than\nfinal predictions), while IMC optimizes trigger patterns\nand trojan models jointly. Both strategies may mitigate\nα’s impact.\nAttack CIFAR10 CIFAR100 ImageNet\n|m| = 3, α= 0.8 |m| = 3, α= 0.8 |m| = 3, α= 0 |m| = 7, α= 0.8\nBN 72.4 (0.96) 64.5 (0.96) 90.0 (0.98) 11.4 (0.56)\nTNN 91.5 (0.97) 89.8 (0.98) 95.2 (0.99) 11.6 (0.62)\nRB 52.1 (1.0) 42.8 (0.95) 94.6 (0.98) 11.2 (0.59)\nTB 11.5 (0.66) 23.4 (0.75) 82.8 (0.97) 11.4 (0.58)\nLB 100.0 (1.0) 97.8 (0.99) 97.4 (0.99) 11.4 (0.59)\nESB 10.3 (0.43) 1.0 (0.72) 100.0 (0.50) N/A\nABE 74.3 (0.91) 67.9 (0.96) 82.6 (0.97) 12.00 (0.50)\nIMC 100.0 (1.0) 98.8 (0.99) 98.4 (1.0) 96.6 (0.99)\nTable 5. Impact of data complexity on ASR and TMC.\nData complexity – The trade-off between attack ef\u0002fectiveness and trigger evasiveness is especially evident\nfor complex data. We compare the ASR and TMC of given\nattacks on different datasets, with results in Table 5 (more\nin Table 22).\nWe observe that the class-space size (the number of\nclasses) negatively affects the attack effectiveness. For\n7\nexample, the ASR of BN drops by 7.9% from CIFAR10 to\nCIFAR100. Intuitively, it is more difficult to force trigger\ninputs from all the classes to be misclassified in larger\noutput space. Moreover, it tends to require more signif\u0002icant triggers to achieve comparable attack performance\non more complex data. For instance, for IMC to attain\nsimilar ASR on CIFAR10 and ImageNet, it needs to either\nincrease trigger size (from 3×3 to 7×7) or reduce trigger\ntransparency (from 0.8 to 0.0).\nRemark 1 – There exists a trade-off between attack effective\u0002ness and trigger evasiveness (in terms of transparency), which\nis especially evident for complex data.",
      "length": 1956
    },
    {
      "chunk_id": 20,
      "content": "is especially evident for complex data.\n5.2.2. Effectiveness vs. Evasiveness (Model) Further,\nwe consider the evasiveness of trojan models, which is\nmeasured by their difference from benign models in terms\nof classifying clean inputs. One intriguing property of the\nattacks is the trade-off between maximizing the attack\neffectiveness with respect to trigger inputs and minimizing\nthe influence over clean inputs. Here, we characterize\nthis trade-off via varying the fraction of trigger inputs\nin the training data. For each attack, we bound its CAD\nwithin 3%, measure its highest and lowest ASR (which\ncorresponds to its lowest and highest CAD respectively),\nand then normalize the ASR and CAD measures to [0, 1].\nBN (0.852) LB (0.850)\nRB (0.931) IMC (0.966)\nTB (0.852)\nTNN (0.903) ABE (0.837)\nNormalized ASR\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized CAD\n0.0 0.2 0.4 0.6 0.8 1.0\nFigure 5: Trade-off between attack effectiveness and model\nevasiveness (|m| = 3 × 3, α = 0.8).\nFigure 5 visualizes the normalized CAD-ASR trade-off.\nObserve that the curves of all the attacks manifest strong\nconvexity, indicating the “leverage” effects [43]: it is\npractical to greatly improve ASR at a disproportionally\nsmall cost of CAD. Also, observe that different attacks\nfeature varying Area Under the Curve (AUC). Intuitively,\na smaller AUC implies a stronger leverage effect. Among\nall the attacks, IMC shows the smallest AUC. This may\nbe explained by that IMC uses the trigger-model co\u0002optimization framework, which allows the adversary to\nmaximally optimize ASR at given CAD.\nRemark 2 – The trade-off between attack effectiveness and\nmodel evasiveness demonstrates strong “leverage” effects.\n5.2.3. Effectiveness vs. Transferability. Next, we eval\u0002uate the transferability of different attacks to the down\u0002stream tasks. We consider two scenarios: (i) the pre\u0002training and downstream tasks share the same dataset; and\n(ii) the downstream task uses a different dataset.",
      "length": 1950
    },
    {
      "chunk_id": 21,
      "content": "(ii) the downstream task uses a different dataset.\nTransferability (classifier) – In (i), we focus on eval\u0002uating the impact of downstream-classifier selection and\nfine-tuning strategy on the attacks. We consider 5 different\nclassifiers (1/2 fully-connected layer, Bayes, SVM, and\nRandom Forest) and 3 fine-tuning strategies (none, partial\ntuning, and full tuning). Notably, the adversary is unaware\nof such settings.\nAttack Fine-Tuning Downstream Classifier\nNone Partial Full 2-FCN Bayes SVM RF\nBN 72.4 72.3 30.4 72.2 73.5 64.7 66.0\nTNN 91.5 89.6 27.1 90.8 90.3 82.9 81.1\nRB 79.2 77.0 12.4 78.3 76.8 61.5 63.7\nLB 100.0 100.0 95.3 99.9 99.9 99.9 99.8\nIMC 100.0 99.9 88.7 99.9 100.0 99.9 99.8\nTable 6. Impact of fine-tuning and downstream-model selection.\nTable 6 compares the ASR of 5 attacks with respect to\nvarying downstream classifiers and fine-tuning strategies.\nObserve that fine-tuning has a large impact on attack\neffectiveness. For instance, the ASR of TNN drops by\n62.5% from partial- to full-tuning. Yet, LB and IMC are\nless sensitive to fine-tuning, due to their optimization\nstrategies. Also, note that the attack performance seems\nagnostic to the downstream classifier. This may be ex\u0002plained by that the downstream classifier in practice tends\nto manifest “pseudo-linearity” [27] (details in § A).\nTransferability (data) – In (ii), we focus on eval\u0002uating the transferability of the attacks across different\ndatasets.\nTransfer Attack\nSetting BN TNN RB LB IMC\nC → C 94.5 (0.99) 100.0 (1.0) 100.0 (1.0) 100.0 (1.0) 100.0 (1.0)\nC → I 8.4 (0.29) 7.8 (0.29) 8.6 (0.30) 8.2 (0.30) 9.4 (0.32)\nI → I 90.0 (0.98) 95.2 (0.99) 94.6 (0.98) 97.4 (0.99) 98.4 (1.0)\nI → C 77.0 (0.84)) 26.9 (0.72) 11.0 (0.38) 10.0 (0.38) 14.3 (0.48)\nTable 7. ASR and TMC of transfer attacks across CIFAR10 (C)\nand ImageNet (I) (|m| = 3×3, α = 0.0).\nWe evaluate the effectiveness of transferring attacks",
      "length": 1885
    },
    {
      "chunk_id": 22,
      "content": "and ImageNet (I) (|m| = 3×3, α = 0.0).\nWe evaluate the effectiveness of transferring attacks\nacross two datasets, CIFAR10 and ImageNet, with re\u0002sults summarized in Table 7. We have the following find\u0002ings. Several attacks (e.g., BN) are able to transfer from\nImageNet to CIFAR10 to a certain extent, but most at\u0002tacks fail to transfer from CIFAR10 to ImageNet. The\nfinding may be justified as follows. A model pre-trained\non complex data (i.e., ImageNet) tends to maintain its\neffectiveness of feature extraction on simple data (i.e.,\nCIFAR10) [17]; as a side effect, it may also preserve its\neffectiveness of propagating trigger patterns. Meanwhile,\na model pre-trained on simple data may not generalize\nwell to complex data. Moreover, compared with stronger\nattacks in non-transfer cases (e.g., LB), BN shows much\nhigher transferability. This may be explained by that to\nmaximize the attack efficacy, the trigger and trojan model\noften need to “over-fit” the training data, resulting in poor\ntransferability.\nRemark 3 – Most attacks transfer across classifiers; how\u0002ever, weaker attacks demonstrate higher transferability across\ndatasets.\n5.3. Defense Evaluation\nAs the defenses from different categories bear distinct\nobjectives (e.g., detecting trigger inputs versus cleansing\ntrojan models), below we evaluate each defense category\nseparately.\n8\nDefense Attack\nBN TNN RB TB LB ESB ABE IMC\n– 93.3 (0.99) 99.9 (1.0) 99.8 (1.0) 96.7 (0.99) 100.0 (1.0) 100.0 (0.86) 95.3 (0.99) 100.0 (1.0)\nRS\n-0.5 (0.99) -0.0 (1.0) -0.0 -(1.0) -0.3 (0.99) -0.0 (1.0) -89.1 (0.86) -0.5 (0.99) -0.0 (1.0)\n(±0.2) (±0.0) (±0.0) (±0.1) (±0.0) (±7.3) (±0.1) (±0.0)\nDU\n-2.2 (0.99) -0.4 (1.0) -5.4 (1.0) -67.8 (1.0) -4.1 (1.0) -89.9 (0.86) -0.5 (0.99) -0.2 (1.0)\n(±0.7) (±0.1) (±1.4) (±12.8) (±1.4) (±22.7) (±0.3) (±0.0)\nMP\n-6.0 (0.99) -37.4 (1.0) -78.6 (1.0) -11.0 (0.99) -42.6 (1.0) -87.8 (0.86) -4.6 (0.99) -16.0 (1.0)\n(±2.1) (±5.5) (±14.2) (±4.1) (±1.5) (±6.6) (±0.4) (±2.3)\nFP",
      "length": 1958
    },
    {
      "chunk_id": 23,
      "content": "(±2.1) (±5.5) (±14.2) (±4.1) (±1.5) (±6.6) (±0.4) (±2.3)\nFP\n-82.9 (0.60) -86.5 (0.64) -89.1 (0.73) -38.0 (0.89) -27.6 (0.82) -100.0 (0.81) -84.5 (0.64) -26.9 (0.83)\n(±1.8) (±4.3) (±2.6) (±6.1) (±3.7) (±0.0) (±9.3) (±4.6)\nAR\n-83.2 (0.84) -89.6 (0.85) -89.8 (0.62) -86.2 (0.63) -90.1 (0.83) -100.0 (0.86) -85.3 (0.81) -89.7 (0.83)\n(±2.2) (±1.9) (±0.7) (±4.5) (±2.8) (±0.0) (±4.4) (±1.8)\nTable 8. ARD and TMC of attack-agnostic defenses against various attacks (±: standard deviation).\n5.3.1. Robustness vs. Utility. As input transformation\nand model sanitization mitigate backdoors in an attack\u0002agnostic manner, while input filtering and model inspec\u0002tion have no direct influence on clean accuracy, we focus\non evaluating attack-agnostic defenses to study the trade\u0002off between robustness and utility preservation.\nRobustness – With the no-defense (vanilla) case as\nreference, we compare different defences in terms of\nattack rate deduction (ARD) and trojan misclassification\nconfidence (TMC), with results shown in Table 8. We have\nthe following observations: (i) MP and AR are the most\nrobust methods in the categories of input transforma\u0002tion and model sanitization, respectively. (ii) FP seems\nrobust against most attacks except LB and IMC, which\nis explained as follows: unlike attacks (e.g., TNN) that\noptimize the trigger with respect to selected neurons, LB\nand IMC perform optimization with respect to all the\nneurons, making them immune to the pruning of FP. (iii)\nMost defenses are able to defend against ESB (over 85%\nARD), which is attributed to its hard-coded trigger pattern\nand modified DNN architecture: slight perturbation to the\ntrigger input or trojan model may destroy the embedded\nbackdoor.\nDefense Attack\n– BN TNN RB TB LB ESB ABE IMC\n– 95.4 95.3 95.2 95.4 95.3 95.5 95.3 95.0 95.5\nRS\n-0.3 -0.6 -0.3 -0.4 -0.4 -0.3 -0.3 -0.4 -0.5\n(±0.2)(±0.3)(±0.1)(±0.1)(±0.3)(±0.1)(±0.1)(±0.1)(±0.2)\nDU\n-4.0 -4.5 -4.5 -4.4 -4.3 -4.3 -4.0 -4.9 -4.6",
      "length": 1954
    },
    {
      "chunk_id": 24,
      "content": "DU\n-4.0 -4.5 -4.5 -4.4 -4.3 -4.3 -4.0 -4.9 -4.6\n(±0.1)(±0.4)(±0.3)(±0.3)(±0.1)(±0.2)(±0.2)(±0.6)(±0.3)\nMP\n-11.2 -11.9 -11.3 -10.8 -11.3 -11.4 -11.2 -11.9 -11.0\n(±3.3)(±2.1)(±2.3)(±1.8)(±3.7)(±3.2)(±3.6)(±3.5)(±2.8)\nFP\n-0.1 -0.2 +0.0 +0.0 +0.0 -0.2 -0.2 +0.3 -0.4\n(±0.0)(±0.0)(±0.0)(±0.0)(±0.0)(±0.1)(±0.0)(±0.0)(±0.1)\nAR\n-11.1 -11.1 -10.4 -10.4 -10.4 -10.9 -10.9 -10.5 -11.4\n(±4.6)(±3.7)(±4.4)(±2.8)(±3.6)(±5.1)(±3.0)(±3.1)(±3.6)\nTable 9. Impact of defenses on classification accuracy (−: clean\nmodel without attack/defense; ±: standard deviation).\nUtility – We now measure the impact of defenses on\nthe accuracy of classifying clean inputs. Table 9 summa\u0002rizes the results. With the vanilla setting as the baseline,\nmost defenses tend to negatively affect clean accuracy, yet\nwith varying impact. For instance, across all the cases, FP\nattains the least CAD across all the cases, mainly due to\nits fine-tuning; RS and AR cause about 0.4% and 11%\nCAD, respectively. This is explained by the difference of\ntheir underlying mechanisms: although both attempt to\nalleviate the influence of trigger patterns, RS smooths\nthe prediction of an input x over its vicinity, while AR\nforces the model to make consistent predictions in x’s\nvicinity. Notably, comparing with Table 8, while MP and\nAR seem generically effective against all the attacks, they\nalso suffer over 10% CAD, indicating the trade-off between\nrobustness and utility preservation.\nRemark 4 – The design of attack-agnostic defenses faces the\ntrade-off between robustness and utility preservation.\n5.3.2. Detection Accuracy of Different Attacks. We\nevaluate the effectiveness of input filtering by measuring\nits accuracy in detecting trigger inputs.\nDetection accuracy – For each attack, we randomly\ngenerate 100 pairs of trigger-clean inputs and measure\nthe true positive (TPR) and false positive (FPR) rates of\nSTRIP and NEO, two input filtering methods. To make\ncomparison, we fix FPR as 0.05 and report TPR in Table 10\n(statistics in § B).",
      "length": 1999
    },
    {
      "chunk_id": 25,
      "content": "comparison, we fix FPR as 0.05 and report TPR in Table 10\n(statistics in § B).\nDefense Attack\nBN TNN RB TB LB ESB ABE IMC\nSTRIP\n0.07 0.13 0.34 0.27 0.91 0.10 0.07 0.99\n(±0.01)(±0.01)(±0.13)(±0.08)(±0.20)(±0.01)(±0.01)(±0.02)\nNEO\n0.29 0.23 0.29 0.36 0.29 0.64 0.28 0.29\n(±0.09)(±0.10)(±0.07)(±0.11)(±0.06)(±0.24)(±0.05)(±0.05)\nTable 10. TPR of NEO and STRIP (FPR = 0.05, α = 0.0, ±\nstandard deviation).\nWe have the following findings. (i) STRIP is particu\u0002larly effective against LB and IMC (over 0.9 TPR). Recall\nthat STRIP detects a trigger input using the self-entropy\nof its mixture with a clean input. This indicates that the\ntriggers produced by LB and IMC effectively dominate the\nmixtures, which is consistent with the findings in other\nexperiments (cf. Figure 2). (ii) NEO is effective against\nmost attacks to a limited extent (less than 0.3 TPR), but\nespecially effective against ESB (over 0.6 TPR), due to its\nrequirement for recognizing the trigger pattern precisely\nto overwrite the model prediction.\nImpact of trigger definition – We also evaluate the\nimpact of trigger definition on input filtering, with results\nin Figure 6 (results for other defenses in §B). With fixed\ntrigger transparency, NEO constantly attains higher TPR\nunder larger triggers; in comparison, STRIP seems less\nsensitive but also less effective under larger triggers. This\nis attributed to the difference of their detection rationale:\n9\n0.0\n0.2\n0.4\n0.8\n0.6\nFigure 6: TPR of NEO and STRIP under varying trigger definition (left: |m| = 3 × 3, right: |m| = 6 × 6; lower: α = 0.0, upper:\nα = 0.8).\ngiven input x, NEO searches for the “tipping” position in\nx to cause prediction change, which is clearly subjective\nto the trigger size; while STRIP measures the self-entropy\nof x’s mixture with a clean input, which does not rely on\nthe trigger size.\nRemark 5 – The design of input filtering defenses needs\nto balance the detection accuracy with respect to different\nattacks.\nDefense Attack\nBN TNN RB TB LB ESB ABE IMC",
      "length": 1998
    },
    {
      "chunk_id": 26,
      "content": "attacks.\nDefense Attack\nBN TNN RB TB LB ESB ABE IMC\nNC\n3.08 2.69 2.48 2.44 2.12 0.04 2.67 1.66\n(±0.65)(±0.47)(±0.51)(±0.38)(±0.20)(±0.02)(±0.51)(±0.25)\nDI\n0.54 0.46 0.39 0.29 0.21 0.01 0.76 0.26\n(±0.06)(±0.04)(±0.04)(±0.03)(±0.04)(±0.00)(±0.10)(±0.03)\nTABOR\n3.26 2.49 2.32 2.15 2.01 0.89 2.44 1.89\n(±0.77)(±0.49)(±0.51)(±0.29)(±0.63)(±0.04)(±0.22)(±0.19)\nNI\n1.28 0.59 0.78 1.11 0.86 0.71 0.41 0.52\n(±0.21)(±0.11)(±0.06)(±0.34)(±0.87)(±0.10)(±0.05)(±0.13)\nABS\n3.02 4.16 4.10 15.55 2.88 8.45 3.15\n(±0.81)(±1.33)(±1.27)(±6.59)(±0.25) (±3.22)(±0.43)\nTable 11. AIV of clean models and trojan models by various attacks.\n5.3.3. Detection Accuracy vs. Recovery Capability. We\nevaluate model-inspection defenses in terms of their effec\u0002tiveness of (i) identifying trojan models and (ii) recovering\ntrigger patterns.\nDetection Accuracy – Given defense D and model\nf, we measure the anomaly index value (AIV) of all the\nclasses; if f is a trojan model, we use the AIV of the target\nclass to quantify D’s TPR of detecting trojan models and\ntarget classes; if f is a clean model, we use the largest\nAIV to quantify D’s FPR of misclassifying clean models.\nThe results are shown in Table 11. We observe: (i)\ncompared with other defenses, ABS is highly effective in\ndetecting trojan models (with largest AIV), attributed to\nits neuron sifting strategy; (ii) IMC seems evasive to most\ndefenses (with AIV below 2), explainable by its trigger\u0002model co-optimization strategy that minimizes model dis\u0002tortion; (iii) most model-inspection defenses are either\nineffective or inapplicable against ESB, as it keeps the\noriginal DNN intact but adds an additional module. This\ncontrasts the high effectiveness of other defenses against\nESB (cf. Table 8).\nRecovery Capability – For successfully detected tro\u0002jan models, we further evaluate the trigger recovery of\nvarious defenses by measuring the mask `1 norm (MLN)\nof recovered triggers and mask jaccard similarity (MJS)",
      "length": 1944
    },
    {
      "chunk_id": 27,
      "content": "of recovered triggers and mask jaccard similarity (MJS)\nbetween the recovered and injected triggers, with results\nshown in Table 12. While the ground-truth trigger has MLN\n= 9 (α = 0.0, |m| = 3×3), most defenses recover triggers\nof varying MLN and non-zero MJS, indicating that they\nrecover triggers different from, yet overlapping with, the\ninjected ones. In contrast to Table 11, NC and TABOR out\u0002perform ABS in trigger recovery, which may be explained\nby that while ABS relies on the most abnormal neuron to\nrecover the trigger, the actual trigger may be embedded\ninto multiple neurons. This may also be corroborated by\nthat ABS attains the highest MJS on LB and IMC, which\ntend to generate triggers embedded in a few neurons\n(Table 10).\nRemark 6 – The design of model-inspection defenses faces\nthe trade-off between the accuracy of detecting trojan models\nand the effectiveness of recovering trigger patterns.\n5.3.4. Execution Time. We compare the overhead of var\u0002ious defenses by measuring their ART (§ 4.4) on a NVIDIA\nQuodro RTX6000. The results are listed in Table 13. Note\nthat online defenses (e.g., STRIP) have negligible over\u0002head, while offline methods (e.g., ABS) require longer but\nacceptable running time (103∼104seconds).\nRemark 7 – Most defenses have marginal execution overhead\nwith respect to practical datasets and models.\n5.4. Summary\nAlthough the defense from different categories bear\ndistinct objectives (e.g., detecting trigger inputs versus\ncleansing trojan models), the evaluation above leads to\nthe following observations: (i) attack-agnostic defenses\noften face a dilemma of trade-off between robustness and\naccuracy: input transformation retains high accuracy but is\noften ineffective against most attacks; model sanitization\nis effective to mitigate neural backdoors but at the cost\nof significant accuracy drop; (ii) input-filtering is com\u0002putationally efficient but only effective against a limited\nset of attacks; (iii) model-inspection requires extensive",
      "length": 1990
    },
    {
      "chunk_id": 28,
      "content": "set of attacks; (iii) model-inspection requires extensive\noptimization but the recovered trigger is able to serve\nas a guidance for possible backdoor unlearning. These\nobservations may provide guidance for choosing suitable\ndefense strategies for given application scenarios.\n6. Exploration\nNext, we examine the current practices of operating\nbackdoor attacks and defenses and explore potential im\u0002provement.\n10\nDefense\nAttack\nBN TNN RB TB LB ESB ABE IMC\nMLN MJS MLN MJS MLN MJS MLN MJS MLN MJS MLN MJS MLN MJS MLN MJS\nNC 4.98 0.55 4.65 0.70 2.64 0.89 3.53 7.52 0.21 35.16 0.00 5.84 0.42 8.63 0.13\nDI 9.65 0.25 6.88 0.17 4.77 0.30 8.44 20.17 0.21 0.00 0.06 10.21 0.30 12.78 0.25\nTABOR 5.63 0.70 4.47 0.42 3.03 0.70 3.67 7.65 0.21 43.37 0.00 5.65 0.42 8.69 0.13\nABS 17.74 0.42 17.91 0.55 17.60 0.70 16.00 17.29 0.42 17.46 0.31 17.67 0.31\nTable 12. MLN and MJS of triggers recovered by model-inspection defenses with respect to various attacks (Note: as the trigger\nposition is randomly chosen in TB, its MJS is un-defined).\nMP NEO STRIP AR FP\n2.4×101 7.7×100 1.8×10−1 1.7×104 2.1×103\nNC TABOR ABS NI DI\n1.8×103 4.2×103 1.9×103 4.6×101 4.1×102\nTable 13. Running time of various defenses (second).\n6.1. Attack – Trigger\nWe first explore improving the trigger definition by\nanswering the following questions.\nRQ1: Is it necessary to use large triggers? – It is found\nin § 5.2 that attack efficacy seems insensitive to trigger\nsize. We now consider the extreme case that the trigger\nis defined as a single pixel and evaluate the efficacy of\ndifferent attacks (constrained by CAD below 5%), with\nresults show in Table 14. Note that the trigger definition\nis inapplicable to ESB, due to its requirement for trigger\nsize.\nBN TNN RB TB LB ESB ABE IMC\n95.1 98.1 77.7 98.0 100.0 90.0 99.7\n(0.99) (0.96) (0.96) (0.99) (0.99) (0.97) (0.99)\nTable 14. ASR and TMC of single-pixel triggers (α= 0.0, CAD ≤ 5%).\nNote that single-pixel adversarial attacks have been",
      "length": 1946
    },
    {
      "chunk_id": 29,
      "content": "Note that single-pixel adversarial attacks have been\nexplored in the literature [54]; however, its study in the\ncontext of backdoor attacks is fairly limited. While it is\nmentioned in blind backdoor attacks [5], the discussion\nis limited to the specific attack and does not explore\nthe global pattern of neural backdoors. Interestingly, with\nsingle-pixel triggers, most attacks attain ASR compara\u0002ble with the cases of larger triggers (cf. Figure 3). This\nimplies the existence of universal, single-pixel perturba\u0002tion [42] with respect to trojan models (but not clean\nmodels!), highlighting the mutual-reinforcement effects\nbetween trigger inputs and trojan models [43].\nRemark 8 – There often exists universal, single-pixel pertur\u0002bation with respect to trojan models (but not clean models).\nRQ2: Is it necessary to use regular-shaped triggers? –\nThe triggers in the existing attacks are mostly regular\u0002shaped (e.g., square), which seems a common design\nchoice. We explore the impact of trigger shape on attack\nefficacy. We fix |m| = 9 but select the positions of |m|\npixels independently and randomly. Table 15 compares\nASR under the settings of regular and random triggers.\nTrigger BN TNN RB LB IMC\nRegular 72.4 91.5 79.2 100.0 100.0\nRandom 97.6 98.5 92.7 97.6 94.5\nTable 15. Comparison of regular and random triggers.\nExcept for LB and IMC which already attain extremely\nhigh ASR under the regular-trigger setting, all the other at\u0002tacks achieve higher ASR under the random-trigger setting.\nFor instance, the ASR of BN increases by 25.2%. This may\nbe explained by that lifting the spatial constraint on the\ntrigger entails a larger optimization space for the attacks.\nRemark 9 – Lifting spatial constraints on trigger patterns\ntends to lead to more effective attacks.\nRQ3: Is the “neuron-separation” guidance effective?\n– A common search strategy for trigger patterns is using\nthe neuron-separation guidance: searching for triggers that\nactivate neurons rarely used by clean inputs [38]. Here,",
      "length": 1998
    },
    {
      "chunk_id": 30,
      "content": "activate neurons rarely used by clean inputs [38]. Here,\nwe validate this guidance by measuring the NSR (§ 4.2) of\nbenign and trojan models before and after FP, as shown\nin Table 16.\nFine-Pruning – BN TNN RB LB ABE IMC\nBefore 0.03 0.59 0.61 0.65 0.61 0.54 0.64\nAfter 0.03 0.20 0.19 0.27 0.37 0.18 0.38\nTable 16. NSR of benign and trojan models before and after FP.\nAcross all the cases, compared with its benign counter\u0002part, the trojan model tends to have higher NSR, while fine\u0002tuning reduces NSR significantly. More effective attacks\n(cf. Figure 2) tend to have higher NSR (e.g., IMC). We thus\nconclude that the neuron-separation heuristic is in general\nvalid.\nRemark 10 – The separation between the neurons activated by\nclean and trigger inputs is an indicator of attack effectiveness.\n6.2. Attack – Optimization\nWe now examine the optimization strategies used by\nthe existing attacks and explore potential improvements.\nRQ4: Is it necessary to start from benign models? –\nTo forge a trojan model, a common strategy is to re\u0002train a benign, pre-trained model. Here, we challenge this\npractice by evaluating whether re-training a benign model\nleads to more effective attacks than training a trojan model\nfrom scratch.\nTraining Strategy BN TNN RB LB IMC\nBenign model re-training ASR 72.4 91.5 79.2 100.0 100.0\nCAD -1.3 -0.4 -0.6 -0.5 -2.8\nTraining from scratch ASR 76.9 98.9 81.2 100.0 100.0\nCAD -0.7 -0.6 -0.7 -0.8 -0.9\nTable 17. ASR and CAD of trojan models by training from\nscratch and re-training from benign models.\nTable 17 compares the ASR of trojan models generated\nusing the two strategies. Except for LB and IMC achieving\nsimilar ASR in both settings, the other attacks observe\nmarginal improvement if they are trained from scratch. For\ninstance, the ASR of TNN improves by 7.4%. One possible\nexplanation is as follows. Let f and f\n∗\nrepresent the\nbenign and trojan models, respectively. In the parameter\nspace, re-training constrains the search for f\n∗ within in\n11",
      "length": 1978
    },
    {
      "chunk_id": 31,
      "content": "space, re-training constrains the search for f\n∗ within in\n11\nf’s vicinity, while training from scratch searches for f\n∗\nin\nthe vicinity of a randomly initialized configuration, which\nmay lead to better starting points.\nRemark 11 – Training from scratch tends to lead to more\neffective attacks than benign-model re-training.\nRQ5: Is it feasible to exploit model architectures?\n–Most attacks train trojan models in a model-agnostic\nmanner, ignoring their unique architectures (e.g., resid\u0002ual block). We explore the possibility of exploiting such\nfeatures.\nASR (%)\n80 84 88 92 96 100\nBN\nLB\nTNN\nIMC\nRB\nTB\nESB\nABE\nAattack\nResNet DenseNet VGG\nFigure 7: Impact of DNN architecture on attack efficacy.\nWe first compare the attack performance on three\nDNN models, VGG, ResNet, and DenseNet, with re\u0002sults shown in Figure 7. First, different model archi\u0002tectures manifest varying attack vulnerabilities, ranked\nas ResNet > DenseNet > VGG. This may be explained\nas follows. Compared with traditional convolutional net\u0002works (e.g., VGG), the unique constructs of ResNet\n(i.e., residual block) and DenseNet (i.e., dense connection)\nenable more effective feature extraction, but also allow\nmore effective propagation of trigger patterns. Second,\namong all the attacks, LB, IMC, and ESB seem insensitive\nto model architectures, which may be attributed to the\noptimization strategies of LB and IMC, and the direct\nmodification of DNN architectures by ESB.\nWe then consider the skip-connect structures and\nattempt to improve the gradient backprop in training\ntrojan models. In such networks, gradients propagate\nthrough both skip-connects and residual blocks. By setting\nthe weights of gradients from skip-connects or residual\nblocks, it amplifies the gradient update towards inputs\nor model parameters [64]. Specifically, we modify the\nbackprop procedure in IMC by setting a decay coefficient\nγ = 0.5 for the gradient through skip connections, with\nASR improvement over normal training shown in Figure 8.",
      "length": 1990
    },
    {
      "chunk_id": 32,
      "content": "ASR improvement over normal training shown in Figure 8.\nASR improvement (%)\n1 1 2 2 3 3 4 4 5 5\nTrigger size ( )\n0\n1\n2\n0.69\n(+1.13)\n1.80\n(-2.21)\n0.06\n(+0.73)\n0.01\n(+0.13)\n0.00\n(+0.07)\nFigure 8: ASR improvement (and CAD change) by reducing skip\u0002connection gradients (α = 0.9).\nObserve that by reducing the skip-connection gradi\u0002ents, it marginally improves the ASR of IMC especially for\nsmall triggers (e.g., |m| = 2×2). We consider searching for\nthe optimal γ to maximize attack efficacy as our ongoing\nwork.\nRemark 12 – It is feasible to exploit skip-connect structures\nto improve attack efficacy marginally.\nRQ6: How to mix clean and trigger inputs in training?\n– To balance attack efficacy and specificity, the adversary\noften mixes clean and trigger inputs in training trojan\nmodels. There are typically three mixing strategies: (i)\ndataset-level – mixing trigger inputs Tt with clean inputs\nTc directly, (ii) batch-level – adding trigger inputs to each\nbatch of clean inputs during training, and (iii) loss-level\n– computing and aggregating the average losses of Tt\nand Tc. Here, we fix the mixing coefficient λ = 0.01 and\ncompare the effectiveness of different strategies.\nMixing Strategy BN TNN RB LB IMC\nDataset-level 59.3 72.2 46.2 99.6 92.0\nBatch-level 72.4 91.5 79.2 100.0 100.0\nLoss-level 21.6 22.9 18.1 33.6 96.5\nTable 18. Impact of mixing strategies on attack efficacy (α = 0.0,\nλ = 0.01).\nWe observe in Table 18 that across all the cases, the\nbatch-level mixing strategy leads to the highest ASR. This\ncan be explained as follows. With dataset-level mixing,\nthe ratio of trigger inputs in each batch tends to fluctuate\nsignificantly due to random shuffling, resulting in inferior\ntraining quality. With loss-level mixing, λ = 0.01 results\nin fairly small gradients of trigger inputs, equivalent to\nsetting an overly small learning rate. In comparison, batch\u0002level mixing asserts every poisoning instance and its clean\nversion must share the same batch, making the model",
      "length": 1984
    },
    {
      "chunk_id": 33,
      "content": "version must share the same batch, making the model\nfocus more on the trigger as the classification evidence\nof target class.\nHere, we provide a potential explanation: the loss-level\nmixing involves the gradient scale of poisoning data. If the\nloss is defined as L = Lclean+λ·Lpoison and optimization\nstep as ∆ = lr·\n∂(Lclean+λ·Lpoison)\n∂θ where lr is the learning\nrate and Lclean and Lpoison are the losses on the clean\nand poisoning data. Observe that ∆ = lr ·\n∂Lclean\n∂θ + lr ·\nλ·\n∂Lpoison\n∂θ The real gradient scale is lr ·λ rather than lr,\nwhich makes the step size smaller than expected.\nRemark 13 – Batch-level mixing tends to lead to the most\neffective training of trojan models.\nRQ7: How to optimize the trigger pattern? – An attack\ninvolves optimizing both the trigger pattern and the trojan\nmodel. The existing attacks use 3 typical strategies: (i)\nPre-defined trigger – it fixes the trigger pattern and only\noptimizes the trojan model. (ii) Partially optimized trigger\n– it optimizes the trigger pattern in a pre-processing stage\nand optimizes the trojan model. (iii) Trigger-model co\u0002optimization – it optimizes the trigger pattern and the\ntrojan model jointly during training. Here, we implement\n3 variants of BN that use these optimization strategies,\nrespectively. Figure 9 compares their ASR under varying\ntrigger transparency. Observe that the trigger-optimization\nstrategy has a significant impact on ASR, especially un\u0002der high transparency. For instance, if α = 0.9, the co\u0002optimization strategy improves ASR by over 60% from the\nnon-optimization strategy.\nRemark 14 – Optimizing the trigger pattern and the trojan\nmodel jointly leads to more effective attacks.\n12\nTrigger transparency ( )\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nASR (%)\n40\n60\n80\n100\npre-defined trigger\npartially optimized trigger\ntrigger-model co-optimization\nFigure 9: Impact of trigger optimization.\n6.3. Defense – Evadability\nRQ8: Are the existing defenses evadable? – We now",
      "length": 1968
    },
    {
      "chunk_id": 34,
      "content": "6.3. Defense – Evadability\nRQ8: Are the existing defenses evadable? – We now\nexplore whether the existing defenses are potentially evad\u0002able by adaptive attacks. We select IMC as the basic attack,\ndue to its flexible optimization framework, and consider\nMP, AR, STRIP, and ABS as the representative defenses\nfrom the categories in Table 3. Specifically, we adapt IMC\nto each defense.\nRecall that MP uses an auto-encoder to downsample\nthen upsample a given input, during which the trigger\npattern tends to be blurred and loses effect. To adapt IMC\nto MP, we train a surrogate autoencoder h and conduct\noptimization with inputs reformed by h.\nRecall that AR considers trigger inputs as one type\nof adversarial inputs and applies adversarial training to\nimprove model robustness against backdoor attacks. To\nadapt IMC to AR, during training f\n∗ we replace clean\naccuracy loss with adversarial accuracy loss; thus, the\nprocess is a combination of adversarial training and trojan\nmodel training, resulting in a robust but trojan model. This\nway, AR has a limited impact on the embedded backdoor,\nas the model is already robust.\nRecall that STRIP mixes up given inputs with clean\ninputs and measures the self-entropy of their predictions.\nNote that in the mixture, the transparency of the orig\u0002inal trigger is doubled; yet, STRIP works as the high\u0002transparency trigger remains effective. To adapt IMC to\nSTRIP, we use trigger inputs with high-transparency trig\u0002gers together with their ground-truth classes to re-train\nf\n∗ The re-training reduces the effectiveness of high\u0002transparency triggers while keeping low-transparency trig\u0002gers effective.\nRecall that ABS identifies triggers by maximizing ab\u0002normal activation while preserving normal neuron behav\u0002ior. To adapt IMC to ABS, we integrate the cost function\n(Algorithm 2 in [37]) in the loss function to train f\n∗ We compare the efficacy of non-adaptive and adaptive\nIMC, as shown in Figure 10. Observe that across all the",
      "length": 1970
    },
    {
      "chunk_id": 35,
      "content": "IMC, as shown in Figure 10. Observe that across all the\ncases, the adaptive IMC significantly outperforms the non\u0002adaptive one. For instance, under |m| = 6×6, it increases\nthe ASR with respect to MP by 80% and reduces the TPR\nof STRIP by over 0.85. Also note that a larger trigger\nsize leads to more effective adaptive attacks, as it entails\na larger optimization space.\nRemark 15 – Most existing defenses are potentially evadable\nby adaptive attacks.\n6.4. Defense – Interpretability\nRQ9: Does interpretability help mitigate backdoor at\u0002tacks? – The interpretability of DNNs explain how they\nmake predictions for given inputs [18], [48]. Recent stud\u0002ies [22], [58] show that such interpretability helps defend\nagainst adversarial attacks. Here, we explore whether it\nmitigates backdoor attacks. Specifically, for a pair of\nbenign-trojan models and 100 pairs of clean-trigger inputs,\nwe generate the attribution map [48] of each input with\nrespect to both models and ground and target classes, with\nan example shown in Figure 11.\nWe measure the difference (`1-norm normalized by\nimage size) of attribution maps of clean and trigger in\u0002puts. Observe in Table 19 that their attribution maps with\nrespect to the target class differ significantly on the trojan\nmodel, indicating the possibility of using interpretability\nto detect the attack. This finding also corroborates recent\nwork on using interpretability to identify possibly tam\u0002pered regions in images [13]. However, it may require\nfurther study whether the adversary may adapt the attack\nto deceive such detection [71].\nBenign model Trojan model\nOriginal class Target class Original class Target class\n0.08% 0.12% 0.63% 8.52%\nTable 19. Distance between the heatmaps of clean and trigger\ninputs (α = 0.0).\nRemark 16 – It seems promising to exploit model inter\u0002pretability to enhance defense robustness.\n6.5. Summary\nBased on the study above, we recommend the follow\u0002ing testing strategy for a new neural backdoor attack: (i)",
      "length": 1978
    },
    {
      "chunk_id": 36,
      "content": "attacks that optimize models only (e.g., BN), (ii) attacks\nthat partially optimize triggers (e.g., TNN), (iii) attacks\nthat optimize both models and triggers (e.g., IMC), and\n(iv) attacks adaptive to the given defense. The increasing\nlevel of complexity gives the adversary more flexibility to\noptimize various settings (e.g., trigger transparency and\nsize) to evade the defense, leading to stronger attacks.\nLooking forward, the study also opens several research\ndirections for future defenses: (i) ensemble defenses that\nleverage the strengths of individual ones (e.g., input trans\u0002formation and model sanitization), (ii) defenses that in\u0002volve human in the loop via interpretability, and (iii)\ndefenses that provide theoretical guarantees based on the\ninvariant properties of various attacks.\n7. Limitations\nFirst, to date TROJANZOO has integrated 8 attacks and\n14 defenses, representing the state of the art of neural\nbackdoor research. Yet, as a highly active research field,\na set of concurrent work has proposed new backdoor at\u0002tacks/defenses [34], [44], [49], [56], [63], [67], which are\nnot included in the current implementation of TROJANZOO.\nAs examples, [56] presents a new attack that obscures the\nrepresentations of benign and trigger inputs; [49] proposes\nto leverage interpretability to improve attack effectiveness;\nwhile [44] investigates data augmentation-based defenses.\nHowever, thanks to its modular design, TROJANZOO can be\nreadily extended to incorporate new attacks, defenses, and\nmetrics. Moreover, we plan to open-source all the code\n13\nTrigger size ( )\n3 3 6 6 3 3 6 6 3 3 6 6 3 3 6 6\nARMP STRIP ABS\nACC (%) ASR (%)\nAIV\n0\n25\n50\n75\n100\n0\n100\n0\n100 0\n1\n2\n3\n4\n5\nACC (%) ASR (%)\nTPR\n0\n.2\n.4\n.6\n.8\n1\n0\n25\n50\n75\n100\nFigure 10: Performance of non-adaptive and adaptive IMC against representative defenses (α = 0.0).\nBenign model\nClean inputTrigger input\nTrojan model\nOriginal\nclass\nTarget\nclass\nOriginal\nclass\nTarget\nclass",
      "length": 1943
    },
    {
      "chunk_id": 37,
      "content": "Clean inputTrigger input\nTrojan model\nOriginal\nclass\nTarget\nclass\nOriginal\nclass\nTarget\nclass\nFigure 11: Sample attribution maps of clean and trigger inputs\nwith respect to benign and trojan models (α = 0.0, ImageNet).\nand data of TROJANZOO and encourage the community to\ncontribute.\nSecond, to conduct a unified evaluation, we mainly\nconsider the attack vector of re-using pre-trained trojan\nmodels. There are other attack vectors through which\nbackdoor attacks can be launched, including poisoning\nvictims’ training data [50], [73] and knowledge distilla\u0002tion [69], which entail additional constraints for attacks\nor defenses. For instance, the poisoning data needs to\nbe evasive to bypass inspection. We consider studying\nalternative attack vectors as our ongoing work.\nThird, due to space limitations, our evaluation focuses\non popular DNN models (e.g., ResNet) and assumes fixed\ntraining/test data split. We consider evaluating the impact\nof model configuration and data split on neural backdoor\nattacks/defenses as our ongoing work.\nFinally, because of the plethora of work on neural\nbackdoors in the computer vision domain, TROJANZOO\nfocuses on the image classification task, while recent\nwork has also explored neural backdoors in other settings,\nincluding natural language processing [30], [46], [72],\nreinforcement learning [28], and federated learning [6],\n[65]. We plan to extend TROJANZOO to support such\nsettings in its future releases.\n8. Conclusion\nWe design and implement TROJANZOO, the first\nplatform dedicated to assessing neural backdoor at\u0002tacks/defenses in a holistic, unified, and practical manner.\nLeveraging TROJANZOO, we conduct a systematic eval\u0002uation of existing attacks/defenses, which demystifies a\nnumber of open questions, reveals various design trade\u0002offs, and sheds light on further improvement. We envision\nTROJANZOO will serve as a useful benchmark to facilitate\nneural backdoor research.\nAcknowledgment",
      "length": 1940
    },
    {
      "chunk_id": 38,
      "content": "TROJANZOO will serve as a useful benchmark to facilitate\nneural backdoor research.\nAcknowledgment\nWe thank anonymous reviewers and shepherd for valu\u0002able feedback. This work is supported by the National\nScience Foundation under Grant No. 1951729, 1953813,\nand 1953893. Any opinions, findings, and conclusions\nor recommendations are those of the authors and do\nnot necessarily reflect the views of the National Science\nFoundation. X. Luo is partly supported by Hong Kong\nRGC Project (No. PolyU15222320).\n14\nReferences\n[1] Advbox. https://github.com/advboxes/AdvBox/.\n[2] CleverHans Adversarial Examples Library. https://github.com/\ntensorflow/cleverhans/.\n[3] IBM Adversarial Robustness Toolbox (ART). https://github.com/\nTrusted-AI/adversarial-robustness-toolbox/.\n[4] Trojai. https://trojai.readthedocs.io.\n[5] Eugene Bagdasaryan and Vitaly Shmatikov. Blind Backdoors in\nDeep Learning Models. Proceedings of USENIX Security Sympo\u0002sium (SEC), 2021.\n[6] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin,\nand Vitaly Shmatikov. How To Backdoor Federated Learning. In\nInternational Conference on Artificial Intelligence and Statistics\n(AISTATS), 2020.\n[7] Battista Biggio, Giorgio Fumera, Fabio Roli, and Luca Didaci.\nPoisoning Adaptive Biometric Systems. In Proceedings of Joint\nIAPR International Workshop on Structural, Syntactic, and Statis\u0002tical Pattern Recognition (SSPR&SPR), 2012.\n[8] BVLC. Model zoo. https://github.com/BVLC/caffe/wiki/\nModel-Zoo, 2017.\n[9] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew\nZisserman. Vggface2: A dataset for recognising faces across pose\nand age. In 13th IEEE International Conference on Automatic Face\n& Gesture Recognition, 2018.\n[10] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig,\nBenjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivas\u0002tava. Detecting Backdoor Attacks on Deep Neural Networks by\nActivation Clustering. In ArXiv e-prints, 2018.\n[11] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar.",
      "length": 1992
    },
    {
      "chunk_id": 39,
      "content": "[11] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar.\nDeepInspect: A Black-box Trojan Detection and Mitigation Frame\u0002work for Deep Neural Networks. In Proceedings of International\nJoint Conference on Artificial Intelligence, 2019.\n[12] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song.\nTargeted Backdoor Attacks on Deep Learning Systems Using Data\nPoisoning. ArXiv e-prints, 2017.\n[13] Edward Chou, Florian Tramer, Giancarlo Pellegrino, and Dan\nBoneh. SentiNet: Detecting Physical Attacks Against Deep Learn\u0002ing Systems. In ArXiv e-prints, 2018.\n[14] Jeremy M Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified Ad\u0002versarial Robustness via Randomized Smoothing. In Proceedings\nof IEEE Conference on Machine Learning (ICML), 2019.\n[15] Paul Cooper. Meet AISight: The scary CCTV network completely\nrun by AI. http://www.itproportal.com/, 2014.\n[16] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Ima\u0002geNet: A Large-scale Hierarchical Image Database. In Proceedings\nof IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2009.\n[17] Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko,\nSusan M. Swetter, Helen M. Blau, and Sebastian Thrun.\nDermatologist-level classification of skin cancer with deep neural\nnetworks. Nature, 542(7639):115–118, 2017.\n[18] Ruth C Fong and Andrea Vedaldi. Interpretable Explanations of\nBlack Boxes by Meaningful Perturbation. In Proceedings of IEEE\nInternational Conference on Computer Vision (ICCV), 2017.\n[19] Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith\nRanasinghe, and Surya Nepal. STRIP: A Defence Against Trojan\nAttacks on Deep Neural Networks. In Proceedings of Annual\nComputer Security Applications Conference (ACSAC), 2019.\n[20] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,\nDavid Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua\nBengio. Generative Adversarial Networks. In Proceedings of\nAdvances in Neural Information Processing Systems (NeurIPS),\n2014.",
      "length": 1977
    },
    {
      "chunk_id": 40,
      "content": "Advances in Neural Information Processing Systems (NeurIPS),\n2014.\n[21] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. BadNets:\nIdentifying Vulnerabilities in the Machine Learning Model Supply\nChain. ArXiv e-prints, 2017.\n[22] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and\nXinyu Xing. LEMNA: Explaining Deep Learning Based Security\nApplications. In Proceedings of ACM Conference on Computer\nand Communications (CCS), 2018.\n[23] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song.\nTABOR: A Highly Accurate Approach to Inspecting and Restor\u0002ing Trojan Backdoors in AI Systems. In Proceedings of IEEE\nInternational Conference on Data Mining (ICDM), 2019.\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nResidual Learning for Image Recognition. In Proceedings of IEEE\nConference on Computer Vision and Pattern Recognition (CVPR),\n2016.\n[25] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q.\nWeinberger. Densely Connected Convolutional Networks. In\nProceedings of IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[26] Xijie Huang, Moustafa Alzantot, and Mani Srivastava. Neu\u0002ronInspect: Detecting Backdoors in Neural Networks via Output\nExplanations. In Proceedings of AAAI Conference on Artificial\nIntelligence (AAAI), 2019.\n[27] Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang.\nModel-Reuse Attacks on Deep Learning Systems. In Proceed\u0002ings of ACM SAC Conference on Computer and Communications\n(CCS), 2018.\n[28] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li.\nTrojDRL: Trojan Attacks on Deep Reinforcement Learning Agents.\nArXiv e-prints, 2019.\n[29] Alex Krizhevsky and Geoffrey Hinton. Learning Multiple Layers\nof Features from Tiny Images. Technical report, University of\nToronto, 2009.\n[30] Keita Kurita, Paul Michel, and Graham Neubig. Weight Poisoning\nAttacks on Pre-trained Models. In Proceedings of Annual Meeting\nof the Association for Computational Linguistics (ACL), 2020.",
      "length": 1982
    },
    {
      "chunk_id": 41,
      "content": "of the Association for Computational Linguistics (ACL), 2020.\n[31] Te Lester Juin Tan and Reza Shokri. Bypassing Backdoor Detection\nAlgorithms in Deep Learning. In Proceedings of IEEE European\nSymposium on Security and Privacy (Euro S&P), 2020.\n[32] Shaofeng Li, Benjamin Zi Hao Zhao, Jiahao Yu, Minhui Xue, Dali\nKaafar, and Haojin Zhu. Invisible Backdoor Attacks Against Deep\nNeural Networks. ArXiv e-prints, 2019.\n[33] Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao\nXia. Backdoor Learning: A Survey. ArXiv e-prints, 2020.\n[34] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. Composite\nBackdoor Attack for Deep Neural Network by Mixing Existing\nBenign Features. In Proceedings of ACM SAC Conference on\nComputer and Communications (CCS), 2020.\n[35] X. Ling, S. Ji, J. Zou, J. Wang, C. Wu, B. Li, and T. Wang.\nDEEPSEC: A Uniform Platform for Security Analysis of Deep\nLearning Model. In Proceedings of IEEE Symposium on Security\nand Privacy (S&P), 2019.\n[36] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine\u0002Pruning: Defending Against Backdooring Attacks on Deep Neural\nNetworks. In Proceedings of Symposium on Research in Attacks,\nIntrusions and Defenses (RAID), 2018.\n[37] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra\nAafer, and Xiangyu Zhang. ABS: Scanning Neural Networks for\nBack-Doors by Artificial Brain Stimulation. In Proceedings of\nACM SAC Conference on Computer and Communications (CCS),\n2019.\n[38] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai,\nWeihang Wang, and Xiangyu Zhang. Trojaning attack on neural\nnetworks. In Proceedings of Network and Distributed System\nSecurity Symposium (NDSS), 2018.\n[39] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflec\u0002tion Backdoor: A Natural Backdoor Attack on Deep Neural Net\u0002works. In Proceedings of European Conference on Computer\nVision (ECCV), 2020.\n[40] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dim\u0002itris Tsipras, and Adrian Vladu. Towards Deep Learning Models",
      "length": 1987
    },
    {
      "chunk_id": 42,
      "content": "Resistant to Adversarial Attacks. In Proceedings of International\nConference on Learning Representations (ICLR), 2018.\n15\n[41] Dongyu Meng and Hao Chen. MagNet: A Two-Pronged Defense\nAgainst Adversarial Examples. In Proceedings of ACM SAC\nConference on Computer and Communications (CCS), 2017.\n[42] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi,\nPascal Frossard, and Stefano Soatto. Analysis of Universal Adver\u0002sarial Perturbations. ArXiv e-prints, 2017.\n[43] Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy\nVorobeychik, Xiapu Luo, Alex Liu, and Ting Wang. A Tale of Evil\nTwins: Adversarial Inputs versus Poisoned Models. In Proceed\u0002ings of ACM SAC Conference on Computer and Communications\n(CCS), 2020.\n[44] Han Qiu, Yi Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu,\nand Bhavani Thuraisingham. Deepsweep: An evaluation framework\nfor mitigating dnn backdoor attacks using data augmentation. In\nProceedings of ACM Symposium on Information, Computer and\nCommunications Security (AsiaCCS), 2021.\n[45] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang\nZhang. Dynamic Backdoor Attacks Against Machine Learning\nModels. ArXiv e-prints, 2020.\n[46] Roei Schuster, Tal Schuster, Yoav Meri, and Vitaly Shmatikov.\nHumpty Dumpty: Controlling Word Meanings via Corpus Poison\u0002ing. In Proceedings of IEEE Symposium on Security and Privacy\n(S&P), 2020.\n[47] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd\nPhillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean\u0002Francois Crespo, and Dan Dennison. Hidden Technical Debt in\nMachine Learning Systems. In Proceedings of Advances in Neural\nInformation Processing Systems (NeurIPS), 2015.\n[48] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh,\nand D. Batra. Grad-CAM: Visual Explanations from Deep Net\u0002works via Gradient-Based Localization. In Proceedings of IEEE\nInternational Conference on Computer Vision (ICCV), 2017.\n[49] Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea.",
      "length": 1972
    },
    {
      "chunk_id": 43,
      "content": "[49] Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea.\nExplanation-Guided Backdoor Poisoning Attacks Against Malware\nClassifiers. 2021.\n[50] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu,\nChristoph Studer, Tudor Dumitras, and Tom Goldstein. Poison\nFrogs! Targeted Clean-Label Poisoning Attacks on Neural Net\u0002works. In Proceedings of Advances in Neural Information Pro\u0002cessing Systems (NeurIPS), 2018.\n[51] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional\nNetworks for Large-Scale Image Recognition. In Proceedings\nof International Conference on Learning Representations (ICLR),\n2014.\n[52] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian\nIgel. Man vs. Computer: Benchmarking Machine Learning Al\u0002gorithms for Traffic Sign Recognition. Neural Metworks, pages\n323–32, 2012.\n[53] Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certified\nDefenses for Data Poisoning Attacks. In Proceedings of Advances\nin Neural Information Processing Systems (NeurIPS), 2017.\n[54] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One\npixel attack for fooling deep neural networks. IEEE Transactions\non Evolutionary Computation, 23(5):828–841, 2019.\n[55] Octavian Suciu, Radu Marginean, Yi ˘ gitcan Kaya, Hal Daum ˘ e,´\nIII, and Tudor Dumitras¸. When Does Machine Learning FAIL?\nGeneralized Transferability for Evasion and Poisoning Attacks. In\nProceedings of USENIX Security Symposium (SEC), 2018.\n[56] Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang. Demon\nin the Variant: Statistical Analysis of DNNs for Robust Backdoor\nContamination Detection. 2021.\n[57] Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu.\nAn Embarrassingly Simple Approach for Trojan Attack in Deep\nNeural Networks. In Proceedings of ACM International Conference\non Knowledge Discovery and Data Mining (KDD), 2020.",
      "length": 1839
    },
    {
      "chunk_id": 44,
      "content": "on Knowledge Discovery and Data Mining (KDD), 2020.\n[58] Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu Zhang. At\u0002tacks Meet Interpretability: Attribute-Steered Detection of Adver\u0002sarial Samples. In Proceedings of Advances in Neural Information\nProcessing Systems (NeurIPS), 2018.\n[59] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral Signatures\nin Backdoor Attacks. In Proceedings of Advances in Neural\nInformation Processing Systems (NeurIPS), 2018.\n[60] Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth\nRawshan, and Sudipta Chattopadhyay. Model Agnostic Defence\nagainst Backdoor Attacks in Machine Learning. ArXiv e-prints,\n2019.\n[61] Allyson Versprille. Researchers Hack Into Driverless Car System,\nTake Control of Vehicle. http://www.nationaldefensemagazine.org/,\n2015.\n[62] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and\nB. Y. Zhao. Neural Cleanse: Identifying and Mitigating Backdoor\nAttacks in Neural Networks. In Proceedings of IEEE Symposium\non Security and Privacy (S&P), 2019.\n[63] Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li.\nRAB: Provable Robustness Against Backdoor Attacks. ArXiv e\u0002prints, 2020.\n[64] Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and\nXingjun Ma. Skip Connections Matter: On the Transferability of\nAdversarial Examples Generated with ResNets. In Proceedings\nof International Conference on Learning Representations (ICLR),\n2020.\n[65] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. DBA: Dis\u0002tributed Backdoor Attacks against Federated Learning. In Pro\u0002ceedings of International Conference on Learning Representations\n(ICLR), 2020.\n[66] W. Xu, D. Evans, and Y. Qi. Feature Squeezing: Detecting\nAdversarial Examples in Deep Neural Networks. In Proceedings\nof Network and Distributed System Security Symposium (NDSS),\n2018.\n[67] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A. Gunter,\nand Bo Li. Detecting AI Trojans Using Meta Neural Analysis. In",
      "length": 1942
    },
    {
      "chunk_id": 45,
      "content": "and Bo Li. Detecting AI Trojans Using Meta Neural Analysis. In\nProceedings of IEEE Symposium on Security and Privacy (S&P),\n2020.\n[68] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. Latent\nBackdoor Attacks on Deep Neural Networks. In Proceedings of\nACM SAC Conference on Computer and Communications (CCS),\n2019.\n[69] Kota Yoshida and Takeshi Fujino. Disabling Backdoor and Iden\u0002tifying Poison Data by Using Knowledge Distillation in Backdoor\nAttacks on Deep Neural Networks. In Proceedings of ACM\nWorkshop on Artificial Intelligence and Security (AISec), 2020.\n[70] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How\nTransferable Are Features in Deep Neural Networks? In Pro\u0002ceedings of Advances in Neural Information Processing Systems\n(NeurIPS), 2014.\n[71] Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu\nLuo, and Ting Wang. Interpretable Deep Learning under Fire.\nIn Proceedings of USENIX Security Symposium (SEC), 2020.\n[72] Xinyang Zhang, Zheng Zhang, and Ting Wang. Trojaning Lan\u0002guage Models for Fun and Profit. ArXiv e-prints, 2020.\n[73] Chen Zhu, W. Ronny Huang, Ali Shafahi, Hengduo Li, Gavin\nTaylor, Christoph Studer, and Tom Goldstein. Transferable Clean\u0002Label Poisoning Attacks on Deep Neural Nets. In Proceedings of\nIEEE Conference on Machine Learning (ICML), 2019.\n16\nAppendix A.\nImplementation Details\nBelow we elaborate on the implementation of attacks\nand defenses in this paper.\nA.1. Default Parameter Setting\nTable 20 and Table 21 summarize the default parameter\nsetting in our empirical evaluation (§ 5).\nAttack Parameter Setting\nTraining\nlearning rate 0.01\nretrain epoch 50\noptimizer SGD (nesterov)\nmomentum 0.9\nweight decay 2e-4\nBN toxic data percent 1%\nTNN\npreprocess layer penultimate logits\nneuron number 2\npreprocess optimizer PGD\npreprocess lr 0.015\npreprocess iter 20\nthreshold 5\ntarget value 10\nRB\ncandidate number 50\nselection number 10\nselection iter 5\ninner epoch 5\nLB\npreprocess layer penultimate logits\npreprocess lr 0.1",
      "length": 1987
    },
    {
      "chunk_id": 46,
      "content": "selection iter 5\ninner epoch 5\nLB\npreprocess layer penultimate logits\npreprocess lr 0.1\npreprocess optimizer Adam (tanh constrained)\npreprocess iter 100\nsamples per class 1000\nMSE loss weight 0.5\nESB\nTrojanNet 4-layer MLP\nhidden neurons per layer 8\nsingle layer structure [fc, bn, relu]\nTrojanNet influence α =0.7\namplify rate 100\ntemperature 0.1\nABE\ndiscriminator loss weight λ =0.1\ndiscriminator lr 1e-3\nIMC\ntrigger optimizer PGD\nPGD lr α =20/255\nPGD iter 20\nTable 20. Attack default parameter setting.\nA.2. Pseudo-linearity of downstream model\nWe have shown in § 5 that most attacks seem agnostic\nto the downstream model. Here, we provide possible\nexplanations. Consider a binary classification setting and\na trigger input x with ground-truth class “-” and target\nclass “+”. Recall that a backdoor attack essentially shifts\nx in the feature space by maximizing the quantity of\n∆f = Eµ+ [f(x)] − Eµ− [f(x)] (7)\nwhere µ\n+ and µ− respectively denote the data distribution\nof the ground-truth positive and negative classes.\nNow consider the end-to-end system g ◦ f. The like\u0002lihood that x is misclassified into “+” is given by:\n∆g◦f = Eµ+ [g ◦ f(x)] − Eµ− [g ◦ f(x)] (8)\nDefense Parameter Setting\nRS\nsample distribution Gaussian\nsample number 100\nsample std 0.01\nDU\ndownsample filter Anti Alias\ndownsample ratio 0.95\nMP\ntraining noise std 0.1\nstructure [32]\nSTRIP\nmixing weight 0.5 (equal)\nsample number 64\nNEO\nsample number 100\nKmeans cluster number 3\nthreshold 80\nAR\nPGD lr α =2/255\nperturbation threshold \u000f =8/255\nPGD iter 7\nlearning rate 0.01\nepoch 50\nFP prune ratio 0.95\nNC\nnorm regularization weight 1e-3\nremask lr 0.1\nremask epoch per label 10\nDI\nsample dataset ratio 0.1\nnoise dimension 100\nremask lr 0.01\nremask epoch per label 20\nTABOR regularization weight\nλ1 =1e-6\nλ2 =1e-5\nλ3 =1e-7\nλ4 =1e-8\nλ5 =0\nλ6 =1e-2\nNI\nweighting coefficient\nλsp =1e-5\nλsm =1e-5\nλpe =1\nthreshold 0\nsample ratio 0.1\nABS\nsample k 1\nsample number 5\nmax trojan size 16\nremask lr 0.1\nremask iter per neuron 1000",
      "length": 1990
    },
    {
      "chunk_id": 47,
      "content": "ABS\nsample k 1\nsample number 5\nmax trojan size 16\nremask lr 0.1\nremask iter per neuron 1000\nremask weight\n0.1 if norm< 16\n10 if 16 <norm< 100\n100 if norm> 100\nTable 21. Defense default parameter setting.\nOne sufficient condition for the attack to succeed is\nthat ∆g◦f is linearly correlated with ∆f (i.e., ∆g◦f ∝\n∆f ). If so, we say that the function represented by g\nis pseudo-linear. Unfortunately, in practice, most down\u0002stream models are fairly simple (e.g., one fully-connected\nlayer), showing pseudo-linearity. Possible reasons include:\n(i) complex architectures are difficult to train especially\nwhen the training data is limited; (ii) they imply much\nhigher computational overhead; (iii) the ground-truth map\u0002ping from the feature space to the output space may indeed\nbe pseudo-linear.\n17\nAppendix B.\nAdditional Experiments\nB.1. Attack\nFigure 12 and 13 complement the results of attack\nperformance evaluation on ImageNet with respect to trig\u0002ger size and trigger transparency in Section 5.2. Note that\nFigure 13 uses α = 0.3, which is more transparent than\nα = 0.0 used in Table 14. Therefore, all attacks at 1 × 1\ntrigger size are not working and their ASR are close to\n10%. This is not conflict to the observation in Table 14.\nThe attacks tend to be sensitive to the trigger trans\u0002parency but insensitive to the trigger size (claimed in\nSection 4.2.1). All the attacks fail under |m| = 1 × 1\nand are excluded from Figure 3 in Section 4.2.1. Table 14\nand Figure 13 use different settings. Table 14: α = 0.0 on\nCIFAR10, Figure 13: α = 0.3 on ImageNet, which cause\nthe difference in terms of trigger transparency and data\ncomplexity.\nThe attacks tend to be sensitive to the trigger trans\u0002parency but insensitive to the trigger size (claimed in\nSection 4.2.1). |m| = 1 × 1 is not working for all attacks\nand are excluded from Figure 3 in Section 4.2.1. Table 14\nand Figure 13 use different settings. Table 14: α = 0.0 on\nCIFAR10, Figure 13: α = 0.3 on ImageNet, which cause",
      "length": 1979
    },
    {
      "chunk_id": 48,
      "content": "CIFAR10, Figure 13: α = 0.3 on ImageNet, which cause\nthe difference in terms of trigger transparency and data\ncomplexity.\nASR (%)\n0\n20\n40\n60\n80\n100\nTrigger transparency ( )\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nFigure 12: ASR with respect to trigger transparency (|m| = 3×3,\nImageNet).\nTrigger size ( )\nASR (%)\n0\n20\n40\n60\n80\n100\n1 1 2 2 3 3 4 4 5 5 6 6 7 7\nFigure 13: ASR with respect to trigger size (α = 0.3, ImageNet).\nTable 22 complements the results in Table 5.\nB.2. Defense\nTable 23 presents more information (F1-score, preci\u0002sion, recall, and accuracy), which complements Table 10.\nBN TNN RB TB LB ESB ABE IMC\nGTSRB 65.63 71.70 0.94 0.58 98.42 68.41 68.41 97.58\nCIFAR100 64.53 89.76 42.77 23.44 97.83 0.98 67.86 98.75\nVGGFace2 85.62 97.30 92.31 88.75 98.08 100.00 72.74 98.43\nTable 22. Impact of data complexity on ASR (|m| = 3 × 3 and\nα = 0.8 for GTSRB and CIFAR100, |m| = 25×25 and α = 0.0\nfor VGGFace2).\nDefense Measure BN TNN RB TB LB ESB ABE IMC\nSTRIP\nF1 Score 0.12 0.21 0.47 0.39 0.91 0.18 0.13 0.95\nPrecision 0.41 0.56 0.77 0.73 0.90 0.52 0.43 0.91\nRecall 0.07 0.13 0.34 0.27 0.91 0.10 0.07 0.99\nAccuracy 0.48 0.51 0.62 0.58 0.91 0.50 0.49 0.95\nNEO\nF1 Score 0.45 0.37 0.45 0.34 0.45 0.77 0.43 0.45\nPrecision 1.00 1.00 1.00 0.35 1.00 0.96 0.90 1.00\nRecall 0.29 0.23 0.29 0.36 0.29 0.64 0.28 0.29\nAccuracy 0.65 0.62 0.65 0.36 0.65 0.81 0.63 0.65\nTable 23. Additional statistics of input filtering.\nFigure 14 and 15 shows the influence of DNN archi\u0002tecture and trigger definition on the performance of attack\u0002agnostic defenses (MP, AR, RS, DU).\nFigure 16 illustrate the impact of DNN architecture on\nthe performance of input filtering defenses (NEO, STRIP),\nwhich complements Figure 6.\nFigure 17 and 18 illustrate the impact of DNN architec\u0002ture and trigger definition on the performance of model\u0002inspection defenses (ABS, NI, TABOR, DI, NC).\n0\n20\n40\n60\n80\n100\nFigure 14: Impact of DNN architecture on attack-agnostic de\u0002fenses (lower: ResNet18, middle: DenseNet121; upper: VGG13).\n18\n0",
      "length": 1999
    },
    {
      "chunk_id": 49,
      "content": "18\n0\n20\n40\n60\n80\n100\nFigure 15: Impact of trigger definition on attack-agnostic de\u0002fenses (left: |m| = 3 × 3, right: |m| = 6 × 6; lower: α = 0.0,\nupper: α = 0.8).\n0.0\n0.2\n0.4\n0.6\n0.8\nFigure 16: Impact of DNN architecture on input filtering de\u0002fenses (lower: ResNet18, middle: DenseNet121; upper: VGG13).\n0.0\n1.0\n1.5\n2.0\n2.5\n3.0\n0.5\nFigure 17: Impact of DNN architecture on model filtering de\u0002fenses (lower: ResNet18, middle: DenseNet121; upper: VGG13;\nnote: ESB–ABS pair is inapplicable).\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFigure 18: Impact of trigger definition on model filtering de\u0002fenses (left: |m| = 3 × 3, right: |m| = 6 × 6; lower: α = 0.0,\nupper: α = 0.8; note: ESB–ABS pair is inapplicable).\n19",
      "length": 697
    }
  ]
}